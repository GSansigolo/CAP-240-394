{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Redes Neurais Artificiais simulam a maneira de transmissão e aprendizagem de informação dos neurônios presentes no cérebro humano. RNA’s são definidas como uma técnica de otimização onde se busca por uma hipótese de uma função recorrendo a otimização da mesma, ou seja, minimizar (ou otimizar) uma função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diversos modelos de ANN: Perceptron, Auto Encoder, Convolution, Kohonen são alguns exemplos. Neste trabalho utilizamos a Rede Neural Perceptron Multicamadas (MLP). A rede MLP é um modelo composto por múltiplas camadas de neurônios artificiais totalmente conectadas, denominadas: Camada de Entrada, Camada Intermediária e Camada de Saída. Um exemplo de sua arquitetura pode ser visualizada na figura a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQAGlDDqdEvLbBHwJw9qpYzXHMJFhO7wB1EU0rL5Bn-bqUVIl5cyw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que a MLP consiga prever com exatidão a hipótese é necessário realizar seu treinamento. Por se tratar de um algoritmo de aprendizado supervisionado, devemos apresentar o conjunto $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})$. A seguir são apresentados os resultados dos algoritmos de aprendizagem supervisionado. Foram realizados dois teste. No primeiro os algoritmos receberam como entrada NDVI, Diferença do NVDI, NBRL, Diferença do NBRL. O conjunto de entrada neste caso ficou definido como $x^{(i)} \\in \\mathbb{R}^{4} y^{(i)} \\in {0,1} $. No segundo teste os algoritmos receberam como entrada os atributos dos índices espectrais NDVI, Diferença do NVDI, NBRL, Diferença do NBRL, e as medianas das bandas $[2,5]$. O conjunto de entrada neste caso ficou definido como $x^{(i)} \\in \\mathbb{R}^{8} y^{(i)} \\in {0,1} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código MLP usando o Sckit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando os **import** das bibliotecas necessárias para realizar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import gdal, gdalconst\n",
    "import geopandas as gpd\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um **`GeoDataFrame`** com o shapely de queimadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"DS/221_067_2017.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamando o método **`head()`** do **Pandas** para visualizar os atributos do objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cod_sat</th>\n",
       "      <th>cena_id</th>\n",
       "      <th>nome_arq</th>\n",
       "      <th>orb_pto</th>\n",
       "      <th>area_ha</th>\n",
       "      <th>perim</th>\n",
       "      <th>n_arq_ant</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>nbrl</th>\n",
       "      <th>...</th>\n",
       "      <th>medianb4</th>\n",
       "      <th>medianb5</th>\n",
       "      <th>medianb6</th>\n",
       "      <th>medianb7</th>\n",
       "      <th>verifica</th>\n",
       "      <th>proc_id</th>\n",
       "      <th>focos</th>\n",
       "      <th>data_atual</th>\n",
       "      <th>data_anter</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23946449</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>12.968645</td>\n",
       "      <td>2639</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.211523</td>\n",
       "      <td>-0.044128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144337</td>\n",
       "      <td>0.227026</td>\n",
       "      <td>0.304834</td>\n",
       "      <td>0.245830</td>\n",
       "      <td>1</td>\n",
       "      <td>5886</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.9715795250224 -10.3363584942161,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23946450</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>1.263320</td>\n",
       "      <td>600</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.042114</td>\n",
       "      <td>-0.027508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>0.044027</td>\n",
       "      <td>0.049672</td>\n",
       "      <td>0.046862</td>\n",
       "      <td>1</td>\n",
       "      <td>5886</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.358025045795 -10.3373571795547, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23946451</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>7.568930</td>\n",
       "      <td>2340</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.306289</td>\n",
       "      <td>-0.003284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083032</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.205350</td>\n",
       "      <td>0.169358</td>\n",
       "      <td>1</td>\n",
       "      <td>5886</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.9860921817999 -10.3359982294198,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23946452</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>1.804195</td>\n",
       "      <td>660</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.256839</td>\n",
       "      <td>-0.017937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143473</td>\n",
       "      <td>0.242181</td>\n",
       "      <td>0.360290</td>\n",
       "      <td>0.251297</td>\n",
       "      <td>0</td>\n",
       "      <td>5886</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.0128759745859 -10.3521908768478,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23946453</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>2.886433</td>\n",
       "      <td>1140</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116126</td>\n",
       "      <td>0.195281</td>\n",
       "      <td>0.244584</td>\n",
       "      <td>0.189877</td>\n",
       "      <td>1</td>\n",
       "      <td>5886</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.3574643464459 -10.3343756813685,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  cod_sat                cena_id                      nome_arq  \\\n",
       "0  23946449        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "1  23946450        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "2  23946451        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "3  23946452        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "4  23946453        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "\n",
       "   orb_pto    area_ha  perim                     n_arq_ant      ndvi  \\\n",
       "0  221_067  12.968645   2639  LC82210672017109LGN00.tar.bz  0.211523   \n",
       "1  221_067   1.263320    600  LC82210672017109LGN00.tar.bz  0.042114   \n",
       "2  221_067   7.568930   2340  LC82210672017109LGN00.tar.bz  0.306289   \n",
       "3  221_067   1.804195    660  LC82210672017109LGN00.tar.bz  0.256839   \n",
       "4  221_067   2.886433   1140  LC82210672017109LGN00.tar.bz  0.240240   \n",
       "\n",
       "       nbrl                        ...                          medianb4  \\\n",
       "0 -0.044128                        ...                          0.144337   \n",
       "1 -0.027508                        ...                          0.040925   \n",
       "2 -0.003284                        ...                          0.083032   \n",
       "3 -0.017937                        ...                          0.143473   \n",
       "4  0.000871                        ...                          0.116126   \n",
       "\n",
       "   medianb5  medianb6  medianb7  verifica  proc_id  focos  data_atual  \\\n",
       "0  0.227026  0.304834  0.245830         1     5886      2  2017-05-05   \n",
       "1  0.044027  0.049672  0.046862         1     5886      0  2017-05-05   \n",
       "2  0.151609  0.205350  0.169358         1     5886      1  2017-05-05   \n",
       "3  0.242181  0.360290  0.251297         0     5886      0  2017-05-05   \n",
       "4  0.195281  0.244584  0.189877         1     5886      2  2017-05-05   \n",
       "\n",
       "            data_anter                                           geometry  \n",
       "0  2017-04-19 00:00:00  POLYGON ((-46.9715795250224 -10.3363584942161,...  \n",
       "1  2017-04-19 00:00:00  POLYGON ((-46.358025045795 -10.3373571795547, ...  \n",
       "2  2017-04-19 00:00:00  POLYGON ((-46.9860921817999 -10.3359982294198,...  \n",
       "3  2017-04-19 00:00:00  POLYGON ((-46.0128759745859 -10.3521908768478,...  \n",
       "4  2017-04-19 00:00:00  POLYGON ((-46.3574643464459 -10.3343756813685,...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será realizado dois testes com a MLP. No primeiro, o algoritmo será treinado somente com os índices espectrais **` NDVI`, `DIF_NDVI`, `NBRL`, `DIF_NBRL`** e no segundo teste além dos índices espectrais, será usado as medianas das bandas [2 à 5] do arquivo shapely para o treinamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Teste 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próximo passo, será criado duas váriaveis com dados a ser passado ao algoritmo de treinamento. \n",
    "\n",
    "- `x` - Essa variável contém como dados de entrada os índices espectrais **`ndvi`**, **`nbrl`**, e suas diferenças o **`dif_ndvi`** e o **`dif_dnbrl`**. \n",
    "\n",
    "- `y` - Essa variável contém os dados da coluna **`verifica`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df[['ndvi','nbrl','dif_ndvi','dif_dnbrl']])\n",
    "y = np.array(df[['verifica']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados separado, será definido o modelo da **Rede Neural Artificial Multilayer**. Os parâmetros foram definidos de forma empirica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(activation='identity', solver='adam', verbose=True, learning_rate_init=0.00001,\n",
    "                    alpha=1e-5,hidden_layer_sizes=(80,2),  momentum=0.7, random_state=1, max_iter=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das metologias de avaliação consiste em dividir os dados em dois conjuntos: teste e treinamento, calculando a pontuação 7 vezes consecutivas (com diferentes divisões de cada vez).\n",
    "\n",
    "Assim é definido no atributo de entrada **random_state** da função **KFold** o valor 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliar a performance dos algoritmos foi utilizada a função Cross-validation da biblioteca scikit-learn. A avaliação dos dados não deve ser feita utilizando o conjunto de treinamento.\n",
    "\n",
    "Dito isso, agora é realizado o treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73628137\n",
      "Iteration 2, loss = 0.73219083\n",
      "Iteration 3, loss = 0.72831176\n",
      "Iteration 4, loss = 0.72461181\n",
      "Iteration 5, loss = 0.72109237\n",
      "Iteration 6, loss = 0.71773563\n",
      "Iteration 7, loss = 0.71456291\n",
      "Iteration 8, loss = 0.71156365\n",
      "Iteration 9, loss = 0.70871941\n",
      "Iteration 10, loss = 0.70601857\n",
      "Iteration 11, loss = 0.70349401\n",
      "Iteration 12, loss = 0.70110945\n",
      "Iteration 13, loss = 0.69887461\n",
      "Iteration 14, loss = 0.69676361\n",
      "Iteration 15, loss = 0.69477623\n",
      "Iteration 16, loss = 0.69292899\n",
      "Iteration 17, loss = 0.69117934\n",
      "Iteration 18, loss = 0.68954142\n",
      "Iteration 19, loss = 0.68800179\n",
      "Iteration 20, loss = 0.68656979\n",
      "Iteration 21, loss = 0.68521460\n",
      "Iteration 22, loss = 0.68394877\n",
      "Iteration 23, loss = 0.68276039\n",
      "Iteration 24, loss = 0.68163338\n",
      "Iteration 25, loss = 0.68057293\n",
      "Iteration 26, loss = 0.67955545\n",
      "Iteration 27, loss = 0.67859946\n",
      "Iteration 28, loss = 0.67768865\n",
      "Iteration 29, loss = 0.67681056\n",
      "Iteration 30, loss = 0.67597170\n",
      "Iteration 31, loss = 0.67516794\n",
      "Iteration 32, loss = 0.67438381\n",
      "Iteration 33, loss = 0.67362621\n",
      "Iteration 34, loss = 0.67287611\n",
      "Iteration 35, loss = 0.67214959\n",
      "Iteration 36, loss = 0.67143540\n",
      "Iteration 37, loss = 0.67073098\n",
      "Iteration 38, loss = 0.67003497\n",
      "Iteration 39, loss = 0.66934426\n",
      "Iteration 40, loss = 0.66866146\n",
      "Iteration 41, loss = 0.66797624\n",
      "Iteration 42, loss = 0.66729459\n",
      "Iteration 43, loss = 0.66661517\n",
      "Iteration 44, loss = 0.66593629\n",
      "Iteration 45, loss = 0.66525781\n",
      "Iteration 46, loss = 0.66457596\n",
      "Iteration 47, loss = 0.66389547\n",
      "Iteration 48, loss = 0.66321429\n",
      "Iteration 49, loss = 0.66252847\n",
      "Iteration 50, loss = 0.66183710\n",
      "Iteration 51, loss = 0.66114619\n",
      "Iteration 52, loss = 0.66045179\n",
      "Iteration 53, loss = 0.65975544\n",
      "Iteration 54, loss = 0.65905505\n",
      "Iteration 55, loss = 0.65835477\n",
      "Iteration 56, loss = 0.65764534\n",
      "Iteration 57, loss = 0.65693610\n",
      "Iteration 58, loss = 0.65622494\n",
      "Iteration 59, loss = 0.65551294\n",
      "Iteration 60, loss = 0.65478918\n",
      "Iteration 61, loss = 0.65407108\n",
      "Iteration 62, loss = 0.65334836\n",
      "Iteration 63, loss = 0.65262713\n",
      "Iteration 64, loss = 0.65190054\n",
      "Iteration 65, loss = 0.65117428\n",
      "Iteration 66, loss = 0.65044702\n",
      "Iteration 67, loss = 0.64971611\n",
      "Iteration 68, loss = 0.64898284\n",
      "Iteration 69, loss = 0.64824722\n",
      "Iteration 70, loss = 0.64751006\n",
      "Iteration 71, loss = 0.64677370\n",
      "Iteration 72, loss = 0.64603344\n",
      "Iteration 73, loss = 0.64529157\n",
      "Iteration 74, loss = 0.64454780\n",
      "Iteration 75, loss = 0.64380024\n",
      "Iteration 76, loss = 0.64305030\n",
      "Iteration 77, loss = 0.64229875\n",
      "Iteration 78, loss = 0.64154379\n",
      "Iteration 79, loss = 0.64078835\n",
      "Iteration 80, loss = 0.64003503\n",
      "Iteration 81, loss = 0.63927475\n",
      "Iteration 82, loss = 0.63851618\n",
      "Iteration 83, loss = 0.63776177\n",
      "Iteration 84, loss = 0.63700210\n",
      "Iteration 85, loss = 0.63624272\n",
      "Iteration 86, loss = 0.63548130\n",
      "Iteration 87, loss = 0.63472453\n",
      "Iteration 88, loss = 0.63395962\n",
      "Iteration 89, loss = 0.63319681\n",
      "Iteration 90, loss = 0.63243857\n",
      "Iteration 91, loss = 0.63167698\n",
      "Iteration 92, loss = 0.63091500\n",
      "Iteration 93, loss = 0.63015188\n",
      "Iteration 94, loss = 0.62938811\n",
      "Iteration 95, loss = 0.62862456\n",
      "Iteration 96, loss = 0.62787169\n",
      "Iteration 97, loss = 0.62710432\n",
      "Iteration 98, loss = 0.62635104\n",
      "Iteration 99, loss = 0.62558310\n",
      "Iteration 100, loss = 0.62482282\n",
      "Iteration 101, loss = 0.62406055\n",
      "Iteration 102, loss = 0.62329914\n",
      "Iteration 103, loss = 0.62253217\n",
      "Iteration 104, loss = 0.62176660\n",
      "Iteration 105, loss = 0.62100286\n",
      "Iteration 106, loss = 0.62023791\n",
      "Iteration 107, loss = 0.61947228\n",
      "Iteration 108, loss = 0.61871222\n",
      "Iteration 109, loss = 0.61794241\n",
      "Iteration 110, loss = 0.61717518\n",
      "Iteration 111, loss = 0.61640500\n",
      "Iteration 112, loss = 0.61563736\n",
      "Iteration 113, loss = 0.61486514\n",
      "Iteration 114, loss = 0.61409397\n",
      "Iteration 115, loss = 0.61332668\n",
      "Iteration 116, loss = 0.61255561\n",
      "Iteration 117, loss = 0.61177981\n",
      "Iteration 118, loss = 0.61101074\n",
      "Iteration 119, loss = 0.61023464\n",
      "Iteration 120, loss = 0.60946380\n",
      "Iteration 121, loss = 0.60869553\n",
      "Iteration 122, loss = 0.60792376\n",
      "Iteration 123, loss = 0.60715693\n",
      "Iteration 124, loss = 0.60638933\n",
      "Iteration 125, loss = 0.60562073\n",
      "Iteration 126, loss = 0.60485814\n",
      "Iteration 127, loss = 0.60408952\n",
      "Iteration 128, loss = 0.60332458\n",
      "Iteration 129, loss = 0.60256284\n",
      "Iteration 130, loss = 0.60180100\n",
      "Iteration 131, loss = 0.60103207\n",
      "Iteration 132, loss = 0.60027401\n",
      "Iteration 133, loss = 0.59951991\n",
      "Iteration 134, loss = 0.59876122\n",
      "Iteration 135, loss = 0.59801467\n",
      "Iteration 136, loss = 0.59725121\n",
      "Iteration 137, loss = 0.59649788\n",
      "Iteration 138, loss = 0.59574325\n",
      "Iteration 139, loss = 0.59499528\n",
      "Iteration 140, loss = 0.59424674\n",
      "Iteration 141, loss = 0.59350154\n",
      "Iteration 142, loss = 0.59274910\n",
      "Iteration 143, loss = 0.59200671\n",
      "Iteration 144, loss = 0.59126015\n",
      "Iteration 145, loss = 0.59051371\n",
      "Iteration 146, loss = 0.58977279\n",
      "Iteration 147, loss = 0.58902848\n",
      "Iteration 148, loss = 0.58829015\n",
      "Iteration 149, loss = 0.58754849\n",
      "Iteration 150, loss = 0.58681049\n",
      "Iteration 151, loss = 0.58607520\n",
      "Iteration 152, loss = 0.58533891\n",
      "Iteration 153, loss = 0.58460581\n",
      "Iteration 154, loss = 0.58387565\n",
      "Iteration 155, loss = 0.58314512\n",
      "Iteration 156, loss = 0.58241821\n",
      "Iteration 157, loss = 0.58169337\n",
      "Iteration 158, loss = 0.58097979\n",
      "Iteration 159, loss = 0.58025403\n",
      "Iteration 160, loss = 0.57953541\n",
      "Iteration 161, loss = 0.57882219\n",
      "Iteration 162, loss = 0.57810761\n",
      "Iteration 163, loss = 0.57739716\n",
      "Iteration 164, loss = 0.57668635\n",
      "Iteration 165, loss = 0.57598004\n",
      "Iteration 166, loss = 0.57526975\n",
      "Iteration 167, loss = 0.57456421\n",
      "Iteration 168, loss = 0.57386691\n",
      "Iteration 169, loss = 0.57316636\n",
      "Iteration 170, loss = 0.57246608\n",
      "Iteration 171, loss = 0.57176788\n",
      "Iteration 172, loss = 0.57107772\n",
      "Iteration 173, loss = 0.57038282\n",
      "Iteration 174, loss = 0.56969340\n",
      "Iteration 175, loss = 0.56901044\n",
      "Iteration 176, loss = 0.56832096\n",
      "Iteration 177, loss = 0.56763832\n",
      "Iteration 178, loss = 0.56695413\n",
      "Iteration 179, loss = 0.56627579\n",
      "Iteration 180, loss = 0.56560599\n",
      "Iteration 181, loss = 0.56493310\n",
      "Iteration 182, loss = 0.56426704\n",
      "Iteration 183, loss = 0.56360413\n",
      "Iteration 184, loss = 0.56294557\n",
      "Iteration 185, loss = 0.56229185\n",
      "Iteration 186, loss = 0.56164597\n",
      "Iteration 187, loss = 0.56098925\n",
      "Iteration 188, loss = 0.56033870\n",
      "Iteration 189, loss = 0.55969557\n",
      "Iteration 190, loss = 0.55904926\n",
      "Iteration 191, loss = 0.55841181\n",
      "Iteration 192, loss = 0.55777286\n",
      "Iteration 193, loss = 0.55714491\n",
      "Iteration 194, loss = 0.55651836\n",
      "Iteration 195, loss = 0.55589092\n",
      "Iteration 196, loss = 0.55527026\n",
      "Iteration 197, loss = 0.55465146\n",
      "Iteration 198, loss = 0.55403975\n",
      "Iteration 199, loss = 0.55342538\n",
      "Iteration 200, loss = 0.55281371\n",
      "Iteration 201, loss = 0.55220937\n",
      "Iteration 202, loss = 0.55160486\n",
      "Iteration 203, loss = 0.55100673\n",
      "Iteration 204, loss = 0.55041195\n",
      "Iteration 205, loss = 0.54981217\n",
      "Iteration 206, loss = 0.54922174\n",
      "Iteration 207, loss = 0.54864629\n",
      "Iteration 208, loss = 0.54805123\n",
      "Iteration 209, loss = 0.54746198\n",
      "Iteration 210, loss = 0.54688371\n",
      "Iteration 211, loss = 0.54629707\n",
      "Iteration 212, loss = 0.54572664\n",
      "Iteration 213, loss = 0.54515519\n",
      "Iteration 214, loss = 0.54458088\n",
      "Iteration 215, loss = 0.54401706\n",
      "Iteration 216, loss = 0.54345344\n",
      "Iteration 217, loss = 0.54289371\n",
      "Iteration 218, loss = 0.54234022\n",
      "Iteration 219, loss = 0.54178331\n",
      "Iteration 220, loss = 0.54122998\n",
      "Iteration 221, loss = 0.54068529\n",
      "Iteration 222, loss = 0.54014149\n",
      "Iteration 223, loss = 0.53960273\n",
      "Iteration 224, loss = 0.53906328\n",
      "Iteration 225, loss = 0.53852821\n",
      "Iteration 226, loss = 0.53800142\n",
      "Iteration 227, loss = 0.53748269\n",
      "Iteration 228, loss = 0.53695993\n",
      "Iteration 229, loss = 0.53643929\n",
      "Iteration 230, loss = 0.53592161\n",
      "Iteration 231, loss = 0.53540982\n",
      "Iteration 232, loss = 0.53490099\n",
      "Iteration 233, loss = 0.53439626\n",
      "Iteration 234, loss = 0.53389299\n",
      "Iteration 235, loss = 0.53339127\n",
      "Iteration 236, loss = 0.53289780\n",
      "Iteration 237, loss = 0.53239838\n",
      "Iteration 238, loss = 0.53191193\n",
      "Iteration 239, loss = 0.53142046\n",
      "Iteration 240, loss = 0.53093878\n",
      "Iteration 241, loss = 0.53046254\n",
      "Iteration 242, loss = 0.52998874\n",
      "Iteration 243, loss = 0.52950699\n",
      "Iteration 244, loss = 0.52903881\n",
      "Iteration 245, loss = 0.52857291\n",
      "Iteration 246, loss = 0.52810683\n",
      "Iteration 247, loss = 0.52764764\n",
      "Iteration 248, loss = 0.52719772\n",
      "Iteration 249, loss = 0.52673404\n",
      "Iteration 250, loss = 0.52627543\n",
      "Iteration 251, loss = 0.52582814\n",
      "Iteration 252, loss = 0.52537865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.52494155\n",
      "Iteration 254, loss = 0.52450168\n",
      "Iteration 255, loss = 0.52406489\n",
      "Iteration 256, loss = 0.52363496\n",
      "Iteration 257, loss = 0.52320376\n",
      "Iteration 258, loss = 0.52277501\n",
      "Iteration 259, loss = 0.52235900\n",
      "Iteration 260, loss = 0.52193361\n",
      "Iteration 261, loss = 0.52151460\n",
      "Iteration 262, loss = 0.52110239\n",
      "Iteration 263, loss = 0.52068788\n",
      "Iteration 264, loss = 0.52027936\n",
      "Iteration 265, loss = 0.51987975\n",
      "Iteration 266, loss = 0.51947562\n",
      "Iteration 267, loss = 0.51908147\n",
      "Iteration 268, loss = 0.51868574\n",
      "Iteration 269, loss = 0.51829570\n",
      "Iteration 270, loss = 0.51790209\n",
      "Iteration 271, loss = 0.51751952\n",
      "Iteration 272, loss = 0.51714860\n",
      "Iteration 273, loss = 0.51676331\n",
      "Iteration 274, loss = 0.51638674\n",
      "Iteration 275, loss = 0.51601363\n",
      "Iteration 276, loss = 0.51564672\n",
      "Iteration 277, loss = 0.51527569\n",
      "Iteration 278, loss = 0.51491768\n",
      "Iteration 279, loss = 0.51455985\n",
      "Iteration 280, loss = 0.51419391\n",
      "Iteration 281, loss = 0.51384367\n",
      "Iteration 282, loss = 0.51349712\n",
      "Iteration 283, loss = 0.51314235\n",
      "Iteration 284, loss = 0.51280704\n",
      "Iteration 285, loss = 0.51245952\n",
      "Iteration 286, loss = 0.51211913\n",
      "Iteration 287, loss = 0.51178843\n",
      "Iteration 288, loss = 0.51145525\n",
      "Iteration 289, loss = 0.51113398\n",
      "Iteration 290, loss = 0.51081639\n",
      "Iteration 291, loss = 0.51048357\n",
      "Iteration 292, loss = 0.51018139\n",
      "Iteration 293, loss = 0.50985842\n",
      "Iteration 294, loss = 0.50953886\n",
      "Iteration 295, loss = 0.50923216\n",
      "Iteration 296, loss = 0.50893110\n",
      "Iteration 297, loss = 0.50862429\n",
      "Iteration 298, loss = 0.50832698\n",
      "Iteration 299, loss = 0.50802945\n",
      "Iteration 300, loss = 0.50773753\n",
      "Iteration 301, loss = 0.50744186\n",
      "Iteration 302, loss = 0.50714839\n",
      "Iteration 303, loss = 0.50686431\n",
      "Iteration 304, loss = 0.50657654\n",
      "Iteration 305, loss = 0.50629683\n",
      "Iteration 306, loss = 0.50601674\n",
      "Iteration 307, loss = 0.50574210\n",
      "Iteration 308, loss = 0.50546492\n",
      "Iteration 309, loss = 0.50519435\n",
      "Iteration 310, loss = 0.50492708\n",
      "Iteration 311, loss = 0.50465594\n",
      "Iteration 312, loss = 0.50439168\n",
      "Iteration 313, loss = 0.50413294\n",
      "Iteration 314, loss = 0.50386953\n",
      "Iteration 315, loss = 0.50361541\n",
      "Iteration 316, loss = 0.50335832\n",
      "Iteration 317, loss = 0.50311010\n",
      "Iteration 318, loss = 0.50286364\n",
      "Iteration 319, loss = 0.50262240\n",
      "Iteration 320, loss = 0.50237082\n",
      "Iteration 321, loss = 0.50213202\n",
      "Iteration 322, loss = 0.50189325\n",
      "Iteration 323, loss = 0.50165118\n",
      "Iteration 324, loss = 0.50141846\n",
      "Iteration 325, loss = 0.50118647\n",
      "Iteration 326, loss = 0.50095746\n",
      "Iteration 327, loss = 0.50072997\n",
      "Iteration 328, loss = 0.50050470\n",
      "Iteration 329, loss = 0.50028113\n",
      "Iteration 330, loss = 0.50005989\n",
      "Iteration 331, loss = 0.49984298\n",
      "Iteration 332, loss = 0.49962632\n",
      "Iteration 333, loss = 0.49941328\n",
      "Iteration 334, loss = 0.49920371\n",
      "Iteration 335, loss = 0.49899623\n",
      "Iteration 336, loss = 0.49878385\n",
      "Iteration 337, loss = 0.49857882\n",
      "Iteration 338, loss = 0.49837493\n",
      "Iteration 339, loss = 0.49818271\n",
      "Iteration 340, loss = 0.49797906\n",
      "Iteration 341, loss = 0.49778607\n",
      "Iteration 342, loss = 0.49758985\n",
      "Iteration 343, loss = 0.49739907\n",
      "Iteration 344, loss = 0.49721099\n",
      "Iteration 345, loss = 0.49702916\n",
      "Iteration 346, loss = 0.49684196\n",
      "Iteration 347, loss = 0.49665728\n",
      "Iteration 348, loss = 0.49647925\n",
      "Iteration 349, loss = 0.49630060\n",
      "Iteration 350, loss = 0.49612344\n",
      "Iteration 351, loss = 0.49595544\n",
      "Iteration 352, loss = 0.49578326\n",
      "Iteration 353, loss = 0.49561280\n",
      "Iteration 354, loss = 0.49544189\n",
      "Iteration 355, loss = 0.49529324\n",
      "Iteration 356, loss = 0.49511506\n",
      "Iteration 357, loss = 0.49494850\n",
      "Iteration 358, loss = 0.49479105\n",
      "Iteration 359, loss = 0.49462962\n",
      "Iteration 360, loss = 0.49447601\n",
      "Iteration 361, loss = 0.49432486\n",
      "Iteration 362, loss = 0.49417198\n",
      "Iteration 363, loss = 0.49402180\n",
      "Iteration 364, loss = 0.49386620\n",
      "Iteration 365, loss = 0.49372390\n",
      "Iteration 366, loss = 0.49358179\n",
      "Iteration 367, loss = 0.49343609\n",
      "Iteration 368, loss = 0.49329573\n",
      "Iteration 369, loss = 0.49315629\n",
      "Iteration 370, loss = 0.49302009\n",
      "Iteration 371, loss = 0.49289373\n",
      "Iteration 372, loss = 0.49275264\n",
      "Iteration 373, loss = 0.49262079\n",
      "Iteration 374, loss = 0.49249211\n",
      "Iteration 375, loss = 0.49236182\n",
      "Iteration 376, loss = 0.49223580\n",
      "Iteration 377, loss = 0.49210598\n",
      "Iteration 378, loss = 0.49197716\n",
      "Iteration 379, loss = 0.49185896\n",
      "Iteration 380, loss = 0.49174115\n",
      "Iteration 381, loss = 0.49162062\n",
      "Iteration 382, loss = 0.49150653\n",
      "Iteration 383, loss = 0.49138692\n",
      "Iteration 384, loss = 0.49127392\n",
      "Iteration 385, loss = 0.49116714\n",
      "Iteration 386, loss = 0.49105039\n",
      "Iteration 387, loss = 0.49094914\n",
      "Iteration 388, loss = 0.49083937\n",
      "Iteration 389, loss = 0.49073024\n",
      "Iteration 390, loss = 0.49062692\n",
      "Iteration 391, loss = 0.49052977\n",
      "Iteration 392, loss = 0.49042523\n",
      "Iteration 393, loss = 0.49031985\n",
      "Iteration 394, loss = 0.49022176\n",
      "Iteration 395, loss = 0.49012792\n",
      "Iteration 396, loss = 0.49002877\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72994337\n",
      "Iteration 2, loss = 0.72627489\n",
      "Iteration 3, loss = 0.72282017\n",
      "Iteration 4, loss = 0.71954049\n",
      "Iteration 5, loss = 0.71643411\n",
      "Iteration 6, loss = 0.71348713\n",
      "Iteration 7, loss = 0.71071347\n",
      "Iteration 8, loss = 0.70810712\n",
      "Iteration 9, loss = 0.70564696\n",
      "Iteration 10, loss = 0.70332915\n",
      "Iteration 11, loss = 0.70117405\n",
      "Iteration 12, loss = 0.69915319\n",
      "Iteration 13, loss = 0.69727066\n",
      "Iteration 14, loss = 0.69550142\n",
      "Iteration 15, loss = 0.69384151\n",
      "Iteration 16, loss = 0.69230606\n",
      "Iteration 17, loss = 0.69085781\n",
      "Iteration 18, loss = 0.68950890\n",
      "Iteration 19, loss = 0.68824335\n",
      "Iteration 20, loss = 0.68707153\n",
      "Iteration 21, loss = 0.68596250\n",
      "Iteration 22, loss = 0.68492775\n",
      "Iteration 23, loss = 0.68395472\n",
      "Iteration 24, loss = 0.68303520\n",
      "Iteration 25, loss = 0.68216509\n",
      "Iteration 26, loss = 0.68132584\n",
      "Iteration 27, loss = 0.68053347\n",
      "Iteration 28, loss = 0.67977843\n",
      "Iteration 29, loss = 0.67904387\n",
      "Iteration 30, loss = 0.67833734\n",
      "Iteration 31, loss = 0.67765590\n",
      "Iteration 32, loss = 0.67698796\n",
      "Iteration 33, loss = 0.67633687\n",
      "Iteration 34, loss = 0.67568892\n",
      "Iteration 35, loss = 0.67505555\n",
      "Iteration 36, loss = 0.67442801\n",
      "Iteration 37, loss = 0.67380752\n",
      "Iteration 38, loss = 0.67319063\n",
      "Iteration 39, loss = 0.67257734\n",
      "Iteration 40, loss = 0.67196490\n",
      "Iteration 41, loss = 0.67135305\n",
      "Iteration 42, loss = 0.67073885\n",
      "Iteration 43, loss = 0.67012890\n",
      "Iteration 44, loss = 0.66951590\n",
      "Iteration 45, loss = 0.66890567\n",
      "Iteration 46, loss = 0.66828862\n",
      "Iteration 47, loss = 0.66767358\n",
      "Iteration 48, loss = 0.66705589\n",
      "Iteration 49, loss = 0.66643631\n",
      "Iteration 50, loss = 0.66581336\n",
      "Iteration 51, loss = 0.66518985\n",
      "Iteration 52, loss = 0.66456255\n",
      "Iteration 53, loss = 0.66393370\n",
      "Iteration 54, loss = 0.66330295\n",
      "Iteration 55, loss = 0.66266967\n",
      "Iteration 56, loss = 0.66203207\n",
      "Iteration 57, loss = 0.66139364\n",
      "Iteration 58, loss = 0.66075244\n",
      "Iteration 59, loss = 0.66011216\n",
      "Iteration 60, loss = 0.65946342\n",
      "Iteration 61, loss = 0.65881802\n",
      "Iteration 62, loss = 0.65817059\n",
      "Iteration 63, loss = 0.65752356\n",
      "Iteration 64, loss = 0.65687162\n",
      "Iteration 65, loss = 0.65622359\n",
      "Iteration 66, loss = 0.65557226\n",
      "Iteration 67, loss = 0.65491703\n",
      "Iteration 68, loss = 0.65426398\n",
      "Iteration 69, loss = 0.65360670\n",
      "Iteration 70, loss = 0.65294847\n",
      "Iteration 71, loss = 0.65229028\n",
      "Iteration 72, loss = 0.65162953\n",
      "Iteration 73, loss = 0.65096730\n",
      "Iteration 74, loss = 0.65030563\n",
      "Iteration 75, loss = 0.64964085\n",
      "Iteration 76, loss = 0.64897085\n",
      "Iteration 77, loss = 0.64830283\n",
      "Iteration 78, loss = 0.64763240\n",
      "Iteration 79, loss = 0.64696346\n",
      "Iteration 80, loss = 0.64629501\n",
      "Iteration 81, loss = 0.64562291\n",
      "Iteration 82, loss = 0.64495252\n",
      "Iteration 83, loss = 0.64428468\n",
      "Iteration 84, loss = 0.64361751\n",
      "Iteration 85, loss = 0.64294338\n",
      "Iteration 86, loss = 0.64227085\n",
      "Iteration 87, loss = 0.64160298\n",
      "Iteration 88, loss = 0.64092641\n",
      "Iteration 89, loss = 0.64025290\n",
      "Iteration 90, loss = 0.63958603\n",
      "Iteration 91, loss = 0.63891339\n",
      "Iteration 92, loss = 0.63824219\n",
      "Iteration 93, loss = 0.63757136\n",
      "Iteration 94, loss = 0.63689964\n",
      "Iteration 95, loss = 0.63622604\n",
      "Iteration 96, loss = 0.63556386\n",
      "Iteration 97, loss = 0.63488588\n",
      "Iteration 98, loss = 0.63422740\n",
      "Iteration 99, loss = 0.63354828\n",
      "Iteration 100, loss = 0.63288075\n",
      "Iteration 101, loss = 0.63221092\n",
      "Iteration 102, loss = 0.63153806\n",
      "Iteration 103, loss = 0.63086098\n",
      "Iteration 104, loss = 0.63018889\n",
      "Iteration 105, loss = 0.62951403\n",
      "Iteration 106, loss = 0.62883894\n",
      "Iteration 107, loss = 0.62816531\n",
      "Iteration 108, loss = 0.62749496\n",
      "Iteration 109, loss = 0.62681847\n",
      "Iteration 110, loss = 0.62614318\n",
      "Iteration 111, loss = 0.62546714\n",
      "Iteration 112, loss = 0.62479133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 113, loss = 0.62411261\n",
      "Iteration 114, loss = 0.62343344\n",
      "Iteration 115, loss = 0.62276051\n",
      "Iteration 116, loss = 0.62208374\n",
      "Iteration 117, loss = 0.62140005\n",
      "Iteration 118, loss = 0.62072590\n",
      "Iteration 119, loss = 0.62004279\n",
      "Iteration 120, loss = 0.61936471\n",
      "Iteration 121, loss = 0.61869073\n",
      "Iteration 122, loss = 0.61801279\n",
      "Iteration 123, loss = 0.61733889\n",
      "Iteration 124, loss = 0.61666368\n",
      "Iteration 125, loss = 0.61598988\n",
      "Iteration 126, loss = 0.61532084\n",
      "Iteration 127, loss = 0.61464553\n",
      "Iteration 128, loss = 0.61397443\n",
      "Iteration 129, loss = 0.61330413\n",
      "Iteration 130, loss = 0.61264219\n",
      "Iteration 131, loss = 0.61196752\n",
      "Iteration 132, loss = 0.61130523\n",
      "Iteration 133, loss = 0.61064074\n",
      "Iteration 134, loss = 0.60997687\n",
      "Iteration 135, loss = 0.60931734\n",
      "Iteration 136, loss = 0.60865201\n",
      "Iteration 137, loss = 0.60798960\n",
      "Iteration 138, loss = 0.60732762\n",
      "Iteration 139, loss = 0.60667294\n",
      "Iteration 140, loss = 0.60601473\n",
      "Iteration 141, loss = 0.60536339\n",
      "Iteration 142, loss = 0.60470377\n",
      "Iteration 143, loss = 0.60405379\n",
      "Iteration 144, loss = 0.60339886\n",
      "Iteration 145, loss = 0.60274542\n",
      "Iteration 146, loss = 0.60209260\n",
      "Iteration 147, loss = 0.60144159\n",
      "Iteration 148, loss = 0.60079014\n",
      "Iteration 149, loss = 0.60013774\n",
      "Iteration 150, loss = 0.59948980\n",
      "Iteration 151, loss = 0.59884376\n",
      "Iteration 152, loss = 0.59819802\n",
      "Iteration 153, loss = 0.59755550\n",
      "Iteration 154, loss = 0.59691431\n",
      "Iteration 155, loss = 0.59627564\n",
      "Iteration 156, loss = 0.59564007\n",
      "Iteration 157, loss = 0.59500902\n",
      "Iteration 158, loss = 0.59438261\n",
      "Iteration 159, loss = 0.59374762\n",
      "Iteration 160, loss = 0.59311899\n",
      "Iteration 161, loss = 0.59249492\n",
      "Iteration 162, loss = 0.59187054\n",
      "Iteration 163, loss = 0.59124837\n",
      "Iteration 164, loss = 0.59062805\n",
      "Iteration 165, loss = 0.59001341\n",
      "Iteration 166, loss = 0.58939449\n",
      "Iteration 167, loss = 0.58877654\n",
      "Iteration 168, loss = 0.58816702\n",
      "Iteration 169, loss = 0.58755546\n",
      "Iteration 170, loss = 0.58694338\n",
      "Iteration 171, loss = 0.58633565\n",
      "Iteration 172, loss = 0.58573396\n",
      "Iteration 173, loss = 0.58512682\n",
      "Iteration 174, loss = 0.58452498\n",
      "Iteration 175, loss = 0.58392951\n",
      "Iteration 176, loss = 0.58332849\n",
      "Iteration 177, loss = 0.58273206\n",
      "Iteration 178, loss = 0.58213632\n",
      "Iteration 179, loss = 0.58154692\n",
      "Iteration 180, loss = 0.58096133\n",
      "Iteration 181, loss = 0.58037676\n",
      "Iteration 182, loss = 0.57979447\n",
      "Iteration 183, loss = 0.57921788\n",
      "Iteration 184, loss = 0.57864246\n",
      "Iteration 185, loss = 0.57807230\n",
      "Iteration 186, loss = 0.57751429\n",
      "Iteration 187, loss = 0.57694163\n",
      "Iteration 188, loss = 0.57637296\n",
      "Iteration 189, loss = 0.57581249\n",
      "Iteration 190, loss = 0.57524996\n",
      "Iteration 191, loss = 0.57469515\n",
      "Iteration 192, loss = 0.57413754\n",
      "Iteration 193, loss = 0.57358979\n",
      "Iteration 194, loss = 0.57304626\n",
      "Iteration 195, loss = 0.57249501\n",
      "Iteration 196, loss = 0.57195452\n",
      "Iteration 197, loss = 0.57141762\n",
      "Iteration 198, loss = 0.57088117\n",
      "Iteration 199, loss = 0.57034473\n",
      "Iteration 200, loss = 0.56981079\n",
      "Iteration 201, loss = 0.56928455\n",
      "Iteration 202, loss = 0.56875786\n",
      "Iteration 203, loss = 0.56823679\n",
      "Iteration 204, loss = 0.56771939\n",
      "Iteration 205, loss = 0.56719790\n",
      "Iteration 206, loss = 0.56668085\n",
      "Iteration 207, loss = 0.56618589\n",
      "Iteration 208, loss = 0.56566516\n",
      "Iteration 209, loss = 0.56515339\n",
      "Iteration 210, loss = 0.56465242\n",
      "Iteration 211, loss = 0.56414344\n",
      "Iteration 212, loss = 0.56364796\n",
      "Iteration 213, loss = 0.56315113\n",
      "Iteration 214, loss = 0.56265150\n",
      "Iteration 215, loss = 0.56216100\n",
      "Iteration 216, loss = 0.56167506\n",
      "Iteration 217, loss = 0.56118491\n",
      "Iteration 218, loss = 0.56070535\n",
      "Iteration 219, loss = 0.56022012\n",
      "Iteration 220, loss = 0.55973775\n",
      "Iteration 221, loss = 0.55926277\n",
      "Iteration 222, loss = 0.55879024\n",
      "Iteration 223, loss = 0.55831777\n",
      "Iteration 224, loss = 0.55784903\n",
      "Iteration 225, loss = 0.55738293\n",
      "Iteration 226, loss = 0.55692390\n",
      "Iteration 227, loss = 0.55647598\n",
      "Iteration 228, loss = 0.55602046\n",
      "Iteration 229, loss = 0.55556950\n",
      "Iteration 230, loss = 0.55511899\n",
      "Iteration 231, loss = 0.55467643\n",
      "Iteration 232, loss = 0.55423499\n",
      "Iteration 233, loss = 0.55379738\n",
      "Iteration 234, loss = 0.55336570\n",
      "Iteration 235, loss = 0.55292676\n",
      "Iteration 236, loss = 0.55249892\n",
      "Iteration 237, loss = 0.55206800\n",
      "Iteration 238, loss = 0.55164698\n",
      "Iteration 239, loss = 0.55121854\n",
      "Iteration 240, loss = 0.55080248\n",
      "Iteration 241, loss = 0.55039016\n",
      "Iteration 242, loss = 0.54997558\n",
      "Iteration 243, loss = 0.54956240\n",
      "Iteration 244, loss = 0.54915584\n",
      "Iteration 245, loss = 0.54875266\n",
      "Iteration 246, loss = 0.54834788\n",
      "Iteration 247, loss = 0.54795080\n",
      "Iteration 248, loss = 0.54755859\n",
      "Iteration 249, loss = 0.54715858\n",
      "Iteration 250, loss = 0.54676197\n",
      "Iteration 251, loss = 0.54637527\n",
      "Iteration 252, loss = 0.54598403\n",
      "Iteration 253, loss = 0.54560915\n",
      "Iteration 254, loss = 0.54522126\n",
      "Iteration 255, loss = 0.54484391\n",
      "Iteration 256, loss = 0.54447064\n",
      "Iteration 257, loss = 0.54409639\n",
      "Iteration 258, loss = 0.54372437\n",
      "Iteration 259, loss = 0.54335940\n",
      "Iteration 260, loss = 0.54299272\n",
      "Iteration 261, loss = 0.54262689\n",
      "Iteration 262, loss = 0.54226759\n",
      "Iteration 263, loss = 0.54190722\n",
      "Iteration 264, loss = 0.54155414\n",
      "Iteration 265, loss = 0.54120610\n",
      "Iteration 266, loss = 0.54085499\n",
      "Iteration 267, loss = 0.54051113\n",
      "Iteration 268, loss = 0.54016735\n",
      "Iteration 269, loss = 0.53982573\n",
      "Iteration 270, loss = 0.53948302\n",
      "Iteration 271, loss = 0.53914807\n",
      "Iteration 272, loss = 0.53882912\n",
      "Iteration 273, loss = 0.53848923\n",
      "Iteration 274, loss = 0.53815979\n",
      "Iteration 275, loss = 0.53783436\n",
      "Iteration 276, loss = 0.53751663\n",
      "Iteration 277, loss = 0.53719120\n",
      "Iteration 278, loss = 0.53687923\n",
      "Iteration 279, loss = 0.53656685\n",
      "Iteration 280, loss = 0.53624846\n",
      "Iteration 281, loss = 0.53594472\n",
      "Iteration 282, loss = 0.53564182\n",
      "Iteration 283, loss = 0.53533169\n",
      "Iteration 284, loss = 0.53504163\n",
      "Iteration 285, loss = 0.53473909\n",
      "Iteration 286, loss = 0.53444005\n",
      "Iteration 287, loss = 0.53415412\n",
      "Iteration 288, loss = 0.53386436\n",
      "Iteration 289, loss = 0.53358846\n",
      "Iteration 290, loss = 0.53331134\n",
      "Iteration 291, loss = 0.53301631\n",
      "Iteration 292, loss = 0.53275797\n",
      "Iteration 293, loss = 0.53247258\n",
      "Iteration 294, loss = 0.53219479\n",
      "Iteration 295, loss = 0.53192647\n",
      "Iteration 296, loss = 0.53166517\n",
      "Iteration 297, loss = 0.53139861\n",
      "Iteration 298, loss = 0.53113601\n",
      "Iteration 299, loss = 0.53088081\n",
      "Iteration 300, loss = 0.53062091\n",
      "Iteration 301, loss = 0.53036771\n",
      "Iteration 302, loss = 0.53010759\n",
      "Iteration 303, loss = 0.52986082\n",
      "Iteration 304, loss = 0.52961003\n",
      "Iteration 305, loss = 0.52936300\n",
      "Iteration 306, loss = 0.52911977\n",
      "Iteration 307, loss = 0.52887675\n",
      "Iteration 308, loss = 0.52863610\n",
      "Iteration 309, loss = 0.52840022\n",
      "Iteration 310, loss = 0.52816396\n",
      "Iteration 311, loss = 0.52792949\n",
      "Iteration 312, loss = 0.52769698\n",
      "Iteration 313, loss = 0.52747041\n",
      "Iteration 314, loss = 0.52723892\n",
      "Iteration 315, loss = 0.52701633\n",
      "Iteration 316, loss = 0.52679191\n",
      "Iteration 317, loss = 0.52657790\n",
      "Iteration 318, loss = 0.52635721\n",
      "Iteration 319, loss = 0.52614642\n",
      "Iteration 320, loss = 0.52592590\n",
      "Iteration 321, loss = 0.52571893\n",
      "Iteration 322, loss = 0.52550545\n",
      "Iteration 323, loss = 0.52529444\n",
      "Iteration 324, loss = 0.52509125\n",
      "Iteration 325, loss = 0.52488676\n",
      "Iteration 326, loss = 0.52468561\n",
      "Iteration 327, loss = 0.52448594\n",
      "Iteration 328, loss = 0.52428832\n",
      "Iteration 329, loss = 0.52409166\n",
      "Iteration 330, loss = 0.52389897\n",
      "Iteration 331, loss = 0.52370701\n",
      "Iteration 332, loss = 0.52352127\n",
      "Iteration 333, loss = 0.52333415\n",
      "Iteration 334, loss = 0.52314910\n",
      "Iteration 335, loss = 0.52296464\n",
      "Iteration 336, loss = 0.52277592\n",
      "Iteration 337, loss = 0.52259770\n",
      "Iteration 338, loss = 0.52241946\n",
      "Iteration 339, loss = 0.52225502\n",
      "Iteration 340, loss = 0.52207138\n",
      "Iteration 341, loss = 0.52190160\n",
      "Iteration 342, loss = 0.52173382\n",
      "Iteration 343, loss = 0.52156504\n",
      "Iteration 344, loss = 0.52139946\n",
      "Iteration 345, loss = 0.52124001\n",
      "Iteration 346, loss = 0.52107501\n",
      "Iteration 347, loss = 0.52091528\n",
      "Iteration 348, loss = 0.52075624\n",
      "Iteration 349, loss = 0.52060102\n",
      "Iteration 350, loss = 0.52044796\n",
      "Iteration 351, loss = 0.52029873\n",
      "Iteration 352, loss = 0.52014962\n",
      "Iteration 353, loss = 0.51999759\n",
      "Iteration 354, loss = 0.51984356\n",
      "Iteration 355, loss = 0.51971606\n",
      "Iteration 356, loss = 0.51955631\n",
      "Iteration 357, loss = 0.51941671\n",
      "Iteration 358, loss = 0.51927671\n",
      "Iteration 359, loss = 0.51913385\n",
      "Iteration 360, loss = 0.51899753\n",
      "Iteration 361, loss = 0.51886336\n",
      "Iteration 362, loss = 0.51872845\n",
      "Iteration 363, loss = 0.51859679\n",
      "Iteration 364, loss = 0.51846253\n",
      "Iteration 365, loss = 0.51833727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 366, loss = 0.51821095\n",
      "Iteration 367, loss = 0.51808257\n",
      "Iteration 368, loss = 0.51795687\n",
      "Iteration 369, loss = 0.51783540\n",
      "Iteration 370, loss = 0.51771578\n",
      "Iteration 371, loss = 0.51760610\n",
      "Iteration 372, loss = 0.51747826\n",
      "Iteration 373, loss = 0.51735872\n",
      "Iteration 374, loss = 0.51724474\n",
      "Iteration 375, loss = 0.51713087\n",
      "Iteration 376, loss = 0.51702496\n",
      "Iteration 377, loss = 0.51690490\n",
      "Iteration 378, loss = 0.51679189\n",
      "Iteration 379, loss = 0.51668677\n",
      "Iteration 380, loss = 0.51658411\n",
      "Iteration 381, loss = 0.51647567\n",
      "Iteration 382, loss = 0.51637414\n",
      "Iteration 383, loss = 0.51626813\n",
      "Iteration 384, loss = 0.51616506\n",
      "Iteration 385, loss = 0.51607063\n",
      "Iteration 386, loss = 0.51596511\n",
      "Iteration 387, loss = 0.51587970\n",
      "Iteration 388, loss = 0.51578103\n",
      "Iteration 389, loss = 0.51568577\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74969870\n",
      "Iteration 2, loss = 0.74493354\n",
      "Iteration 3, loss = 0.74037079\n",
      "Iteration 4, loss = 0.73599192\n",
      "Iteration 5, loss = 0.73180007\n",
      "Iteration 6, loss = 0.72778373\n",
      "Iteration 7, loss = 0.72394651\n",
      "Iteration 8, loss = 0.72029849\n",
      "Iteration 9, loss = 0.71680870\n",
      "Iteration 10, loss = 0.71347888\n",
      "Iteration 11, loss = 0.71033034\n",
      "Iteration 12, loss = 0.70734475\n",
      "Iteration 13, loss = 0.70451618\n",
      "Iteration 14, loss = 0.70182801\n",
      "Iteration 15, loss = 0.69927893\n",
      "Iteration 16, loss = 0.69688338\n",
      "Iteration 17, loss = 0.69460154\n",
      "Iteration 18, loss = 0.69245936\n",
      "Iteration 19, loss = 0.69043583\n",
      "Iteration 20, loss = 0.68853777\n",
      "Iteration 21, loss = 0.68674021\n",
      "Iteration 22, loss = 0.68505385\n",
      "Iteration 23, loss = 0.68347117\n",
      "Iteration 24, loss = 0.68197733\n",
      "Iteration 25, loss = 0.68057119\n",
      "Iteration 26, loss = 0.67922222\n",
      "Iteration 27, loss = 0.67796006\n",
      "Iteration 28, loss = 0.67677266\n",
      "Iteration 29, loss = 0.67563399\n",
      "Iteration 30, loss = 0.67455997\n",
      "Iteration 31, loss = 0.67353829\n",
      "Iteration 32, loss = 0.67255593\n",
      "Iteration 33, loss = 0.67161607\n",
      "Iteration 34, loss = 0.67069766\n",
      "Iteration 35, loss = 0.66981908\n",
      "Iteration 36, loss = 0.66896507\n",
      "Iteration 37, loss = 0.66813003\n",
      "Iteration 38, loss = 0.66731765\n",
      "Iteration 39, loss = 0.66651769\n",
      "Iteration 40, loss = 0.66573483\n",
      "Iteration 41, loss = 0.66495727\n",
      "Iteration 42, loss = 0.66419018\n",
      "Iteration 43, loss = 0.66343055\n",
      "Iteration 44, loss = 0.66267358\n",
      "Iteration 45, loss = 0.66192341\n",
      "Iteration 46, loss = 0.66116911\n",
      "Iteration 47, loss = 0.66042169\n",
      "Iteration 48, loss = 0.65967231\n",
      "Iteration 49, loss = 0.65892042\n",
      "Iteration 50, loss = 0.65816838\n",
      "Iteration 51, loss = 0.65741370\n",
      "Iteration 52, loss = 0.65665501\n",
      "Iteration 53, loss = 0.65589415\n",
      "Iteration 54, loss = 0.65513199\n",
      "Iteration 55, loss = 0.65436501\n",
      "Iteration 56, loss = 0.65359285\n",
      "Iteration 57, loss = 0.65281899\n",
      "Iteration 58, loss = 0.65204242\n",
      "Iteration 59, loss = 0.65126345\n",
      "Iteration 60, loss = 0.65047701\n",
      "Iteration 61, loss = 0.64969185\n",
      "Iteration 62, loss = 0.64889959\n",
      "Iteration 63, loss = 0.64810744\n",
      "Iteration 64, loss = 0.64731055\n",
      "Iteration 65, loss = 0.64651680\n",
      "Iteration 66, loss = 0.64571744\n",
      "Iteration 67, loss = 0.64491320\n",
      "Iteration 68, loss = 0.64410824\n",
      "Iteration 69, loss = 0.64330099\n",
      "Iteration 70, loss = 0.64248984\n",
      "Iteration 71, loss = 0.64167747\n",
      "Iteration 72, loss = 0.64086336\n",
      "Iteration 73, loss = 0.64004550\n",
      "Iteration 74, loss = 0.63922598\n",
      "Iteration 75, loss = 0.63840196\n",
      "Iteration 76, loss = 0.63757222\n",
      "Iteration 77, loss = 0.63674635\n",
      "Iteration 78, loss = 0.63591224\n",
      "Iteration 79, loss = 0.63507941\n",
      "Iteration 80, loss = 0.63424638\n",
      "Iteration 81, loss = 0.63341010\n",
      "Iteration 82, loss = 0.63257441\n",
      "Iteration 83, loss = 0.63174262\n",
      "Iteration 84, loss = 0.63091141\n",
      "Iteration 85, loss = 0.63007003\n",
      "Iteration 86, loss = 0.62923315\n",
      "Iteration 87, loss = 0.62839716\n",
      "Iteration 88, loss = 0.62755692\n",
      "Iteration 89, loss = 0.62671472\n",
      "Iteration 90, loss = 0.62587804\n",
      "Iteration 91, loss = 0.62503868\n",
      "Iteration 92, loss = 0.62419931\n",
      "Iteration 93, loss = 0.62336095\n",
      "Iteration 94, loss = 0.62252359\n",
      "Iteration 95, loss = 0.62168051\n",
      "Iteration 96, loss = 0.62085252\n",
      "Iteration 97, loss = 0.62000950\n",
      "Iteration 98, loss = 0.61917994\n",
      "Iteration 99, loss = 0.61833640\n",
      "Iteration 100, loss = 0.61750385\n",
      "Iteration 101, loss = 0.61666638\n",
      "Iteration 102, loss = 0.61582599\n",
      "Iteration 103, loss = 0.61498384\n",
      "Iteration 104, loss = 0.61414552\n",
      "Iteration 105, loss = 0.61330434\n",
      "Iteration 106, loss = 0.61246265\n",
      "Iteration 107, loss = 0.61162407\n",
      "Iteration 108, loss = 0.61078641\n",
      "Iteration 109, loss = 0.60994588\n",
      "Iteration 110, loss = 0.60910531\n",
      "Iteration 111, loss = 0.60826413\n",
      "Iteration 112, loss = 0.60742177\n",
      "Iteration 113, loss = 0.60657871\n",
      "Iteration 114, loss = 0.60573650\n",
      "Iteration 115, loss = 0.60489909\n",
      "Iteration 116, loss = 0.60406030\n",
      "Iteration 117, loss = 0.60321383\n",
      "Iteration 118, loss = 0.60237937\n",
      "Iteration 119, loss = 0.60153112\n",
      "Iteration 120, loss = 0.60069090\n",
      "Iteration 121, loss = 0.59985060\n",
      "Iteration 122, loss = 0.59901153\n",
      "Iteration 123, loss = 0.59817052\n",
      "Iteration 124, loss = 0.59733154\n",
      "Iteration 125, loss = 0.59649532\n",
      "Iteration 126, loss = 0.59566190\n",
      "Iteration 127, loss = 0.59482413\n",
      "Iteration 128, loss = 0.59399222\n",
      "Iteration 129, loss = 0.59315805\n",
      "Iteration 130, loss = 0.59233549\n",
      "Iteration 131, loss = 0.59150305\n",
      "Iteration 132, loss = 0.59067884\n",
      "Iteration 133, loss = 0.58985123\n",
      "Iteration 134, loss = 0.58902737\n",
      "Iteration 135, loss = 0.58820627\n",
      "Iteration 136, loss = 0.58738150\n",
      "Iteration 137, loss = 0.58655630\n",
      "Iteration 138, loss = 0.58573532\n",
      "Iteration 139, loss = 0.58491798\n",
      "Iteration 140, loss = 0.58410424\n",
      "Iteration 141, loss = 0.58329209\n",
      "Iteration 142, loss = 0.58247676\n",
      "Iteration 143, loss = 0.58166986\n",
      "Iteration 144, loss = 0.58085677\n",
      "Iteration 145, loss = 0.58004876\n",
      "Iteration 146, loss = 0.57923968\n",
      "Iteration 147, loss = 0.57843513\n",
      "Iteration 148, loss = 0.57762876\n",
      "Iteration 149, loss = 0.57682343\n",
      "Iteration 150, loss = 0.57601985\n",
      "Iteration 151, loss = 0.57522310\n",
      "Iteration 152, loss = 0.57442443\n",
      "Iteration 153, loss = 0.57363245\n",
      "Iteration 154, loss = 0.57284543\n",
      "Iteration 155, loss = 0.57205966\n",
      "Iteration 156, loss = 0.57127815\n",
      "Iteration 157, loss = 0.57050066\n",
      "Iteration 158, loss = 0.56972994\n",
      "Iteration 159, loss = 0.56895311\n",
      "Iteration 160, loss = 0.56818240\n",
      "Iteration 161, loss = 0.56741489\n",
      "Iteration 162, loss = 0.56665048\n",
      "Iteration 163, loss = 0.56588831\n",
      "Iteration 164, loss = 0.56513015\n",
      "Iteration 165, loss = 0.56437579\n",
      "Iteration 166, loss = 0.56362182\n",
      "Iteration 167, loss = 0.56286754\n",
      "Iteration 168, loss = 0.56211925\n",
      "Iteration 169, loss = 0.56137076\n",
      "Iteration 170, loss = 0.56062438\n",
      "Iteration 171, loss = 0.55988297\n",
      "Iteration 172, loss = 0.55914649\n",
      "Iteration 173, loss = 0.55840997\n",
      "Iteration 174, loss = 0.55767844\n",
      "Iteration 175, loss = 0.55695613\n",
      "Iteration 176, loss = 0.55622881\n",
      "Iteration 177, loss = 0.55551012\n",
      "Iteration 178, loss = 0.55479182\n",
      "Iteration 179, loss = 0.55408262\n",
      "Iteration 180, loss = 0.55337546\n",
      "Iteration 181, loss = 0.55267064\n",
      "Iteration 182, loss = 0.55196811\n",
      "Iteration 183, loss = 0.55127364\n",
      "Iteration 184, loss = 0.55058020\n",
      "Iteration 185, loss = 0.54989763\n",
      "Iteration 186, loss = 0.54922265\n",
      "Iteration 187, loss = 0.54854052\n",
      "Iteration 188, loss = 0.54785314\n",
      "Iteration 189, loss = 0.54718218\n",
      "Iteration 190, loss = 0.54650703\n",
      "Iteration 191, loss = 0.54584178\n",
      "Iteration 192, loss = 0.54517465\n",
      "Iteration 193, loss = 0.54452029\n",
      "Iteration 194, loss = 0.54386604\n",
      "Iteration 195, loss = 0.54320764\n",
      "Iteration 196, loss = 0.54256063\n",
      "Iteration 197, loss = 0.54191922\n",
      "Iteration 198, loss = 0.54127971\n",
      "Iteration 199, loss = 0.54064039\n",
      "Iteration 200, loss = 0.54000756\n",
      "Iteration 201, loss = 0.53938128\n",
      "Iteration 202, loss = 0.53875639\n",
      "Iteration 203, loss = 0.53813797\n",
      "Iteration 204, loss = 0.53752332\n",
      "Iteration 205, loss = 0.53690687\n",
      "Iteration 206, loss = 0.53629673\n",
      "Iteration 207, loss = 0.53570666\n",
      "Iteration 208, loss = 0.53510097\n",
      "Iteration 209, loss = 0.53449898\n",
      "Iteration 210, loss = 0.53391001\n",
      "Iteration 211, loss = 0.53331622\n",
      "Iteration 212, loss = 0.53273310\n",
      "Iteration 213, loss = 0.53215087\n",
      "Iteration 214, loss = 0.53156969\n",
      "Iteration 215, loss = 0.53099533\n",
      "Iteration 216, loss = 0.53043008\n",
      "Iteration 217, loss = 0.52986429\n",
      "Iteration 218, loss = 0.52930834\n",
      "Iteration 219, loss = 0.52874603\n",
      "Iteration 220, loss = 0.52819488\n",
      "Iteration 221, loss = 0.52764295\n",
      "Iteration 222, loss = 0.52709787\n",
      "Iteration 223, loss = 0.52655312\n",
      "Iteration 224, loss = 0.52601521\n",
      "Iteration 225, loss = 0.52548040\n",
      "Iteration 226, loss = 0.52495371\n",
      "Iteration 227, loss = 0.52443698\n",
      "Iteration 228, loss = 0.52391721\n",
      "Iteration 229, loss = 0.52340274\n",
      "Iteration 230, loss = 0.52289157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 231, loss = 0.52238319\n",
      "Iteration 232, loss = 0.52188021\n",
      "Iteration 233, loss = 0.52138438\n",
      "Iteration 234, loss = 0.52089331\n",
      "Iteration 235, loss = 0.52039828\n",
      "Iteration 236, loss = 0.51991639\n",
      "Iteration 237, loss = 0.51942955\n",
      "Iteration 238, loss = 0.51895508\n",
      "Iteration 239, loss = 0.51847432\n",
      "Iteration 240, loss = 0.51801107\n",
      "Iteration 241, loss = 0.51754582\n",
      "Iteration 242, loss = 0.51708384\n",
      "Iteration 243, loss = 0.51662733\n",
      "Iteration 244, loss = 0.51617422\n",
      "Iteration 245, loss = 0.51572674\n",
      "Iteration 246, loss = 0.51527982\n",
      "Iteration 247, loss = 0.51483940\n",
      "Iteration 248, loss = 0.51440305\n",
      "Iteration 249, loss = 0.51396655\n",
      "Iteration 250, loss = 0.51352910\n",
      "Iteration 251, loss = 0.51310256\n",
      "Iteration 252, loss = 0.51267641\n",
      "Iteration 253, loss = 0.51226232\n",
      "Iteration 254, loss = 0.51184192\n",
      "Iteration 255, loss = 0.51143147\n",
      "Iteration 256, loss = 0.51102236\n",
      "Iteration 257, loss = 0.51061582\n",
      "Iteration 258, loss = 0.51021061\n",
      "Iteration 259, loss = 0.50981258\n",
      "Iteration 260, loss = 0.50941513\n",
      "Iteration 261, loss = 0.50902182\n",
      "Iteration 262, loss = 0.50863272\n",
      "Iteration 263, loss = 0.50824613\n",
      "Iteration 264, loss = 0.50786578\n",
      "Iteration 265, loss = 0.50749017\n",
      "Iteration 266, loss = 0.50711315\n",
      "Iteration 267, loss = 0.50674796\n",
      "Iteration 268, loss = 0.50638065\n",
      "Iteration 269, loss = 0.50601779\n",
      "Iteration 270, loss = 0.50565556\n",
      "Iteration 271, loss = 0.50529859\n",
      "Iteration 272, loss = 0.50495483\n",
      "Iteration 273, loss = 0.50460132\n",
      "Iteration 274, loss = 0.50425230\n",
      "Iteration 275, loss = 0.50391203\n",
      "Iteration 276, loss = 0.50357407\n",
      "Iteration 277, loss = 0.50323618\n",
      "Iteration 278, loss = 0.50290780\n",
      "Iteration 279, loss = 0.50258176\n",
      "Iteration 280, loss = 0.50225170\n",
      "Iteration 281, loss = 0.50193093\n",
      "Iteration 282, loss = 0.50161404\n",
      "Iteration 283, loss = 0.50129146\n",
      "Iteration 284, loss = 0.50099187\n",
      "Iteration 285, loss = 0.50067774\n",
      "Iteration 286, loss = 0.50036927\n",
      "Iteration 287, loss = 0.50007181\n",
      "Iteration 288, loss = 0.49977105\n",
      "Iteration 289, loss = 0.49948781\n",
      "Iteration 290, loss = 0.49920773\n",
      "Iteration 291, loss = 0.49890063\n",
      "Iteration 292, loss = 0.49863050\n",
      "Iteration 293, loss = 0.49834400\n",
      "Iteration 294, loss = 0.49806050\n",
      "Iteration 295, loss = 0.49778637\n",
      "Iteration 296, loss = 0.49752345\n",
      "Iteration 297, loss = 0.49725126\n",
      "Iteration 298, loss = 0.49698667\n",
      "Iteration 299, loss = 0.49672795\n",
      "Iteration 300, loss = 0.49646591\n",
      "Iteration 301, loss = 0.49620904\n",
      "Iteration 302, loss = 0.49595174\n",
      "Iteration 303, loss = 0.49570101\n",
      "Iteration 304, loss = 0.49545375\n",
      "Iteration 305, loss = 0.49520468\n",
      "Iteration 306, loss = 0.49496444\n",
      "Iteration 307, loss = 0.49471952\n",
      "Iteration 308, loss = 0.49448239\n",
      "Iteration 309, loss = 0.49425109\n",
      "Iteration 310, loss = 0.49401186\n",
      "Iteration 311, loss = 0.49378478\n",
      "Iteration 312, loss = 0.49355362\n",
      "Iteration 313, loss = 0.49333024\n",
      "Iteration 314, loss = 0.49310162\n",
      "Iteration 315, loss = 0.49288461\n",
      "Iteration 316, loss = 0.49266481\n",
      "Iteration 317, loss = 0.49245803\n",
      "Iteration 318, loss = 0.49223975\n",
      "Iteration 319, loss = 0.49203258\n",
      "Iteration 320, loss = 0.49182098\n",
      "Iteration 321, loss = 0.49161592\n",
      "Iteration 322, loss = 0.49141143\n",
      "Iteration 323, loss = 0.49120774\n",
      "Iteration 324, loss = 0.49101013\n",
      "Iteration 325, loss = 0.49081241\n",
      "Iteration 326, loss = 0.49061953\n",
      "Iteration 327, loss = 0.49042793\n",
      "Iteration 328, loss = 0.49023653\n",
      "Iteration 329, loss = 0.49005360\n",
      "Iteration 330, loss = 0.48986471\n",
      "Iteration 331, loss = 0.48968277\n",
      "Iteration 332, loss = 0.48950226\n",
      "Iteration 333, loss = 0.48932442\n",
      "Iteration 334, loss = 0.48915092\n",
      "Iteration 335, loss = 0.48897936\n",
      "Iteration 336, loss = 0.48879633\n",
      "Iteration 337, loss = 0.48863112\n",
      "Iteration 338, loss = 0.48846201\n",
      "Iteration 339, loss = 0.48830836\n",
      "Iteration 340, loss = 0.48813581\n",
      "Iteration 341, loss = 0.48797447\n",
      "Iteration 342, loss = 0.48781742\n",
      "Iteration 343, loss = 0.48766015\n",
      "Iteration 344, loss = 0.48750782\n",
      "Iteration 345, loss = 0.48735772\n",
      "Iteration 346, loss = 0.48720345\n",
      "Iteration 347, loss = 0.48705593\n",
      "Iteration 348, loss = 0.48691082\n",
      "Iteration 349, loss = 0.48676459\n",
      "Iteration 350, loss = 0.48662347\n",
      "Iteration 351, loss = 0.48648743\n",
      "Iteration 352, loss = 0.48635243\n",
      "Iteration 353, loss = 0.48621703\n",
      "Iteration 354, loss = 0.48607456\n",
      "Iteration 355, loss = 0.48595815\n",
      "Iteration 356, loss = 0.48581199\n",
      "Iteration 357, loss = 0.48568534\n",
      "Iteration 358, loss = 0.48555623\n",
      "Iteration 359, loss = 0.48542905\n",
      "Iteration 360, loss = 0.48530444\n",
      "Iteration 361, loss = 0.48518586\n",
      "Iteration 362, loss = 0.48506025\n",
      "Iteration 363, loss = 0.48494288\n",
      "Iteration 364, loss = 0.48482199\n",
      "Iteration 365, loss = 0.48470851\n",
      "Iteration 366, loss = 0.48459458\n",
      "Iteration 367, loss = 0.48448521\n",
      "Iteration 368, loss = 0.48437038\n",
      "Iteration 369, loss = 0.48426658\n",
      "Iteration 370, loss = 0.48415804\n",
      "Iteration 371, loss = 0.48406635\n",
      "Iteration 372, loss = 0.48394536\n",
      "Iteration 373, loss = 0.48384024\n",
      "Iteration 374, loss = 0.48373957\n",
      "Iteration 375, loss = 0.48363881\n",
      "Iteration 376, loss = 0.48354934\n",
      "Iteration 377, loss = 0.48344254\n",
      "Iteration 378, loss = 0.48334133\n",
      "Iteration 379, loss = 0.48324712\n",
      "Iteration 380, loss = 0.48316084\n",
      "Iteration 381, loss = 0.48306527\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76535648\n",
      "Iteration 2, loss = 0.75971304\n",
      "Iteration 3, loss = 0.75427926\n",
      "Iteration 4, loss = 0.74904084\n",
      "Iteration 5, loss = 0.74399433\n",
      "Iteration 6, loss = 0.73912894\n",
      "Iteration 7, loss = 0.73445138\n",
      "Iteration 8, loss = 0.72996593\n",
      "Iteration 9, loss = 0.72565270\n",
      "Iteration 10, loss = 0.72150844\n",
      "Iteration 11, loss = 0.71755042\n",
      "Iteration 12, loss = 0.71376726\n",
      "Iteration 13, loss = 0.71015868\n",
      "Iteration 14, loss = 0.70670288\n",
      "Iteration 15, loss = 0.70340125\n",
      "Iteration 16, loss = 0.70026535\n",
      "Iteration 17, loss = 0.69726373\n",
      "Iteration 18, loss = 0.69443255\n",
      "Iteration 19, loss = 0.69174188\n",
      "Iteration 20, loss = 0.68920707\n",
      "Iteration 21, loss = 0.68679029\n",
      "Iteration 22, loss = 0.68451150\n",
      "Iteration 23, loss = 0.68236284\n",
      "Iteration 24, loss = 0.68033326\n",
      "Iteration 25, loss = 0.67841588\n",
      "Iteration 26, loss = 0.67659272\n",
      "Iteration 27, loss = 0.67488159\n",
      "Iteration 28, loss = 0.67327552\n",
      "Iteration 29, loss = 0.67175118\n",
      "Iteration 30, loss = 0.67031947\n",
      "Iteration 31, loss = 0.66897234\n",
      "Iteration 32, loss = 0.66769596\n",
      "Iteration 33, loss = 0.66648685\n",
      "Iteration 34, loss = 0.66532743\n",
      "Iteration 35, loss = 0.66423581\n",
      "Iteration 36, loss = 0.66319315\n",
      "Iteration 37, loss = 0.66219363\n",
      "Iteration 38, loss = 0.66124371\n",
      "Iteration 39, loss = 0.66032392\n",
      "Iteration 40, loss = 0.65944400\n",
      "Iteration 41, loss = 0.65858416\n",
      "Iteration 42, loss = 0.65775198\n",
      "Iteration 43, loss = 0.65694259\n",
      "Iteration 44, loss = 0.65614370\n",
      "Iteration 45, loss = 0.65536599\n",
      "Iteration 46, loss = 0.65459202\n",
      "Iteration 47, loss = 0.65383270\n",
      "Iteration 48, loss = 0.65308143\n",
      "Iteration 49, loss = 0.65233061\n",
      "Iteration 50, loss = 0.65158696\n",
      "Iteration 51, loss = 0.65084417\n",
      "Iteration 52, loss = 0.65010022\n",
      "Iteration 53, loss = 0.64935647\n",
      "Iteration 54, loss = 0.64861465\n",
      "Iteration 55, loss = 0.64787173\n",
      "Iteration 56, loss = 0.64712344\n",
      "Iteration 57, loss = 0.64637550\n",
      "Iteration 58, loss = 0.64562375\n",
      "Iteration 59, loss = 0.64486967\n",
      "Iteration 60, loss = 0.64411053\n",
      "Iteration 61, loss = 0.64335024\n",
      "Iteration 62, loss = 0.64258309\n",
      "Iteration 63, loss = 0.64181525\n",
      "Iteration 64, loss = 0.64104283\n",
      "Iteration 65, loss = 0.64026823\n",
      "Iteration 66, loss = 0.63948773\n",
      "Iteration 67, loss = 0.63870469\n",
      "Iteration 68, loss = 0.63791651\n",
      "Iteration 69, loss = 0.63712492\n",
      "Iteration 70, loss = 0.63632913\n",
      "Iteration 71, loss = 0.63553000\n",
      "Iteration 72, loss = 0.63472496\n",
      "Iteration 73, loss = 0.63391882\n",
      "Iteration 74, loss = 0.63310765\n",
      "Iteration 75, loss = 0.63229122\n",
      "Iteration 76, loss = 0.63147345\n",
      "Iteration 77, loss = 0.63065300\n",
      "Iteration 78, loss = 0.62982623\n",
      "Iteration 79, loss = 0.62899994\n",
      "Iteration 80, loss = 0.62816997\n",
      "Iteration 81, loss = 0.62734114\n",
      "Iteration 82, loss = 0.62650763\n",
      "Iteration 83, loss = 0.62567655\n",
      "Iteration 84, loss = 0.62484954\n",
      "Iteration 85, loss = 0.62401018\n",
      "Iteration 86, loss = 0.62317639\n",
      "Iteration 87, loss = 0.62234391\n",
      "Iteration 88, loss = 0.62150423\n",
      "Iteration 89, loss = 0.62066317\n",
      "Iteration 90, loss = 0.61982654\n",
      "Iteration 91, loss = 0.61898540\n",
      "Iteration 92, loss = 0.61814546\n",
      "Iteration 93, loss = 0.61730454\n",
      "Iteration 94, loss = 0.61646067\n",
      "Iteration 95, loss = 0.61561137\n",
      "Iteration 96, loss = 0.61477812\n",
      "Iteration 97, loss = 0.61392953\n",
      "Iteration 98, loss = 0.61309430\n",
      "Iteration 99, loss = 0.61224346\n",
      "Iteration 100, loss = 0.61140470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 101, loss = 0.61055985\n",
      "Iteration 102, loss = 0.60971590\n",
      "Iteration 103, loss = 0.60886761\n",
      "Iteration 104, loss = 0.60802219\n",
      "Iteration 105, loss = 0.60717787\n",
      "Iteration 106, loss = 0.60633218\n",
      "Iteration 107, loss = 0.60548818\n",
      "Iteration 108, loss = 0.60464578\n",
      "Iteration 109, loss = 0.60380028\n",
      "Iteration 110, loss = 0.60295550\n",
      "Iteration 111, loss = 0.60210615\n",
      "Iteration 112, loss = 0.60126032\n",
      "Iteration 113, loss = 0.60041913\n",
      "Iteration 114, loss = 0.59957178\n",
      "Iteration 115, loss = 0.59873208\n",
      "Iteration 116, loss = 0.59788939\n",
      "Iteration 117, loss = 0.59704911\n",
      "Iteration 118, loss = 0.59621383\n",
      "Iteration 119, loss = 0.59536755\n",
      "Iteration 120, loss = 0.59452405\n",
      "Iteration 121, loss = 0.59368682\n",
      "Iteration 122, loss = 0.59285291\n",
      "Iteration 123, loss = 0.59201582\n",
      "Iteration 124, loss = 0.59117762\n",
      "Iteration 125, loss = 0.59034168\n",
      "Iteration 126, loss = 0.58951133\n",
      "Iteration 127, loss = 0.58867369\n",
      "Iteration 128, loss = 0.58784471\n",
      "Iteration 129, loss = 0.58700876\n",
      "Iteration 130, loss = 0.58618387\n",
      "Iteration 131, loss = 0.58535283\n",
      "Iteration 132, loss = 0.58452591\n",
      "Iteration 133, loss = 0.58369445\n",
      "Iteration 134, loss = 0.58287038\n",
      "Iteration 135, loss = 0.58204575\n",
      "Iteration 136, loss = 0.58121913\n",
      "Iteration 137, loss = 0.58039157\n",
      "Iteration 138, loss = 0.57956703\n",
      "Iteration 139, loss = 0.57874779\n",
      "Iteration 140, loss = 0.57793176\n",
      "Iteration 141, loss = 0.57711619\n",
      "Iteration 142, loss = 0.57629897\n",
      "Iteration 143, loss = 0.57548899\n",
      "Iteration 144, loss = 0.57467466\n",
      "Iteration 145, loss = 0.57386717\n",
      "Iteration 146, loss = 0.57305536\n",
      "Iteration 147, loss = 0.57225355\n",
      "Iteration 148, loss = 0.57144597\n",
      "Iteration 149, loss = 0.57064278\n",
      "Iteration 150, loss = 0.56984471\n",
      "Iteration 151, loss = 0.56904872\n",
      "Iteration 152, loss = 0.56825085\n",
      "Iteration 153, loss = 0.56745942\n",
      "Iteration 154, loss = 0.56666985\n",
      "Iteration 155, loss = 0.56588457\n",
      "Iteration 156, loss = 0.56510239\n",
      "Iteration 157, loss = 0.56432115\n",
      "Iteration 158, loss = 0.56354341\n",
      "Iteration 159, loss = 0.56276475\n",
      "Iteration 160, loss = 0.56198697\n",
      "Iteration 161, loss = 0.56121358\n",
      "Iteration 162, loss = 0.56044560\n",
      "Iteration 163, loss = 0.55967262\n",
      "Iteration 164, loss = 0.55890377\n",
      "Iteration 165, loss = 0.55813740\n",
      "Iteration 166, loss = 0.55737504\n",
      "Iteration 167, loss = 0.55661309\n",
      "Iteration 168, loss = 0.55584897\n",
      "Iteration 169, loss = 0.55509138\n",
      "Iteration 170, loss = 0.55433527\n",
      "Iteration 171, loss = 0.55358306\n",
      "Iteration 172, loss = 0.55283992\n",
      "Iteration 173, loss = 0.55208950\n",
      "Iteration 174, loss = 0.55134741\n",
      "Iteration 175, loss = 0.55061111\n",
      "Iteration 176, loss = 0.54987096\n",
      "Iteration 177, loss = 0.54914107\n",
      "Iteration 178, loss = 0.54841016\n",
      "Iteration 179, loss = 0.54768467\n",
      "Iteration 180, loss = 0.54696869\n",
      "Iteration 181, loss = 0.54624615\n",
      "Iteration 182, loss = 0.54553112\n",
      "Iteration 183, loss = 0.54482550\n",
      "Iteration 184, loss = 0.54411719\n",
      "Iteration 185, loss = 0.54341648\n",
      "Iteration 186, loss = 0.54272947\n",
      "Iteration 187, loss = 0.54203221\n",
      "Iteration 188, loss = 0.54134071\n",
      "Iteration 189, loss = 0.54065430\n",
      "Iteration 190, loss = 0.53996723\n",
      "Iteration 191, loss = 0.53928777\n",
      "Iteration 192, loss = 0.53860547\n",
      "Iteration 193, loss = 0.53793561\n",
      "Iteration 194, loss = 0.53727214\n",
      "Iteration 195, loss = 0.53660392\n",
      "Iteration 196, loss = 0.53594564\n",
      "Iteration 197, loss = 0.53529193\n",
      "Iteration 198, loss = 0.53464118\n",
      "Iteration 199, loss = 0.53399438\n",
      "Iteration 200, loss = 0.53334583\n",
      "Iteration 201, loss = 0.53270732\n",
      "Iteration 202, loss = 0.53206965\n",
      "Iteration 203, loss = 0.53143667\n",
      "Iteration 204, loss = 0.53080828\n",
      "Iteration 205, loss = 0.53018005\n",
      "Iteration 206, loss = 0.52955585\n",
      "Iteration 207, loss = 0.52894355\n",
      "Iteration 208, loss = 0.52832887\n",
      "Iteration 209, loss = 0.52771480\n",
      "Iteration 210, loss = 0.52711286\n",
      "Iteration 211, loss = 0.52650739\n",
      "Iteration 212, loss = 0.52591456\n",
      "Iteration 213, loss = 0.52531383\n",
      "Iteration 214, loss = 0.52472128\n",
      "Iteration 215, loss = 0.52413230\n",
      "Iteration 216, loss = 0.52355474\n",
      "Iteration 217, loss = 0.52297463\n",
      "Iteration 218, loss = 0.52240215\n",
      "Iteration 219, loss = 0.52182670\n",
      "Iteration 220, loss = 0.52125755\n",
      "Iteration 221, loss = 0.52069269\n",
      "Iteration 222, loss = 0.52013070\n",
      "Iteration 223, loss = 0.51956660\n",
      "Iteration 224, loss = 0.51900921\n",
      "Iteration 225, loss = 0.51846021\n",
      "Iteration 226, loss = 0.51791493\n",
      "Iteration 227, loss = 0.51737826\n",
      "Iteration 228, loss = 0.51684073\n",
      "Iteration 229, loss = 0.51630722\n",
      "Iteration 230, loss = 0.51578335\n",
      "Iteration 231, loss = 0.51525751\n",
      "Iteration 232, loss = 0.51473358\n",
      "Iteration 233, loss = 0.51422208\n",
      "Iteration 234, loss = 0.51371137\n",
      "Iteration 235, loss = 0.51319887\n",
      "Iteration 236, loss = 0.51270119\n",
      "Iteration 237, loss = 0.51220136\n",
      "Iteration 238, loss = 0.51170670\n",
      "Iteration 239, loss = 0.51121105\n",
      "Iteration 240, loss = 0.51073286\n",
      "Iteration 241, loss = 0.51025131\n",
      "Iteration 242, loss = 0.50977354\n",
      "Iteration 243, loss = 0.50930139\n",
      "Iteration 244, loss = 0.50883312\n",
      "Iteration 245, loss = 0.50837104\n",
      "Iteration 246, loss = 0.50790693\n",
      "Iteration 247, loss = 0.50744799\n",
      "Iteration 248, loss = 0.50699236\n",
      "Iteration 249, loss = 0.50654805\n",
      "Iteration 250, loss = 0.50609675\n",
      "Iteration 251, loss = 0.50565130\n",
      "Iteration 252, loss = 0.50521064\n",
      "Iteration 253, loss = 0.50478106\n",
      "Iteration 254, loss = 0.50434633\n",
      "Iteration 255, loss = 0.50392461\n",
      "Iteration 256, loss = 0.50349866\n",
      "Iteration 257, loss = 0.50307366\n",
      "Iteration 258, loss = 0.50265431\n",
      "Iteration 259, loss = 0.50224191\n",
      "Iteration 260, loss = 0.50183232\n",
      "Iteration 261, loss = 0.50142156\n",
      "Iteration 262, loss = 0.50101877\n",
      "Iteration 263, loss = 0.50062136\n",
      "Iteration 264, loss = 0.50022575\n",
      "Iteration 265, loss = 0.49983621\n",
      "Iteration 266, loss = 0.49944541\n",
      "Iteration 267, loss = 0.49906460\n",
      "Iteration 268, loss = 0.49868309\n",
      "Iteration 269, loss = 0.49830617\n",
      "Iteration 270, loss = 0.49793097\n",
      "Iteration 271, loss = 0.49755973\n",
      "Iteration 272, loss = 0.49720434\n",
      "Iteration 273, loss = 0.49683528\n",
      "Iteration 274, loss = 0.49647133\n",
      "Iteration 275, loss = 0.49611678\n",
      "Iteration 276, loss = 0.49576843\n",
      "Iteration 277, loss = 0.49541507\n",
      "Iteration 278, loss = 0.49506991\n",
      "Iteration 279, loss = 0.49473013\n",
      "Iteration 280, loss = 0.49438707\n",
      "Iteration 281, loss = 0.49405631\n",
      "Iteration 282, loss = 0.49372569\n",
      "Iteration 283, loss = 0.49338893\n",
      "Iteration 284, loss = 0.49307219\n",
      "Iteration 285, loss = 0.49274877\n",
      "Iteration 286, loss = 0.49242027\n",
      "Iteration 287, loss = 0.49211231\n",
      "Iteration 288, loss = 0.49179992\n",
      "Iteration 289, loss = 0.49149566\n",
      "Iteration 290, loss = 0.49120342\n",
      "Iteration 291, loss = 0.49088718\n",
      "Iteration 292, loss = 0.49060331\n",
      "Iteration 293, loss = 0.49030249\n",
      "Iteration 294, loss = 0.49000699\n",
      "Iteration 295, loss = 0.48972089\n",
      "Iteration 296, loss = 0.48944070\n",
      "Iteration 297, loss = 0.48915997\n",
      "Iteration 298, loss = 0.48887634\n",
      "Iteration 299, loss = 0.48860162\n",
      "Iteration 300, loss = 0.48833282\n",
      "Iteration 301, loss = 0.48806026\n",
      "Iteration 302, loss = 0.48779546\n",
      "Iteration 303, loss = 0.48752989\n",
      "Iteration 304, loss = 0.48727365\n",
      "Iteration 305, loss = 0.48701237\n",
      "Iteration 306, loss = 0.48676240\n",
      "Iteration 307, loss = 0.48650598\n",
      "Iteration 308, loss = 0.48625822\n",
      "Iteration 309, loss = 0.48601693\n",
      "Iteration 310, loss = 0.48576631\n",
      "Iteration 311, loss = 0.48552865\n",
      "Iteration 312, loss = 0.48528375\n",
      "Iteration 313, loss = 0.48505834\n",
      "Iteration 314, loss = 0.48481616\n",
      "Iteration 315, loss = 0.48458785\n",
      "Iteration 316, loss = 0.48436289\n",
      "Iteration 317, loss = 0.48414598\n",
      "Iteration 318, loss = 0.48391809\n",
      "Iteration 319, loss = 0.48370168\n",
      "Iteration 320, loss = 0.48348462\n",
      "Iteration 321, loss = 0.48326805\n",
      "Iteration 322, loss = 0.48305867\n",
      "Iteration 323, loss = 0.48285028\n",
      "Iteration 324, loss = 0.48264321\n",
      "Iteration 325, loss = 0.48243862\n",
      "Iteration 326, loss = 0.48223824\n",
      "Iteration 327, loss = 0.48204266\n",
      "Iteration 328, loss = 0.48184183\n",
      "Iteration 329, loss = 0.48164507\n",
      "Iteration 330, loss = 0.48145498\n",
      "Iteration 331, loss = 0.48126612\n",
      "Iteration 332, loss = 0.48107833\n",
      "Iteration 333, loss = 0.48089754\n",
      "Iteration 334, loss = 0.48071487\n",
      "Iteration 335, loss = 0.48053562\n",
      "Iteration 336, loss = 0.48034567\n",
      "Iteration 337, loss = 0.48017663\n",
      "Iteration 338, loss = 0.48000139\n",
      "Iteration 339, loss = 0.47983374\n",
      "Iteration 340, loss = 0.47966387\n",
      "Iteration 341, loss = 0.47949593\n",
      "Iteration 342, loss = 0.47933586\n",
      "Iteration 343, loss = 0.47917032\n",
      "Iteration 344, loss = 0.47901430\n",
      "Iteration 345, loss = 0.47886054\n",
      "Iteration 346, loss = 0.47870169\n",
      "Iteration 347, loss = 0.47854954\n",
      "Iteration 348, loss = 0.47839831\n",
      "Iteration 349, loss = 0.47824791\n",
      "Iteration 350, loss = 0.47810298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351, loss = 0.47796321\n",
      "Iteration 352, loss = 0.47782100\n",
      "Iteration 353, loss = 0.47768101\n",
      "Iteration 354, loss = 0.47753780\n",
      "Iteration 355, loss = 0.47741143\n",
      "Iteration 356, loss = 0.47726930\n",
      "Iteration 357, loss = 0.47714138\n",
      "Iteration 358, loss = 0.47700817\n",
      "Iteration 359, loss = 0.47687701\n",
      "Iteration 360, loss = 0.47675260\n",
      "Iteration 361, loss = 0.47662880\n",
      "Iteration 362, loss = 0.47649943\n",
      "Iteration 363, loss = 0.47638134\n",
      "Iteration 364, loss = 0.47625991\n",
      "Iteration 365, loss = 0.47614182\n",
      "Iteration 366, loss = 0.47602830\n",
      "Iteration 367, loss = 0.47591535\n",
      "Iteration 368, loss = 0.47579814\n",
      "Iteration 369, loss = 0.47569455\n",
      "Iteration 370, loss = 0.47558322\n",
      "Iteration 371, loss = 0.47548597\n",
      "Iteration 372, loss = 0.47536465\n",
      "Iteration 373, loss = 0.47525890\n",
      "Iteration 374, loss = 0.47515852\n",
      "Iteration 375, loss = 0.47505717\n",
      "Iteration 376, loss = 0.47495966\n",
      "Iteration 377, loss = 0.47485540\n",
      "Iteration 378, loss = 0.47475632\n",
      "Iteration 379, loss = 0.47466360\n",
      "Iteration 380, loss = 0.47457585\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77096488\n",
      "Iteration 2, loss = 0.76504383\n",
      "Iteration 3, loss = 0.75931917\n",
      "Iteration 4, loss = 0.75378888\n",
      "Iteration 5, loss = 0.74845549\n",
      "Iteration 6, loss = 0.74331582\n",
      "Iteration 7, loss = 0.73835553\n",
      "Iteration 8, loss = 0.73359732\n",
      "Iteration 9, loss = 0.72900516\n",
      "Iteration 10, loss = 0.72458543\n",
      "Iteration 11, loss = 0.72035041\n",
      "Iteration 12, loss = 0.71630102\n",
      "Iteration 13, loss = 0.71242220\n",
      "Iteration 14, loss = 0.70870622\n",
      "Iteration 15, loss = 0.70514992\n",
      "Iteration 16, loss = 0.70176733\n",
      "Iteration 17, loss = 0.69851457\n",
      "Iteration 18, loss = 0.69543911\n",
      "Iteration 19, loss = 0.69250821\n",
      "Iteration 20, loss = 0.68973901\n",
      "Iteration 21, loss = 0.68710506\n",
      "Iteration 22, loss = 0.68461471\n",
      "Iteration 23, loss = 0.68225526\n",
      "Iteration 24, loss = 0.68002732\n",
      "Iteration 25, loss = 0.67791739\n",
      "Iteration 26, loss = 0.67591512\n",
      "Iteration 27, loss = 0.67403159\n",
      "Iteration 28, loss = 0.67226635\n",
      "Iteration 29, loss = 0.67058935\n",
      "Iteration 30, loss = 0.66901814\n",
      "Iteration 31, loss = 0.66754485\n",
      "Iteration 32, loss = 0.66614634\n",
      "Iteration 33, loss = 0.66482822\n",
      "Iteration 34, loss = 0.66356920\n",
      "Iteration 35, loss = 0.66239011\n",
      "Iteration 36, loss = 0.66127202\n",
      "Iteration 37, loss = 0.66020339\n",
      "Iteration 38, loss = 0.65919874\n",
      "Iteration 39, loss = 0.65823247\n",
      "Iteration 40, loss = 0.65731269\n",
      "Iteration 41, loss = 0.65642265\n",
      "Iteration 42, loss = 0.65556627\n",
      "Iteration 43, loss = 0.65473761\n",
      "Iteration 44, loss = 0.65393158\n",
      "Iteration 45, loss = 0.65315093\n",
      "Iteration 46, loss = 0.65237690\n",
      "Iteration 47, loss = 0.65162370\n",
      "Iteration 48, loss = 0.65088428\n",
      "Iteration 49, loss = 0.65014775\n",
      "Iteration 50, loss = 0.64942204\n",
      "Iteration 51, loss = 0.64869973\n",
      "Iteration 52, loss = 0.64798187\n",
      "Iteration 53, loss = 0.64726408\n",
      "Iteration 54, loss = 0.64655169\n",
      "Iteration 55, loss = 0.64584114\n",
      "Iteration 56, loss = 0.64512401\n",
      "Iteration 57, loss = 0.64440779\n",
      "Iteration 58, loss = 0.64368969\n",
      "Iteration 59, loss = 0.64297065\n",
      "Iteration 60, loss = 0.64224552\n",
      "Iteration 61, loss = 0.64152236\n",
      "Iteration 62, loss = 0.64078866\n",
      "Iteration 63, loss = 0.64005514\n",
      "Iteration 64, loss = 0.63931838\n",
      "Iteration 65, loss = 0.63858121\n",
      "Iteration 66, loss = 0.63783566\n",
      "Iteration 67, loss = 0.63708838\n",
      "Iteration 68, loss = 0.63633408\n",
      "Iteration 69, loss = 0.63557991\n",
      "Iteration 70, loss = 0.63481847\n",
      "Iteration 71, loss = 0.63405394\n",
      "Iteration 72, loss = 0.63328401\n",
      "Iteration 73, loss = 0.63251169\n",
      "Iteration 74, loss = 0.63173772\n",
      "Iteration 75, loss = 0.63095546\n",
      "Iteration 76, loss = 0.63017166\n",
      "Iteration 77, loss = 0.62938614\n",
      "Iteration 78, loss = 0.62859465\n",
      "Iteration 79, loss = 0.62779928\n",
      "Iteration 80, loss = 0.62700612\n",
      "Iteration 81, loss = 0.62620847\n",
      "Iteration 82, loss = 0.62541117\n",
      "Iteration 83, loss = 0.62461021\n",
      "Iteration 84, loss = 0.62381621\n",
      "Iteration 85, loss = 0.62300925\n",
      "Iteration 86, loss = 0.62221019\n",
      "Iteration 87, loss = 0.62140993\n",
      "Iteration 88, loss = 0.62060324\n",
      "Iteration 89, loss = 0.61979595\n",
      "Iteration 90, loss = 0.61899071\n",
      "Iteration 91, loss = 0.61818445\n",
      "Iteration 92, loss = 0.61737394\n",
      "Iteration 93, loss = 0.61656435\n",
      "Iteration 94, loss = 0.61575208\n",
      "Iteration 95, loss = 0.61493735\n",
      "Iteration 96, loss = 0.61413670\n",
      "Iteration 97, loss = 0.61331803\n",
      "Iteration 98, loss = 0.61251423\n",
      "Iteration 99, loss = 0.61169685\n",
      "Iteration 100, loss = 0.61089114\n",
      "Iteration 101, loss = 0.61007983\n",
      "Iteration 102, loss = 0.60927029\n",
      "Iteration 103, loss = 0.60845690\n",
      "Iteration 104, loss = 0.60764477\n",
      "Iteration 105, loss = 0.60683115\n",
      "Iteration 106, loss = 0.60601867\n",
      "Iteration 107, loss = 0.60520763\n",
      "Iteration 108, loss = 0.60439543\n",
      "Iteration 109, loss = 0.60358215\n",
      "Iteration 110, loss = 0.60276924\n",
      "Iteration 111, loss = 0.60195173\n",
      "Iteration 112, loss = 0.60113708\n",
      "Iteration 113, loss = 0.60032995\n",
      "Iteration 114, loss = 0.59951064\n",
      "Iteration 115, loss = 0.59870192\n",
      "Iteration 116, loss = 0.59788910\n",
      "Iteration 117, loss = 0.59707784\n",
      "Iteration 118, loss = 0.59627543\n",
      "Iteration 119, loss = 0.59545648\n",
      "Iteration 120, loss = 0.59464596\n",
      "Iteration 121, loss = 0.59383914\n",
      "Iteration 122, loss = 0.59303292\n",
      "Iteration 123, loss = 0.59223086\n",
      "Iteration 124, loss = 0.59142292\n",
      "Iteration 125, loss = 0.59061965\n",
      "Iteration 126, loss = 0.58981791\n",
      "Iteration 127, loss = 0.58901421\n",
      "Iteration 128, loss = 0.58821719\n",
      "Iteration 129, loss = 0.58741133\n",
      "Iteration 130, loss = 0.58661697\n",
      "Iteration 131, loss = 0.58581739\n",
      "Iteration 132, loss = 0.58501856\n",
      "Iteration 133, loss = 0.58421996\n",
      "Iteration 134, loss = 0.58342179\n",
      "Iteration 135, loss = 0.58262866\n",
      "Iteration 136, loss = 0.58183117\n",
      "Iteration 137, loss = 0.58103199\n",
      "Iteration 138, loss = 0.58023753\n",
      "Iteration 139, loss = 0.57944363\n",
      "Iteration 140, loss = 0.57865492\n",
      "Iteration 141, loss = 0.57786454\n",
      "Iteration 142, loss = 0.57707525\n",
      "Iteration 143, loss = 0.57629252\n",
      "Iteration 144, loss = 0.57550725\n",
      "Iteration 145, loss = 0.57472866\n",
      "Iteration 146, loss = 0.57394284\n",
      "Iteration 147, loss = 0.57316673\n",
      "Iteration 148, loss = 0.57238942\n",
      "Iteration 149, loss = 0.57161381\n",
      "Iteration 150, loss = 0.57084278\n",
      "Iteration 151, loss = 0.57007600\n",
      "Iteration 152, loss = 0.56930867\n",
      "Iteration 153, loss = 0.56854754\n",
      "Iteration 154, loss = 0.56778646\n",
      "Iteration 155, loss = 0.56702797\n",
      "Iteration 156, loss = 0.56627277\n",
      "Iteration 157, loss = 0.56551599\n",
      "Iteration 158, loss = 0.56475975\n",
      "Iteration 159, loss = 0.56400505\n",
      "Iteration 160, loss = 0.56325173\n",
      "Iteration 161, loss = 0.56249814\n",
      "Iteration 162, loss = 0.56175666\n",
      "Iteration 163, loss = 0.56100236\n",
      "Iteration 164, loss = 0.56025663\n",
      "Iteration 165, loss = 0.55951497\n",
      "Iteration 166, loss = 0.55877415\n",
      "Iteration 167, loss = 0.55803977\n",
      "Iteration 168, loss = 0.55729834\n",
      "Iteration 169, loss = 0.55656502\n",
      "Iteration 170, loss = 0.55583366\n",
      "Iteration 171, loss = 0.55510628\n",
      "Iteration 172, loss = 0.55438897\n",
      "Iteration 173, loss = 0.55365989\n",
      "Iteration 174, loss = 0.55294321\n",
      "Iteration 175, loss = 0.55222840\n",
      "Iteration 176, loss = 0.55151173\n",
      "Iteration 177, loss = 0.55080343\n",
      "Iteration 178, loss = 0.55010006\n",
      "Iteration 179, loss = 0.54939506\n",
      "Iteration 180, loss = 0.54870141\n",
      "Iteration 181, loss = 0.54800062\n",
      "Iteration 182, loss = 0.54730818\n",
      "Iteration 183, loss = 0.54662473\n",
      "Iteration 184, loss = 0.54593585\n",
      "Iteration 185, loss = 0.54525576\n",
      "Iteration 186, loss = 0.54458743\n",
      "Iteration 187, loss = 0.54390950\n",
      "Iteration 188, loss = 0.54324209\n",
      "Iteration 189, loss = 0.54257334\n",
      "Iteration 190, loss = 0.54190707\n",
      "Iteration 191, loss = 0.54124531\n",
      "Iteration 192, loss = 0.54058299\n",
      "Iteration 193, loss = 0.53993217\n",
      "Iteration 194, loss = 0.53928140\n",
      "Iteration 195, loss = 0.53863041\n",
      "Iteration 196, loss = 0.53798996\n",
      "Iteration 197, loss = 0.53735058\n",
      "Iteration 198, loss = 0.53671414\n",
      "Iteration 199, loss = 0.53608290\n",
      "Iteration 200, loss = 0.53544779\n",
      "Iteration 201, loss = 0.53482103\n",
      "Iteration 202, loss = 0.53419547\n",
      "Iteration 203, loss = 0.53357670\n",
      "Iteration 204, loss = 0.53295772\n",
      "Iteration 205, loss = 0.53234222\n",
      "Iteration 206, loss = 0.53172959\n",
      "Iteration 207, loss = 0.53112663\n",
      "Iteration 208, loss = 0.53052717\n",
      "Iteration 209, loss = 0.52992241\n",
      "Iteration 210, loss = 0.52933378\n",
      "Iteration 211, loss = 0.52874017\n",
      "Iteration 212, loss = 0.52815464\n",
      "Iteration 213, loss = 0.52756979\n",
      "Iteration 214, loss = 0.52698904\n",
      "Iteration 215, loss = 0.52640595\n",
      "Iteration 216, loss = 0.52583798\n",
      "Iteration 217, loss = 0.52526545\n",
      "Iteration 218, loss = 0.52469940\n",
      "Iteration 219, loss = 0.52413552\n",
      "Iteration 220, loss = 0.52357382\n",
      "Iteration 221, loss = 0.52302045\n",
      "Iteration 222, loss = 0.52246745\n",
      "Iteration 223, loss = 0.52190795\n",
      "Iteration 224, loss = 0.52135822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 225, loss = 0.52082113\n",
      "Iteration 226, loss = 0.52027900\n",
      "Iteration 227, loss = 0.51974997\n",
      "Iteration 228, loss = 0.51921919\n",
      "Iteration 229, loss = 0.51869397\n",
      "Iteration 230, loss = 0.51817342\n",
      "Iteration 231, loss = 0.51765651\n",
      "Iteration 232, loss = 0.51713867\n",
      "Iteration 233, loss = 0.51663263\n",
      "Iteration 234, loss = 0.51612870\n",
      "Iteration 235, loss = 0.51562222\n",
      "Iteration 236, loss = 0.51512776\n",
      "Iteration 237, loss = 0.51464206\n",
      "Iteration 238, loss = 0.51414683\n",
      "Iteration 239, loss = 0.51365727\n",
      "Iteration 240, loss = 0.51318705\n",
      "Iteration 241, loss = 0.51270274\n",
      "Iteration 242, loss = 0.51223369\n",
      "Iteration 243, loss = 0.51176624\n",
      "Iteration 244, loss = 0.51130090\n",
      "Iteration 245, loss = 0.51083798\n",
      "Iteration 246, loss = 0.51037740\n",
      "Iteration 247, loss = 0.50992252\n",
      "Iteration 248, loss = 0.50946860\n",
      "Iteration 249, loss = 0.50902809\n",
      "Iteration 250, loss = 0.50857730\n",
      "Iteration 251, loss = 0.50813199\n",
      "Iteration 252, loss = 0.50769315\n",
      "Iteration 253, loss = 0.50726253\n",
      "Iteration 254, loss = 0.50682761\n",
      "Iteration 255, loss = 0.50640750\n",
      "Iteration 256, loss = 0.50598327\n",
      "Iteration 257, loss = 0.50555562\n",
      "Iteration 258, loss = 0.50514097\n",
      "Iteration 259, loss = 0.50472757\n",
      "Iteration 260, loss = 0.50431563\n",
      "Iteration 261, loss = 0.50390437\n",
      "Iteration 262, loss = 0.50350195\n",
      "Iteration 263, loss = 0.50310470\n",
      "Iteration 264, loss = 0.50270988\n",
      "Iteration 265, loss = 0.50231302\n",
      "Iteration 266, loss = 0.50192416\n",
      "Iteration 267, loss = 0.50154360\n",
      "Iteration 268, loss = 0.50116032\n",
      "Iteration 269, loss = 0.50077796\n",
      "Iteration 270, loss = 0.50040174\n",
      "Iteration 271, loss = 0.50002935\n",
      "Iteration 272, loss = 0.49966828\n",
      "Iteration 273, loss = 0.49930039\n",
      "Iteration 274, loss = 0.49893320\n",
      "Iteration 275, loss = 0.49857545\n",
      "Iteration 276, loss = 0.49822225\n",
      "Iteration 277, loss = 0.49786824\n",
      "Iteration 278, loss = 0.49752008\n",
      "Iteration 279, loss = 0.49717610\n",
      "Iteration 280, loss = 0.49682839\n",
      "Iteration 281, loss = 0.49649097\n",
      "Iteration 282, loss = 0.49616064\n",
      "Iteration 283, loss = 0.49581740\n",
      "Iteration 284, loss = 0.49549091\n",
      "Iteration 285, loss = 0.49516326\n",
      "Iteration 286, loss = 0.49483869\n",
      "Iteration 287, loss = 0.49452005\n",
      "Iteration 288, loss = 0.49420376\n",
      "Iteration 289, loss = 0.49388902\n",
      "Iteration 290, loss = 0.49359257\n",
      "Iteration 291, loss = 0.49326843\n",
      "Iteration 292, loss = 0.49296961\n",
      "Iteration 293, loss = 0.49267170\n",
      "Iteration 294, loss = 0.49236791\n",
      "Iteration 295, loss = 0.49207212\n",
      "Iteration 296, loss = 0.49178207\n",
      "Iteration 297, loss = 0.49149919\n",
      "Iteration 298, loss = 0.49120743\n",
      "Iteration 299, loss = 0.49092577\n",
      "Iteration 300, loss = 0.49064801\n",
      "Iteration 301, loss = 0.49036986\n",
      "Iteration 302, loss = 0.49009578\n",
      "Iteration 303, loss = 0.48982480\n",
      "Iteration 304, loss = 0.48955948\n",
      "Iteration 305, loss = 0.48929176\n",
      "Iteration 306, loss = 0.48903387\n",
      "Iteration 307, loss = 0.48877504\n",
      "Iteration 308, loss = 0.48851639\n",
      "Iteration 309, loss = 0.48826490\n",
      "Iteration 310, loss = 0.48800937\n",
      "Iteration 311, loss = 0.48776513\n",
      "Iteration 312, loss = 0.48751278\n",
      "Iteration 313, loss = 0.48728029\n",
      "Iteration 314, loss = 0.48703021\n",
      "Iteration 315, loss = 0.48679436\n",
      "Iteration 316, loss = 0.48656109\n",
      "Iteration 317, loss = 0.48633355\n",
      "Iteration 318, loss = 0.48610272\n",
      "Iteration 319, loss = 0.48587382\n",
      "Iteration 320, loss = 0.48565398\n",
      "Iteration 321, loss = 0.48542758\n",
      "Iteration 322, loss = 0.48520837\n",
      "Iteration 323, loss = 0.48499211\n",
      "Iteration 324, loss = 0.48477563\n",
      "Iteration 325, loss = 0.48456289\n",
      "Iteration 326, loss = 0.48435360\n",
      "Iteration 327, loss = 0.48414799\n",
      "Iteration 328, loss = 0.48394371\n",
      "Iteration 329, loss = 0.48373729\n",
      "Iteration 330, loss = 0.48353646\n",
      "Iteration 331, loss = 0.48333679\n",
      "Iteration 332, loss = 0.48313827\n",
      "Iteration 333, loss = 0.48294967\n",
      "Iteration 334, loss = 0.48276407\n",
      "Iteration 335, loss = 0.48256986\n",
      "Iteration 336, loss = 0.48237220\n",
      "Iteration 337, loss = 0.48219246\n",
      "Iteration 338, loss = 0.48201106\n",
      "Iteration 339, loss = 0.48183542\n",
      "Iteration 340, loss = 0.48165778\n",
      "Iteration 341, loss = 0.48148122\n",
      "Iteration 342, loss = 0.48131572\n",
      "Iteration 343, loss = 0.48114175\n",
      "Iteration 344, loss = 0.48097452\n",
      "Iteration 345, loss = 0.48081299\n",
      "Iteration 346, loss = 0.48064648\n",
      "Iteration 347, loss = 0.48048415\n",
      "Iteration 348, loss = 0.48032339\n",
      "Iteration 349, loss = 0.48016504\n",
      "Iteration 350, loss = 0.48001805\n",
      "Iteration 351, loss = 0.47985968\n",
      "Iteration 352, loss = 0.47970985\n",
      "Iteration 353, loss = 0.47956366\n",
      "Iteration 354, loss = 0.47940985\n",
      "Iteration 355, loss = 0.47927364\n",
      "Iteration 356, loss = 0.47912441\n",
      "Iteration 357, loss = 0.47899047\n",
      "Iteration 358, loss = 0.47885576\n",
      "Iteration 359, loss = 0.47870614\n",
      "Iteration 360, loss = 0.47857284\n",
      "Iteration 361, loss = 0.47844032\n",
      "Iteration 362, loss = 0.47830059\n",
      "Iteration 363, loss = 0.47817618\n",
      "Iteration 364, loss = 0.47804716\n",
      "Iteration 365, loss = 0.47791609\n",
      "Iteration 366, loss = 0.47779267\n",
      "Iteration 367, loss = 0.47767056\n",
      "Iteration 368, loss = 0.47754759\n",
      "Iteration 369, loss = 0.47743196\n",
      "Iteration 370, loss = 0.47731390\n",
      "Iteration 371, loss = 0.47720291\n",
      "Iteration 372, loss = 0.47708043\n",
      "Iteration 373, loss = 0.47696443\n",
      "Iteration 374, loss = 0.47685130\n",
      "Iteration 375, loss = 0.47673971\n",
      "Iteration 376, loss = 0.47663616\n",
      "Iteration 377, loss = 0.47652222\n",
      "Iteration 378, loss = 0.47641333\n",
      "Iteration 379, loss = 0.47631149\n",
      "Iteration 380, loss = 0.47620728\n",
      "Iteration 381, loss = 0.47610818\n",
      "Iteration 382, loss = 0.47600931\n",
      "Iteration 383, loss = 0.47591068\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77709539\n",
      "Iteration 2, loss = 0.77088850\n",
      "Iteration 3, loss = 0.76487548\n",
      "Iteration 4, loss = 0.75904946\n",
      "Iteration 5, loss = 0.75341687\n",
      "Iteration 6, loss = 0.74798260\n",
      "Iteration 7, loss = 0.74272471\n",
      "Iteration 8, loss = 0.73767516\n",
      "Iteration 9, loss = 0.73278406\n",
      "Iteration 10, loss = 0.72807435\n",
      "Iteration 11, loss = 0.72354682\n",
      "Iteration 12, loss = 0.71920826\n",
      "Iteration 13, loss = 0.71504703\n",
      "Iteration 14, loss = 0.71105372\n",
      "Iteration 15, loss = 0.70722557\n",
      "Iteration 16, loss = 0.70356955\n",
      "Iteration 17, loss = 0.70005246\n",
      "Iteration 18, loss = 0.69670897\n",
      "Iteration 19, loss = 0.69352287\n",
      "Iteration 20, loss = 0.69049818\n",
      "Iteration 21, loss = 0.68762209\n",
      "Iteration 22, loss = 0.68488990\n",
      "Iteration 23, loss = 0.68229966\n",
      "Iteration 24, loss = 0.67984473\n",
      "Iteration 25, loss = 0.67751818\n",
      "Iteration 26, loss = 0.67530704\n",
      "Iteration 27, loss = 0.67322319\n",
      "Iteration 28, loss = 0.67127316\n",
      "Iteration 29, loss = 0.66941689\n",
      "Iteration 30, loss = 0.66767896\n",
      "Iteration 31, loss = 0.66604510\n",
      "Iteration 32, loss = 0.66450377\n",
      "Iteration 33, loss = 0.66305222\n",
      "Iteration 34, loss = 0.66166632\n",
      "Iteration 35, loss = 0.66037551\n",
      "Iteration 36, loss = 0.65915430\n",
      "Iteration 37, loss = 0.65799459\n",
      "Iteration 38, loss = 0.65690769\n",
      "Iteration 39, loss = 0.65587221\n",
      "Iteration 40, loss = 0.65489624\n",
      "Iteration 41, loss = 0.65395719\n",
      "Iteration 42, loss = 0.65306472\n",
      "Iteration 43, loss = 0.65220535\n",
      "Iteration 44, loss = 0.65137763\n",
      "Iteration 45, loss = 0.65058176\n",
      "Iteration 46, loss = 0.64980288\n",
      "Iteration 47, loss = 0.64904824\n",
      "Iteration 48, loss = 0.64831539\n",
      "Iteration 49, loss = 0.64759104\n",
      "Iteration 50, loss = 0.64688338\n",
      "Iteration 51, loss = 0.64618262\n",
      "Iteration 52, loss = 0.64548912\n",
      "Iteration 53, loss = 0.64479973\n",
      "Iteration 54, loss = 0.64411935\n",
      "Iteration 55, loss = 0.64344168\n",
      "Iteration 56, loss = 0.64276228\n",
      "Iteration 57, loss = 0.64208341\n",
      "Iteration 58, loss = 0.64140476\n",
      "Iteration 59, loss = 0.64072535\n",
      "Iteration 60, loss = 0.64004306\n",
      "Iteration 61, loss = 0.63936305\n",
      "Iteration 62, loss = 0.63867271\n",
      "Iteration 63, loss = 0.63798349\n",
      "Iteration 64, loss = 0.63729079\n",
      "Iteration 65, loss = 0.63659726\n",
      "Iteration 66, loss = 0.63589552\n",
      "Iteration 67, loss = 0.63519150\n",
      "Iteration 68, loss = 0.63448105\n",
      "Iteration 69, loss = 0.63376944\n",
      "Iteration 70, loss = 0.63305295\n",
      "Iteration 71, loss = 0.63233411\n",
      "Iteration 72, loss = 0.63160890\n",
      "Iteration 73, loss = 0.63088118\n",
      "Iteration 74, loss = 0.63015291\n",
      "Iteration 75, loss = 0.62941423\n",
      "Iteration 76, loss = 0.62867581\n",
      "Iteration 77, loss = 0.62793479\n",
      "Iteration 78, loss = 0.62718865\n",
      "Iteration 79, loss = 0.62643786\n",
      "Iteration 80, loss = 0.62568910\n",
      "Iteration 81, loss = 0.62493634\n",
      "Iteration 82, loss = 0.62418502\n",
      "Iteration 83, loss = 0.62342681\n",
      "Iteration 84, loss = 0.62267870\n",
      "Iteration 85, loss = 0.62191227\n",
      "Iteration 86, loss = 0.62115472\n",
      "Iteration 87, loss = 0.62039344\n",
      "Iteration 88, loss = 0.61962994\n",
      "Iteration 89, loss = 0.61886117\n",
      "Iteration 90, loss = 0.61809360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91, loss = 0.61732520\n",
      "Iteration 92, loss = 0.61655491\n",
      "Iteration 93, loss = 0.61577760\n",
      "Iteration 94, loss = 0.61500743\n",
      "Iteration 95, loss = 0.61423064\n",
      "Iteration 96, loss = 0.61346535\n",
      "Iteration 97, loss = 0.61268616\n",
      "Iteration 98, loss = 0.61191807\n",
      "Iteration 99, loss = 0.61114282\n",
      "Iteration 100, loss = 0.61037585\n",
      "Iteration 101, loss = 0.60960459\n",
      "Iteration 102, loss = 0.60883352\n",
      "Iteration 103, loss = 0.60806175\n",
      "Iteration 104, loss = 0.60729109\n",
      "Iteration 105, loss = 0.60651947\n",
      "Iteration 106, loss = 0.60574476\n",
      "Iteration 107, loss = 0.60497472\n",
      "Iteration 108, loss = 0.60420300\n",
      "Iteration 109, loss = 0.60342809\n",
      "Iteration 110, loss = 0.60265444\n",
      "Iteration 111, loss = 0.60187774\n",
      "Iteration 112, loss = 0.60110395\n",
      "Iteration 113, loss = 0.60033232\n",
      "Iteration 114, loss = 0.59955290\n",
      "Iteration 115, loss = 0.59878309\n",
      "Iteration 116, loss = 0.59800767\n",
      "Iteration 117, loss = 0.59723662\n",
      "Iteration 118, loss = 0.59647218\n",
      "Iteration 119, loss = 0.59569354\n",
      "Iteration 120, loss = 0.59492057\n",
      "Iteration 121, loss = 0.59415348\n",
      "Iteration 122, loss = 0.59338567\n",
      "Iteration 123, loss = 0.59262087\n",
      "Iteration 124, loss = 0.59185280\n",
      "Iteration 125, loss = 0.59108786\n",
      "Iteration 126, loss = 0.59032521\n",
      "Iteration 127, loss = 0.58955799\n",
      "Iteration 128, loss = 0.58879827\n",
      "Iteration 129, loss = 0.58803259\n",
      "Iteration 130, loss = 0.58727651\n",
      "Iteration 131, loss = 0.58651619\n",
      "Iteration 132, loss = 0.58575778\n",
      "Iteration 133, loss = 0.58500010\n",
      "Iteration 134, loss = 0.58424189\n",
      "Iteration 135, loss = 0.58348815\n",
      "Iteration 136, loss = 0.58273335\n",
      "Iteration 137, loss = 0.58197531\n",
      "Iteration 138, loss = 0.58122366\n",
      "Iteration 139, loss = 0.58047007\n",
      "Iteration 140, loss = 0.57972177\n",
      "Iteration 141, loss = 0.57897411\n",
      "Iteration 142, loss = 0.57822707\n",
      "Iteration 143, loss = 0.57748764\n",
      "Iteration 144, loss = 0.57674625\n",
      "Iteration 145, loss = 0.57600784\n",
      "Iteration 146, loss = 0.57526778\n",
      "Iteration 147, loss = 0.57453723\n",
      "Iteration 148, loss = 0.57380065\n",
      "Iteration 149, loss = 0.57306846\n",
      "Iteration 150, loss = 0.57234021\n",
      "Iteration 151, loss = 0.57161447\n",
      "Iteration 152, loss = 0.57088995\n",
      "Iteration 153, loss = 0.57017105\n",
      "Iteration 154, loss = 0.56945019\n",
      "Iteration 155, loss = 0.56873309\n",
      "Iteration 156, loss = 0.56801945\n",
      "Iteration 157, loss = 0.56730456\n",
      "Iteration 158, loss = 0.56658701\n",
      "Iteration 159, loss = 0.56587467\n",
      "Iteration 160, loss = 0.56516103\n",
      "Iteration 161, loss = 0.56444784\n",
      "Iteration 162, loss = 0.56374372\n",
      "Iteration 163, loss = 0.56302817\n",
      "Iteration 164, loss = 0.56232178\n",
      "Iteration 165, loss = 0.56161724\n",
      "Iteration 166, loss = 0.56091738\n",
      "Iteration 167, loss = 0.56021819\n",
      "Iteration 168, loss = 0.55951478\n",
      "Iteration 169, loss = 0.55881922\n",
      "Iteration 170, loss = 0.55812749\n",
      "Iteration 171, loss = 0.55743642\n",
      "Iteration 172, loss = 0.55675921\n",
      "Iteration 173, loss = 0.55606412\n",
      "Iteration 174, loss = 0.55538280\n",
      "Iteration 175, loss = 0.55470650\n",
      "Iteration 176, loss = 0.55402618\n",
      "Iteration 177, loss = 0.55335433\n",
      "Iteration 178, loss = 0.55268571\n",
      "Iteration 179, loss = 0.55201510\n",
      "Iteration 180, loss = 0.55135479\n",
      "Iteration 181, loss = 0.55069069\n",
      "Iteration 182, loss = 0.55003207\n",
      "Iteration 183, loss = 0.54938146\n",
      "Iteration 184, loss = 0.54872850\n",
      "Iteration 185, loss = 0.54808164\n",
      "Iteration 186, loss = 0.54744660\n",
      "Iteration 187, loss = 0.54679927\n",
      "Iteration 188, loss = 0.54616529\n",
      "Iteration 189, loss = 0.54553040\n",
      "Iteration 190, loss = 0.54489632\n",
      "Iteration 191, loss = 0.54426992\n",
      "Iteration 192, loss = 0.54364289\n",
      "Iteration 193, loss = 0.54301548\n",
      "Iteration 194, loss = 0.54239693\n",
      "Iteration 195, loss = 0.54177500\n",
      "Iteration 196, loss = 0.54116611\n",
      "Iteration 197, loss = 0.54055222\n",
      "Iteration 198, loss = 0.53994335\n",
      "Iteration 199, loss = 0.53933810\n",
      "Iteration 200, loss = 0.53873315\n",
      "Iteration 201, loss = 0.53813452\n",
      "Iteration 202, loss = 0.53753434\n",
      "Iteration 203, loss = 0.53694663\n",
      "Iteration 204, loss = 0.53635376\n",
      "Iteration 205, loss = 0.53576696\n",
      "Iteration 206, loss = 0.53518138\n",
      "Iteration 207, loss = 0.53460712\n",
      "Iteration 208, loss = 0.53403563\n",
      "Iteration 209, loss = 0.53345899\n",
      "Iteration 210, loss = 0.53290382\n",
      "Iteration 211, loss = 0.53233268\n",
      "Iteration 212, loss = 0.53177520\n",
      "Iteration 213, loss = 0.53122110\n",
      "Iteration 214, loss = 0.53066552\n",
      "Iteration 215, loss = 0.53010965\n",
      "Iteration 216, loss = 0.52956784\n",
      "Iteration 217, loss = 0.52902053\n",
      "Iteration 218, loss = 0.52847871\n",
      "Iteration 219, loss = 0.52793994\n",
      "Iteration 220, loss = 0.52740219\n",
      "Iteration 221, loss = 0.52687303\n",
      "Iteration 222, loss = 0.52634570\n",
      "Iteration 223, loss = 0.52581228\n",
      "Iteration 224, loss = 0.52528584\n",
      "Iteration 225, loss = 0.52477363\n",
      "Iteration 226, loss = 0.52425395\n",
      "Iteration 227, loss = 0.52374803\n",
      "Iteration 228, loss = 0.52324136\n",
      "Iteration 229, loss = 0.52273880\n",
      "Iteration 230, loss = 0.52224095\n",
      "Iteration 231, loss = 0.52174528\n",
      "Iteration 232, loss = 0.52124729\n",
      "Iteration 233, loss = 0.52076252\n",
      "Iteration 234, loss = 0.52027538\n",
      "Iteration 235, loss = 0.51978878\n",
      "Iteration 236, loss = 0.51930985\n",
      "Iteration 237, loss = 0.51884835\n",
      "Iteration 238, loss = 0.51836676\n",
      "Iteration 239, loss = 0.51789529\n",
      "Iteration 240, loss = 0.51743966\n",
      "Iteration 241, loss = 0.51697208\n",
      "Iteration 242, loss = 0.51651927\n",
      "Iteration 243, loss = 0.51606610\n",
      "Iteration 244, loss = 0.51561198\n",
      "Iteration 245, loss = 0.51516421\n",
      "Iteration 246, loss = 0.51471746\n",
      "Iteration 247, loss = 0.51427738\n",
      "Iteration 248, loss = 0.51383643\n",
      "Iteration 249, loss = 0.51340763\n",
      "Iteration 250, loss = 0.51297178\n",
      "Iteration 251, loss = 0.51253847\n",
      "Iteration 252, loss = 0.51211231\n",
      "Iteration 253, loss = 0.51169719\n",
      "Iteration 254, loss = 0.51127198\n",
      "Iteration 255, loss = 0.51086258\n",
      "Iteration 256, loss = 0.51045116\n",
      "Iteration 257, loss = 0.51003556\n",
      "Iteration 258, loss = 0.50963500\n",
      "Iteration 259, loss = 0.50923579\n",
      "Iteration 260, loss = 0.50883419\n",
      "Iteration 261, loss = 0.50843357\n",
      "Iteration 262, loss = 0.50804466\n",
      "Iteration 263, loss = 0.50765767\n",
      "Iteration 264, loss = 0.50728065\n",
      "Iteration 265, loss = 0.50688936\n",
      "Iteration 266, loss = 0.50650895\n",
      "Iteration 267, loss = 0.50613925\n",
      "Iteration 268, loss = 0.50576732\n",
      "Iteration 269, loss = 0.50539610\n",
      "Iteration 270, loss = 0.50503166\n",
      "Iteration 271, loss = 0.50467183\n",
      "Iteration 272, loss = 0.50431912\n",
      "Iteration 273, loss = 0.50396624\n",
      "Iteration 274, loss = 0.50360871\n",
      "Iteration 275, loss = 0.50325842\n",
      "Iteration 276, loss = 0.50291677\n",
      "Iteration 277, loss = 0.50257232\n",
      "Iteration 278, loss = 0.50223452\n",
      "Iteration 279, loss = 0.50189630\n",
      "Iteration 280, loss = 0.50156113\n",
      "Iteration 281, loss = 0.50123097\n",
      "Iteration 282, loss = 0.50091118\n",
      "Iteration 283, loss = 0.50057795\n",
      "Iteration 284, loss = 0.50026204\n",
      "Iteration 285, loss = 0.49993981\n",
      "Iteration 286, loss = 0.49962307\n",
      "Iteration 287, loss = 0.49930701\n",
      "Iteration 288, loss = 0.49899979\n",
      "Iteration 289, loss = 0.49869465\n",
      "Iteration 290, loss = 0.49839578\n",
      "Iteration 291, loss = 0.49808597\n",
      "Iteration 292, loss = 0.49779020\n",
      "Iteration 293, loss = 0.49749765\n",
      "Iteration 294, loss = 0.49720218\n",
      "Iteration 295, loss = 0.49691194\n",
      "Iteration 296, loss = 0.49662721\n",
      "Iteration 297, loss = 0.49634950\n",
      "Iteration 298, loss = 0.49606060\n",
      "Iteration 299, loss = 0.49578774\n",
      "Iteration 300, loss = 0.49551328\n",
      "Iteration 301, loss = 0.49523877\n",
      "Iteration 302, loss = 0.49496737\n",
      "Iteration 303, loss = 0.49470275\n",
      "Iteration 304, loss = 0.49443862\n",
      "Iteration 305, loss = 0.49417734\n",
      "Iteration 306, loss = 0.49392415\n",
      "Iteration 307, loss = 0.49366703\n",
      "Iteration 308, loss = 0.49341075\n",
      "Iteration 309, loss = 0.49316546\n",
      "Iteration 310, loss = 0.49291063\n",
      "Iteration 311, loss = 0.49267133\n",
      "Iteration 312, loss = 0.49242206\n",
      "Iteration 313, loss = 0.49219245\n",
      "Iteration 314, loss = 0.49194297\n",
      "Iteration 315, loss = 0.49170916\n",
      "Iteration 316, loss = 0.49147705\n",
      "Iteration 317, loss = 0.49125186\n",
      "Iteration 318, loss = 0.49102545\n",
      "Iteration 319, loss = 0.49079549\n",
      "Iteration 320, loss = 0.49057829\n",
      "Iteration 321, loss = 0.49035157\n",
      "Iteration 322, loss = 0.49013461\n",
      "Iteration 323, loss = 0.48991997\n",
      "Iteration 324, loss = 0.48970371\n",
      "Iteration 325, loss = 0.48949168\n",
      "Iteration 326, loss = 0.48928366\n",
      "Iteration 327, loss = 0.48907939\n",
      "Iteration 328, loss = 0.48887484\n",
      "Iteration 329, loss = 0.48867048\n",
      "Iteration 330, loss = 0.48846978\n",
      "Iteration 331, loss = 0.48827503\n",
      "Iteration 332, loss = 0.48807944\n",
      "Iteration 333, loss = 0.48788992\n",
      "Iteration 334, loss = 0.48770531\n",
      "Iteration 335, loss = 0.48751062\n",
      "Iteration 336, loss = 0.48732012\n",
      "Iteration 337, loss = 0.48713738\n",
      "Iteration 338, loss = 0.48696058\n",
      "Iteration 339, loss = 0.48678477\n",
      "Iteration 340, loss = 0.48660923\n",
      "Iteration 341, loss = 0.48643357\n",
      "Iteration 342, loss = 0.48626610\n",
      "Iteration 343, loss = 0.48609482\n",
      "Iteration 344, loss = 0.48593062\n",
      "Iteration 345, loss = 0.48577021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 346, loss = 0.48560399\n",
      "Iteration 347, loss = 0.48543842\n",
      "Iteration 348, loss = 0.48528000\n",
      "Iteration 349, loss = 0.48512240\n",
      "Iteration 350, loss = 0.48497082\n",
      "Iteration 351, loss = 0.48481921\n",
      "Iteration 352, loss = 0.48466726\n",
      "Iteration 353, loss = 0.48451683\n",
      "Iteration 354, loss = 0.48436512\n",
      "Iteration 355, loss = 0.48422294\n",
      "Iteration 356, loss = 0.48407611\n",
      "Iteration 357, loss = 0.48393906\n",
      "Iteration 358, loss = 0.48380229\n",
      "Iteration 359, loss = 0.48365533\n",
      "Iteration 360, loss = 0.48351791\n",
      "Iteration 361, loss = 0.48338856\n",
      "Iteration 362, loss = 0.48324870\n",
      "Iteration 363, loss = 0.48312117\n",
      "Iteration 364, loss = 0.48299149\n",
      "Iteration 365, loss = 0.48286079\n",
      "Iteration 366, loss = 0.48273437\n",
      "Iteration 367, loss = 0.48261357\n",
      "Iteration 368, loss = 0.48248682\n",
      "Iteration 369, loss = 0.48236817\n",
      "Iteration 370, loss = 0.48224498\n",
      "Iteration 371, loss = 0.48213138\n",
      "Iteration 372, loss = 0.48201088\n",
      "Iteration 373, loss = 0.48189401\n",
      "Iteration 374, loss = 0.48177884\n",
      "Iteration 375, loss = 0.48166499\n",
      "Iteration 376, loss = 0.48155750\n",
      "Iteration 377, loss = 0.48144121\n",
      "Iteration 378, loss = 0.48133465\n",
      "Iteration 379, loss = 0.48123380\n",
      "Iteration 380, loss = 0.48112522\n",
      "Iteration 381, loss = 0.48102253\n",
      "Iteration 382, loss = 0.48091992\n",
      "Iteration 383, loss = 0.48082016\n",
      "Iteration 384, loss = 0.48071510\n",
      "Iteration 385, loss = 0.48062219\n",
      "Iteration 386, loss = 0.48052166\n",
      "Iteration 387, loss = 0.48043313\n",
      "Iteration 388, loss = 0.48034255\n",
      "Iteration 389, loss = 0.48024574\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77875392\n",
      "Iteration 2, loss = 0.77245269\n",
      "Iteration 3, loss = 0.76635034\n",
      "Iteration 4, loss = 0.76043916\n",
      "Iteration 5, loss = 0.75472455\n",
      "Iteration 6, loss = 0.74920598\n",
      "Iteration 7, loss = 0.74386502\n",
      "Iteration 8, loss = 0.73873226\n",
      "Iteration 9, loss = 0.73375939\n",
      "Iteration 10, loss = 0.72896794\n",
      "Iteration 11, loss = 0.72435888\n",
      "Iteration 12, loss = 0.71993837\n",
      "Iteration 13, loss = 0.71569933\n",
      "Iteration 14, loss = 0.71162348\n",
      "Iteration 15, loss = 0.70771934\n",
      "Iteration 16, loss = 0.70398496\n",
      "Iteration 17, loss = 0.70039247\n",
      "Iteration 18, loss = 0.69697194\n",
      "Iteration 19, loss = 0.69371085\n",
      "Iteration 20, loss = 0.69061168\n",
      "Iteration 21, loss = 0.68766740\n",
      "Iteration 22, loss = 0.68486369\n",
      "Iteration 23, loss = 0.68220658\n",
      "Iteration 24, loss = 0.67969045\n",
      "Iteration 25, loss = 0.67729900\n",
      "Iteration 26, loss = 0.67502804\n",
      "Iteration 27, loss = 0.67288306\n",
      "Iteration 28, loss = 0.67087634\n",
      "Iteration 29, loss = 0.66896992\n",
      "Iteration 30, loss = 0.66718549\n",
      "Iteration 31, loss = 0.66550636\n",
      "Iteration 32, loss = 0.66392357\n",
      "Iteration 33, loss = 0.66243430\n",
      "Iteration 34, loss = 0.66101258\n",
      "Iteration 35, loss = 0.65968955\n",
      "Iteration 36, loss = 0.65844214\n",
      "Iteration 37, loss = 0.65725672\n",
      "Iteration 38, loss = 0.65614874\n",
      "Iteration 39, loss = 0.65509473\n",
      "Iteration 40, loss = 0.65410382\n",
      "Iteration 41, loss = 0.65315253\n",
      "Iteration 42, loss = 0.65224798\n",
      "Iteration 43, loss = 0.65138216\n",
      "Iteration 44, loss = 0.65055135\n",
      "Iteration 45, loss = 0.64975357\n",
      "Iteration 46, loss = 0.64897657\n",
      "Iteration 47, loss = 0.64822472\n",
      "Iteration 48, loss = 0.64749854\n",
      "Iteration 49, loss = 0.64678309\n",
      "Iteration 50, loss = 0.64608582\n",
      "Iteration 51, loss = 0.64539683\n",
      "Iteration 52, loss = 0.64471705\n",
      "Iteration 53, loss = 0.64404187\n",
      "Iteration 54, loss = 0.64337567\n",
      "Iteration 55, loss = 0.64271371\n",
      "Iteration 56, loss = 0.64205180\n",
      "Iteration 57, loss = 0.64139061\n",
      "Iteration 58, loss = 0.64073018\n",
      "Iteration 59, loss = 0.64006822\n",
      "Iteration 60, loss = 0.63940609\n",
      "Iteration 61, loss = 0.63874396\n",
      "Iteration 62, loss = 0.63807296\n",
      "Iteration 63, loss = 0.63740327\n",
      "Iteration 64, loss = 0.63673016\n",
      "Iteration 65, loss = 0.63605437\n",
      "Iteration 66, loss = 0.63537350\n",
      "Iteration 67, loss = 0.63468751\n",
      "Iteration 68, loss = 0.63399677\n",
      "Iteration 69, loss = 0.63330489\n",
      "Iteration 70, loss = 0.63260670\n",
      "Iteration 71, loss = 0.63190680\n",
      "Iteration 72, loss = 0.63120184\n",
      "Iteration 73, loss = 0.63049266\n",
      "Iteration 74, loss = 0.62978354\n",
      "Iteration 75, loss = 0.62906404\n",
      "Iteration 76, loss = 0.62834459\n",
      "Iteration 77, loss = 0.62762159\n",
      "Iteration 78, loss = 0.62689450\n",
      "Iteration 79, loss = 0.62616253\n",
      "Iteration 80, loss = 0.62543176\n",
      "Iteration 81, loss = 0.62469804\n",
      "Iteration 82, loss = 0.62396423\n",
      "Iteration 83, loss = 0.62322565\n",
      "Iteration 84, loss = 0.62249686\n",
      "Iteration 85, loss = 0.62174820\n",
      "Iteration 86, loss = 0.62100894\n",
      "Iteration 87, loss = 0.62026431\n",
      "Iteration 88, loss = 0.61951970\n",
      "Iteration 89, loss = 0.61876909\n",
      "Iteration 90, loss = 0.61802017\n",
      "Iteration 91, loss = 0.61727191\n",
      "Iteration 92, loss = 0.61651806\n",
      "Iteration 93, loss = 0.61575885\n",
      "Iteration 94, loss = 0.61500439\n",
      "Iteration 95, loss = 0.61424692\n",
      "Iteration 96, loss = 0.61349701\n",
      "Iteration 97, loss = 0.61273615\n",
      "Iteration 98, loss = 0.61198124\n",
      "Iteration 99, loss = 0.61122716\n",
      "Iteration 100, loss = 0.61047723\n",
      "Iteration 101, loss = 0.60972423\n",
      "Iteration 102, loss = 0.60896965\n",
      "Iteration 103, loss = 0.60821681\n",
      "Iteration 104, loss = 0.60746237\n",
      "Iteration 105, loss = 0.60670819\n",
      "Iteration 106, loss = 0.60595387\n",
      "Iteration 107, loss = 0.60520237\n",
      "Iteration 108, loss = 0.60444890\n",
      "Iteration 109, loss = 0.60369256\n",
      "Iteration 110, loss = 0.60293916\n",
      "Iteration 111, loss = 0.60218249\n",
      "Iteration 112, loss = 0.60142822\n",
      "Iteration 113, loss = 0.60067503\n",
      "Iteration 114, loss = 0.59991657\n",
      "Iteration 115, loss = 0.59916756\n",
      "Iteration 116, loss = 0.59841159\n",
      "Iteration 117, loss = 0.59766120\n",
      "Iteration 118, loss = 0.59691462\n",
      "Iteration 119, loss = 0.59615660\n",
      "Iteration 120, loss = 0.59540393\n",
      "Iteration 121, loss = 0.59465663\n",
      "Iteration 122, loss = 0.59390903\n",
      "Iteration 123, loss = 0.59316282\n",
      "Iteration 124, loss = 0.59241271\n",
      "Iteration 125, loss = 0.59166582\n",
      "Iteration 126, loss = 0.59092200\n",
      "Iteration 127, loss = 0.59017352\n",
      "Iteration 128, loss = 0.58943086\n",
      "Iteration 129, loss = 0.58868332\n",
      "Iteration 130, loss = 0.58794264\n",
      "Iteration 131, loss = 0.58720176\n",
      "Iteration 132, loss = 0.58645982\n",
      "Iteration 133, loss = 0.58571955\n",
      "Iteration 134, loss = 0.58497808\n",
      "Iteration 135, loss = 0.58424427\n",
      "Iteration 136, loss = 0.58350549\n",
      "Iteration 137, loss = 0.58276518\n",
      "Iteration 138, loss = 0.58203180\n",
      "Iteration 139, loss = 0.58129483\n",
      "Iteration 140, loss = 0.58056394\n",
      "Iteration 141, loss = 0.57983401\n",
      "Iteration 142, loss = 0.57910524\n",
      "Iteration 143, loss = 0.57838303\n",
      "Iteration 144, loss = 0.57765987\n",
      "Iteration 145, loss = 0.57694037\n",
      "Iteration 146, loss = 0.57621617\n",
      "Iteration 147, loss = 0.57550121\n",
      "Iteration 148, loss = 0.57478162\n",
      "Iteration 149, loss = 0.57406446\n",
      "Iteration 150, loss = 0.57335183\n",
      "Iteration 151, loss = 0.57264057\n",
      "Iteration 152, loss = 0.57193047\n",
      "Iteration 153, loss = 0.57122646\n",
      "Iteration 154, loss = 0.57051972\n",
      "Iteration 155, loss = 0.56981720\n",
      "Iteration 156, loss = 0.56912069\n",
      "Iteration 157, loss = 0.56841859\n",
      "Iteration 158, loss = 0.56771616\n",
      "Iteration 159, loss = 0.56701838\n",
      "Iteration 160, loss = 0.56632051\n",
      "Iteration 161, loss = 0.56562399\n",
      "Iteration 162, loss = 0.56493664\n",
      "Iteration 163, loss = 0.56423633\n",
      "Iteration 164, loss = 0.56354608\n",
      "Iteration 165, loss = 0.56285575\n",
      "Iteration 166, loss = 0.56217108\n",
      "Iteration 167, loss = 0.56148701\n",
      "Iteration 168, loss = 0.56080033\n",
      "Iteration 169, loss = 0.56011906\n",
      "Iteration 170, loss = 0.55944368\n",
      "Iteration 171, loss = 0.55876602\n",
      "Iteration 172, loss = 0.55810149\n",
      "Iteration 173, loss = 0.55742371\n",
      "Iteration 174, loss = 0.55675690\n",
      "Iteration 175, loss = 0.55609915\n",
      "Iteration 176, loss = 0.55543193\n",
      "Iteration 177, loss = 0.55477381\n",
      "Iteration 178, loss = 0.55411997\n",
      "Iteration 179, loss = 0.55346520\n",
      "Iteration 180, loss = 0.55282223\n",
      "Iteration 181, loss = 0.55216914\n",
      "Iteration 182, loss = 0.55152451\n",
      "Iteration 183, loss = 0.55088824\n",
      "Iteration 184, loss = 0.55024836\n",
      "Iteration 185, loss = 0.54961487\n",
      "Iteration 186, loss = 0.54898979\n",
      "Iteration 187, loss = 0.54835941\n",
      "Iteration 188, loss = 0.54773834\n",
      "Iteration 189, loss = 0.54711595\n",
      "Iteration 190, loss = 0.54649438\n",
      "Iteration 191, loss = 0.54588453\n",
      "Iteration 192, loss = 0.54527044\n",
      "Iteration 193, loss = 0.54465562\n",
      "Iteration 194, loss = 0.54404994\n",
      "Iteration 195, loss = 0.54344213\n",
      "Iteration 196, loss = 0.54284832\n",
      "Iteration 197, loss = 0.54224407\n",
      "Iteration 198, loss = 0.54164776\n",
      "Iteration 199, loss = 0.54105586\n",
      "Iteration 200, loss = 0.54046151\n",
      "Iteration 201, loss = 0.53987452\n",
      "Iteration 202, loss = 0.53928826\n",
      "Iteration 203, loss = 0.53870756\n",
      "Iteration 204, loss = 0.53812670\n",
      "Iteration 205, loss = 0.53755176\n",
      "Iteration 206, loss = 0.53697472\n",
      "Iteration 207, loss = 0.53640730\n",
      "Iteration 208, loss = 0.53584916\n",
      "Iteration 209, loss = 0.53528333\n",
      "Iteration 210, loss = 0.53473953\n",
      "Iteration 211, loss = 0.53418009\n",
      "Iteration 212, loss = 0.53363097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 213, loss = 0.53308810\n",
      "Iteration 214, loss = 0.53254477\n",
      "Iteration 215, loss = 0.53200130\n",
      "Iteration 216, loss = 0.53146957\n",
      "Iteration 217, loss = 0.53093604\n",
      "Iteration 218, loss = 0.53040413\n",
      "Iteration 219, loss = 0.52987662\n",
      "Iteration 220, loss = 0.52934916\n",
      "Iteration 221, loss = 0.52883012\n",
      "Iteration 222, loss = 0.52831466\n",
      "Iteration 223, loss = 0.52779348\n",
      "Iteration 224, loss = 0.52727602\n",
      "Iteration 225, loss = 0.52677483\n",
      "Iteration 226, loss = 0.52626520\n",
      "Iteration 227, loss = 0.52576754\n",
      "Iteration 228, loss = 0.52527112\n",
      "Iteration 229, loss = 0.52477718\n",
      "Iteration 230, loss = 0.52428729\n",
      "Iteration 231, loss = 0.52380027\n",
      "Iteration 232, loss = 0.52331148\n",
      "Iteration 233, loss = 0.52283471\n",
      "Iteration 234, loss = 0.52235664\n",
      "Iteration 235, loss = 0.52187895\n",
      "Iteration 236, loss = 0.52140764\n",
      "Iteration 237, loss = 0.52095347\n",
      "Iteration 238, loss = 0.52047948\n",
      "Iteration 239, loss = 0.52001873\n",
      "Iteration 240, loss = 0.51957110\n",
      "Iteration 241, loss = 0.51911219\n",
      "Iteration 242, loss = 0.51866753\n",
      "Iteration 243, loss = 0.51822414\n",
      "Iteration 244, loss = 0.51777801\n",
      "Iteration 245, loss = 0.51733719\n",
      "Iteration 246, loss = 0.51689872\n",
      "Iteration 247, loss = 0.51646601\n",
      "Iteration 248, loss = 0.51603245\n",
      "Iteration 249, loss = 0.51561116\n",
      "Iteration 250, loss = 0.51518209\n",
      "Iteration 251, loss = 0.51475694\n",
      "Iteration 252, loss = 0.51433749\n",
      "Iteration 253, loss = 0.51392408\n",
      "Iteration 254, loss = 0.51351116\n",
      "Iteration 255, loss = 0.51310508\n",
      "Iteration 256, loss = 0.51269977\n",
      "Iteration 257, loss = 0.51229291\n",
      "Iteration 258, loss = 0.51189753\n",
      "Iteration 259, loss = 0.51150571\n",
      "Iteration 260, loss = 0.51111184\n",
      "Iteration 261, loss = 0.51071832\n",
      "Iteration 262, loss = 0.51033557\n",
      "Iteration 263, loss = 0.50995964\n",
      "Iteration 264, loss = 0.50958472\n",
      "Iteration 265, loss = 0.50919787\n",
      "Iteration 266, loss = 0.50882479\n",
      "Iteration 267, loss = 0.50846120\n",
      "Iteration 268, loss = 0.50809540\n",
      "Iteration 269, loss = 0.50773157\n",
      "Iteration 270, loss = 0.50737133\n",
      "Iteration 271, loss = 0.50701829\n",
      "Iteration 272, loss = 0.50666963\n",
      "Iteration 273, loss = 0.50632280\n",
      "Iteration 274, loss = 0.50597396\n",
      "Iteration 275, loss = 0.50563123\n",
      "Iteration 276, loss = 0.50529532\n",
      "Iteration 277, loss = 0.50495771\n",
      "Iteration 278, loss = 0.50462654\n",
      "Iteration 279, loss = 0.50429446\n",
      "Iteration 280, loss = 0.50396504\n",
      "Iteration 281, loss = 0.50363981\n",
      "Iteration 282, loss = 0.50332505\n",
      "Iteration 283, loss = 0.50300022\n",
      "Iteration 284, loss = 0.50268884\n",
      "Iteration 285, loss = 0.50237160\n",
      "Iteration 286, loss = 0.50206325\n",
      "Iteration 287, loss = 0.50175556\n",
      "Iteration 288, loss = 0.50145102\n",
      "Iteration 289, loss = 0.50115146\n",
      "Iteration 290, loss = 0.50085820\n",
      "Iteration 291, loss = 0.50055780\n",
      "Iteration 292, loss = 0.50026484\n",
      "Iteration 293, loss = 0.49997758\n",
      "Iteration 294, loss = 0.49968919\n",
      "Iteration 295, loss = 0.49940487\n",
      "Iteration 296, loss = 0.49912825\n",
      "Iteration 297, loss = 0.49885122\n",
      "Iteration 298, loss = 0.49857119\n",
      "Iteration 299, loss = 0.49830176\n",
      "Iteration 300, loss = 0.49803596\n",
      "Iteration 301, loss = 0.49776569\n",
      "Iteration 302, loss = 0.49750336\n",
      "Iteration 303, loss = 0.49724420\n",
      "Iteration 304, loss = 0.49698664\n",
      "Iteration 305, loss = 0.49673292\n",
      "Iteration 306, loss = 0.49647986\n",
      "Iteration 307, loss = 0.49622905\n",
      "Iteration 308, loss = 0.49597842\n",
      "Iteration 309, loss = 0.49573592\n",
      "Iteration 310, loss = 0.49548684\n",
      "Iteration 311, loss = 0.49525020\n",
      "Iteration 312, loss = 0.49500904\n",
      "Iteration 313, loss = 0.49478295\n",
      "Iteration 314, loss = 0.49453846\n",
      "Iteration 315, loss = 0.49430988\n",
      "Iteration 316, loss = 0.49408194\n",
      "Iteration 317, loss = 0.49386094\n",
      "Iteration 318, loss = 0.49363839\n",
      "Iteration 319, loss = 0.49341207\n",
      "Iteration 320, loss = 0.49320121\n",
      "Iteration 321, loss = 0.49297700\n",
      "Iteration 322, loss = 0.49277041\n",
      "Iteration 323, loss = 0.49255482\n",
      "Iteration 324, loss = 0.49234440\n",
      "Iteration 325, loss = 0.49213592\n",
      "Iteration 326, loss = 0.49193372\n",
      "Iteration 327, loss = 0.49172912\n",
      "Iteration 328, loss = 0.49152880\n",
      "Iteration 329, loss = 0.49132998\n",
      "Iteration 330, loss = 0.49113352\n",
      "Iteration 331, loss = 0.49094175\n",
      "Iteration 332, loss = 0.49075063\n",
      "Iteration 333, loss = 0.49056289\n",
      "Iteration 334, loss = 0.49037736\n",
      "Iteration 335, loss = 0.49018712\n",
      "Iteration 336, loss = 0.49000022\n",
      "Iteration 337, loss = 0.48982077\n",
      "Iteration 338, loss = 0.48964855\n",
      "Iteration 339, loss = 0.48947141\n",
      "Iteration 340, loss = 0.48929984\n",
      "Iteration 341, loss = 0.48912645\n",
      "Iteration 342, loss = 0.48896161\n",
      "Iteration 343, loss = 0.48879369\n",
      "Iteration 344, loss = 0.48863075\n",
      "Iteration 345, loss = 0.48847254\n",
      "Iteration 346, loss = 0.48831246\n",
      "Iteration 347, loss = 0.48814914\n",
      "Iteration 348, loss = 0.48799649\n",
      "Iteration 349, loss = 0.48783864\n",
      "Iteration 350, loss = 0.48769167\n",
      "Iteration 351, loss = 0.48753860\n",
      "Iteration 352, loss = 0.48739153\n",
      "Iteration 353, loss = 0.48724370\n",
      "Iteration 354, loss = 0.48709471\n",
      "Iteration 355, loss = 0.48695415\n",
      "Iteration 356, loss = 0.48681138\n",
      "Iteration 357, loss = 0.48667850\n",
      "Iteration 358, loss = 0.48654422\n",
      "Iteration 359, loss = 0.48640091\n",
      "Iteration 360, loss = 0.48626444\n",
      "Iteration 361, loss = 0.48613428\n",
      "Iteration 362, loss = 0.48599980\n",
      "Iteration 363, loss = 0.48587715\n",
      "Iteration 364, loss = 0.48574877\n",
      "Iteration 365, loss = 0.48561910\n",
      "Iteration 366, loss = 0.48549500\n",
      "Iteration 367, loss = 0.48537548\n",
      "Iteration 368, loss = 0.48525392\n",
      "Iteration 369, loss = 0.48513438\n",
      "Iteration 370, loss = 0.48501933\n",
      "Iteration 371, loss = 0.48490539\n",
      "Iteration 372, loss = 0.48478443\n",
      "Iteration 373, loss = 0.48467144\n",
      "Iteration 374, loss = 0.48455759\n",
      "Iteration 375, loss = 0.48444504\n",
      "Iteration 376, loss = 0.48433945\n",
      "Iteration 377, loss = 0.48422598\n",
      "Iteration 378, loss = 0.48412136\n",
      "Iteration 379, loss = 0.48402326\n",
      "Iteration 380, loss = 0.48391631\n",
      "Iteration 381, loss = 0.48381335\n",
      "Iteration 382, loss = 0.48371170\n",
      "Iteration 383, loss = 0.48361230\n",
      "Iteration 384, loss = 0.48351042\n",
      "Iteration 385, loss = 0.48341686\n",
      "Iteration 386, loss = 0.48331934\n",
      "Iteration 387, loss = 0.48322727\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75507848\n",
      "Iteration 2, loss = 0.74990718\n",
      "Iteration 3, loss = 0.74492476\n",
      "Iteration 4, loss = 0.74012723\n",
      "Iteration 5, loss = 0.73552143\n",
      "Iteration 6, loss = 0.73111253\n",
      "Iteration 7, loss = 0.72688438\n",
      "Iteration 8, loss = 0.72286008\n",
      "Iteration 9, loss = 0.71898594\n",
      "Iteration 10, loss = 0.71528832\n",
      "Iteration 11, loss = 0.71176528\n",
      "Iteration 12, loss = 0.70841588\n",
      "Iteration 13, loss = 0.70523322\n",
      "Iteration 14, loss = 0.70219877\n",
      "Iteration 15, loss = 0.69931877\n",
      "Iteration 16, loss = 0.69658969\n",
      "Iteration 17, loss = 0.69398001\n",
      "Iteration 18, loss = 0.69152868\n",
      "Iteration 19, loss = 0.68921021\n",
      "Iteration 20, loss = 0.68701745\n",
      "Iteration 21, loss = 0.68495486\n",
      "Iteration 22, loss = 0.68300278\n",
      "Iteration 23, loss = 0.68116202\n",
      "Iteration 24, loss = 0.67942684\n",
      "Iteration 25, loss = 0.67779021\n",
      "Iteration 26, loss = 0.67623041\n",
      "Iteration 27, loss = 0.67476286\n",
      "Iteration 28, loss = 0.67339080\n",
      "Iteration 29, loss = 0.67208171\n",
      "Iteration 30, loss = 0.67084553\n",
      "Iteration 31, loss = 0.66967365\n",
      "Iteration 32, loss = 0.66855680\n",
      "Iteration 33, loss = 0.66749331\n",
      "Iteration 34, loss = 0.66645870\n",
      "Iteration 35, loss = 0.66547673\n",
      "Iteration 36, loss = 0.66453108\n",
      "Iteration 37, loss = 0.66361031\n",
      "Iteration 38, loss = 0.66272394\n",
      "Iteration 39, loss = 0.66185860\n",
      "Iteration 40, loss = 0.66101854\n",
      "Iteration 41, loss = 0.66018874\n",
      "Iteration 42, loss = 0.65937222\n",
      "Iteration 43, loss = 0.65856707\n",
      "Iteration 44, loss = 0.65777147\n",
      "Iteration 45, loss = 0.65698036\n",
      "Iteration 46, loss = 0.65619340\n",
      "Iteration 47, loss = 0.65541089\n",
      "Iteration 48, loss = 0.65463492\n",
      "Iteration 49, loss = 0.65385442\n",
      "Iteration 50, loss = 0.65307942\n",
      "Iteration 51, loss = 0.65229969\n",
      "Iteration 52, loss = 0.65151961\n",
      "Iteration 53, loss = 0.65073472\n",
      "Iteration 54, loss = 0.64995110\n",
      "Iteration 55, loss = 0.64916184\n",
      "Iteration 56, loss = 0.64836952\n",
      "Iteration 57, loss = 0.64757325\n",
      "Iteration 58, loss = 0.64677565\n",
      "Iteration 59, loss = 0.64597092\n",
      "Iteration 60, loss = 0.64516535\n",
      "Iteration 61, loss = 0.64435644\n",
      "Iteration 62, loss = 0.64353944\n",
      "Iteration 63, loss = 0.64272038\n",
      "Iteration 64, loss = 0.64189662\n",
      "Iteration 65, loss = 0.64107206\n",
      "Iteration 66, loss = 0.64024099\n",
      "Iteration 67, loss = 0.63940558\n",
      "Iteration 68, loss = 0.63856336\n",
      "Iteration 69, loss = 0.63772284\n",
      "Iteration 70, loss = 0.63687613\n",
      "Iteration 71, loss = 0.63602737\n",
      "Iteration 72, loss = 0.63517502\n",
      "Iteration 73, loss = 0.63431758\n",
      "Iteration 74, loss = 0.63346198\n",
      "Iteration 75, loss = 0.63259582\n",
      "Iteration 76, loss = 0.63173109\n",
      "Iteration 77, loss = 0.63086128\n",
      "Iteration 78, loss = 0.62998951\n",
      "Iteration 79, loss = 0.62911196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80, loss = 0.62823774\n",
      "Iteration 81, loss = 0.62735997\n",
      "Iteration 82, loss = 0.62648078\n",
      "Iteration 83, loss = 0.62560410\n",
      "Iteration 84, loss = 0.62473188\n",
      "Iteration 85, loss = 0.62384346\n",
      "Iteration 86, loss = 0.62296128\n",
      "Iteration 87, loss = 0.62207610\n",
      "Iteration 88, loss = 0.62119187\n",
      "Iteration 89, loss = 0.62030181\n",
      "Iteration 90, loss = 0.61941452\n",
      "Iteration 91, loss = 0.61852657\n",
      "Iteration 92, loss = 0.61764199\n",
      "Iteration 93, loss = 0.61674513\n",
      "Iteration 94, loss = 0.61585410\n",
      "Iteration 95, loss = 0.61496166\n",
      "Iteration 96, loss = 0.61407282\n",
      "Iteration 97, loss = 0.61317920\n",
      "Iteration 98, loss = 0.61228656\n",
      "Iteration 99, loss = 0.61139620\n",
      "Iteration 100, loss = 0.61050898\n",
      "Iteration 101, loss = 0.60962378\n",
      "Iteration 102, loss = 0.60873107\n",
      "Iteration 103, loss = 0.60784186\n",
      "Iteration 104, loss = 0.60695045\n",
      "Iteration 105, loss = 0.60606127\n",
      "Iteration 106, loss = 0.60517064\n",
      "Iteration 107, loss = 0.60428141\n",
      "Iteration 108, loss = 0.60339187\n",
      "Iteration 109, loss = 0.60249919\n",
      "Iteration 110, loss = 0.60160670\n",
      "Iteration 111, loss = 0.60071360\n",
      "Iteration 112, loss = 0.59982104\n",
      "Iteration 113, loss = 0.59892723\n",
      "Iteration 114, loss = 0.59803271\n",
      "Iteration 115, loss = 0.59714111\n",
      "Iteration 116, loss = 0.59624879\n",
      "Iteration 117, loss = 0.59535827\n",
      "Iteration 118, loss = 0.59446730\n",
      "Iteration 119, loss = 0.59357202\n",
      "Iteration 120, loss = 0.59267843\n",
      "Iteration 121, loss = 0.59178865\n",
      "Iteration 122, loss = 0.59090193\n",
      "Iteration 123, loss = 0.59001121\n",
      "Iteration 124, loss = 0.58911695\n",
      "Iteration 125, loss = 0.58822618\n",
      "Iteration 126, loss = 0.58733641\n",
      "Iteration 127, loss = 0.58644320\n",
      "Iteration 128, loss = 0.58555230\n",
      "Iteration 129, loss = 0.58465704\n",
      "Iteration 130, loss = 0.58376878\n",
      "Iteration 131, loss = 0.58287481\n",
      "Iteration 132, loss = 0.58198040\n",
      "Iteration 133, loss = 0.58109226\n",
      "Iteration 134, loss = 0.58019989\n",
      "Iteration 135, loss = 0.57931155\n",
      "Iteration 136, loss = 0.57842584\n",
      "Iteration 137, loss = 0.57753815\n",
      "Iteration 138, loss = 0.57665365\n",
      "Iteration 139, loss = 0.57576755\n",
      "Iteration 140, loss = 0.57488439\n",
      "Iteration 141, loss = 0.57400805\n",
      "Iteration 142, loss = 0.57313075\n",
      "Iteration 143, loss = 0.57226026\n",
      "Iteration 144, loss = 0.57138550\n",
      "Iteration 145, loss = 0.57051818\n",
      "Iteration 146, loss = 0.56964591\n",
      "Iteration 147, loss = 0.56878664\n",
      "Iteration 148, loss = 0.56792282\n",
      "Iteration 149, loss = 0.56705622\n",
      "Iteration 150, loss = 0.56619654\n",
      "Iteration 151, loss = 0.56533781\n",
      "Iteration 152, loss = 0.56448253\n",
      "Iteration 153, loss = 0.56363655\n",
      "Iteration 154, loss = 0.56278287\n",
      "Iteration 155, loss = 0.56193685\n",
      "Iteration 156, loss = 0.56109850\n",
      "Iteration 157, loss = 0.56024946\n",
      "Iteration 158, loss = 0.55940602\n",
      "Iteration 159, loss = 0.55856535\n",
      "Iteration 160, loss = 0.55772617\n",
      "Iteration 161, loss = 0.55688985\n",
      "Iteration 162, loss = 0.55606001\n",
      "Iteration 163, loss = 0.55522084\n",
      "Iteration 164, loss = 0.55439269\n",
      "Iteration 165, loss = 0.55356594\n",
      "Iteration 166, loss = 0.55274428\n",
      "Iteration 167, loss = 0.55192636\n",
      "Iteration 168, loss = 0.55109845\n",
      "Iteration 169, loss = 0.55028398\n",
      "Iteration 170, loss = 0.54947454\n",
      "Iteration 171, loss = 0.54866223\n",
      "Iteration 172, loss = 0.54786219\n",
      "Iteration 173, loss = 0.54705028\n",
      "Iteration 174, loss = 0.54624970\n",
      "Iteration 175, loss = 0.54544991\n",
      "Iteration 176, loss = 0.54465448\n",
      "Iteration 177, loss = 0.54386107\n",
      "Iteration 178, loss = 0.54306919\n",
      "Iteration 179, loss = 0.54228102\n",
      "Iteration 180, loss = 0.54149998\n",
      "Iteration 181, loss = 0.54070922\n",
      "Iteration 182, loss = 0.53993118\n",
      "Iteration 183, loss = 0.53915888\n",
      "Iteration 184, loss = 0.53838856\n",
      "Iteration 185, loss = 0.53761431\n",
      "Iteration 186, loss = 0.53685842\n",
      "Iteration 187, loss = 0.53608587\n",
      "Iteration 188, loss = 0.53533394\n",
      "Iteration 189, loss = 0.53457970\n",
      "Iteration 190, loss = 0.53382660\n",
      "Iteration 191, loss = 0.53307943\n",
      "Iteration 192, loss = 0.53233841\n",
      "Iteration 193, loss = 0.53159267\n",
      "Iteration 194, loss = 0.53085849\n",
      "Iteration 195, loss = 0.53012151\n",
      "Iteration 196, loss = 0.52939825\n",
      "Iteration 197, loss = 0.52866427\n",
      "Iteration 198, loss = 0.52794203\n",
      "Iteration 199, loss = 0.52722718\n",
      "Iteration 200, loss = 0.52650747\n",
      "Iteration 201, loss = 0.52579738\n",
      "Iteration 202, loss = 0.52509057\n",
      "Iteration 203, loss = 0.52438498\n",
      "Iteration 204, loss = 0.52368208\n",
      "Iteration 205, loss = 0.52298391\n",
      "Iteration 206, loss = 0.52228423\n",
      "Iteration 207, loss = 0.52159336\n",
      "Iteration 208, loss = 0.52091223\n",
      "Iteration 209, loss = 0.52022714\n",
      "Iteration 210, loss = 0.51956038\n",
      "Iteration 211, loss = 0.51888023\n",
      "Iteration 212, loss = 0.51821526\n",
      "Iteration 213, loss = 0.51755779\n",
      "Iteration 214, loss = 0.51689445\n",
      "Iteration 215, loss = 0.51623250\n",
      "Iteration 216, loss = 0.51558429\n",
      "Iteration 217, loss = 0.51493343\n",
      "Iteration 218, loss = 0.51428886\n",
      "Iteration 219, loss = 0.51364829\n",
      "Iteration 220, loss = 0.51300579\n",
      "Iteration 221, loss = 0.51236999\n",
      "Iteration 222, loss = 0.51174114\n",
      "Iteration 223, loss = 0.51111021\n",
      "Iteration 224, loss = 0.51048254\n",
      "Iteration 225, loss = 0.50986996\n",
      "Iteration 226, loss = 0.50924774\n",
      "Iteration 227, loss = 0.50863965\n",
      "Iteration 228, loss = 0.50802993\n",
      "Iteration 229, loss = 0.50742429\n",
      "Iteration 230, loss = 0.50681995\n",
      "Iteration 231, loss = 0.50622503\n",
      "Iteration 232, loss = 0.50562341\n",
      "Iteration 233, loss = 0.50503582\n",
      "Iteration 234, loss = 0.50445121\n",
      "Iteration 235, loss = 0.50386346\n",
      "Iteration 236, loss = 0.50328313\n",
      "Iteration 237, loss = 0.50272197\n",
      "Iteration 238, loss = 0.50214219\n",
      "Iteration 239, loss = 0.50157389\n",
      "Iteration 240, loss = 0.50101604\n",
      "Iteration 241, loss = 0.50045283\n",
      "Iteration 242, loss = 0.49989896\n",
      "Iteration 243, loss = 0.49935006\n",
      "Iteration 244, loss = 0.49880618\n",
      "Iteration 245, loss = 0.49825522\n",
      "Iteration 246, loss = 0.49771765\n",
      "Iteration 247, loss = 0.49718450\n",
      "Iteration 248, loss = 0.49665355\n",
      "Iteration 249, loss = 0.49612970\n",
      "Iteration 250, loss = 0.49560439\n",
      "Iteration 251, loss = 0.49508378\n",
      "Iteration 252, loss = 0.49456690\n",
      "Iteration 253, loss = 0.49405730\n",
      "Iteration 254, loss = 0.49354794\n",
      "Iteration 255, loss = 0.49304374\n",
      "Iteration 256, loss = 0.49254381\n",
      "Iteration 257, loss = 0.49204914\n",
      "Iteration 258, loss = 0.49155833\n",
      "Iteration 259, loss = 0.49106929\n",
      "Iteration 260, loss = 0.49058386\n",
      "Iteration 261, loss = 0.49009780\n",
      "Iteration 262, loss = 0.48961654\n",
      "Iteration 263, loss = 0.48914395\n",
      "Iteration 264, loss = 0.48867824\n",
      "Iteration 265, loss = 0.48819995\n",
      "Iteration 266, loss = 0.48773476\n",
      "Iteration 267, loss = 0.48727553\n",
      "Iteration 268, loss = 0.48682075\n",
      "Iteration 269, loss = 0.48636319\n",
      "Iteration 270, loss = 0.48591231\n",
      "Iteration 271, loss = 0.48547108\n",
      "Iteration 272, loss = 0.48502797\n",
      "Iteration 273, loss = 0.48458836\n",
      "Iteration 274, loss = 0.48414685\n",
      "Iteration 275, loss = 0.48371435\n",
      "Iteration 276, loss = 0.48328871\n",
      "Iteration 277, loss = 0.48285867\n",
      "Iteration 278, loss = 0.48243738\n",
      "Iteration 279, loss = 0.48201843\n",
      "Iteration 280, loss = 0.48159702\n",
      "Iteration 281, loss = 0.48118500\n",
      "Iteration 282, loss = 0.48078099\n",
      "Iteration 283, loss = 0.48036935\n",
      "Iteration 284, loss = 0.47996843\n",
      "Iteration 285, loss = 0.47956795\n",
      "Iteration 286, loss = 0.47916917\n",
      "Iteration 287, loss = 0.47877657\n",
      "Iteration 288, loss = 0.47838303\n",
      "Iteration 289, loss = 0.47799908\n",
      "Iteration 290, loss = 0.47761757\n",
      "Iteration 291, loss = 0.47722939\n",
      "Iteration 292, loss = 0.47685572\n",
      "Iteration 293, loss = 0.47647681\n",
      "Iteration 294, loss = 0.47610490\n",
      "Iteration 295, loss = 0.47573710\n",
      "Iteration 296, loss = 0.47537449\n",
      "Iteration 297, loss = 0.47501811\n",
      "Iteration 298, loss = 0.47465174\n",
      "Iteration 299, loss = 0.47430329\n",
      "Iteration 300, loss = 0.47395118\n",
      "Iteration 301, loss = 0.47360044\n",
      "Iteration 302, loss = 0.47325625\n",
      "Iteration 303, loss = 0.47291371\n",
      "Iteration 304, loss = 0.47257302\n",
      "Iteration 305, loss = 0.47224307\n",
      "Iteration 306, loss = 0.47190512\n",
      "Iteration 307, loss = 0.47157660\n",
      "Iteration 308, loss = 0.47124883\n",
      "Iteration 309, loss = 0.47092376\n",
      "Iteration 310, loss = 0.47060002\n",
      "Iteration 311, loss = 0.47028693\n",
      "Iteration 312, loss = 0.46997035\n",
      "Iteration 313, loss = 0.46966509\n",
      "Iteration 314, loss = 0.46935191\n",
      "Iteration 315, loss = 0.46904733\n",
      "Iteration 316, loss = 0.46874564\n",
      "Iteration 317, loss = 0.46844639\n",
      "Iteration 318, loss = 0.46814998\n",
      "Iteration 319, loss = 0.46785203\n",
      "Iteration 320, loss = 0.46756611\n",
      "Iteration 321, loss = 0.46727320\n",
      "Iteration 322, loss = 0.46699642\n",
      "Iteration 323, loss = 0.46670333\n",
      "Iteration 324, loss = 0.46642307\n",
      "Iteration 325, loss = 0.46614497\n",
      "Iteration 326, loss = 0.46586931\n",
      "Iteration 327, loss = 0.46559519\n",
      "Iteration 328, loss = 0.46532136\n",
      "Iteration 329, loss = 0.46505272\n",
      "Iteration 330, loss = 0.46478534\n",
      "Iteration 331, loss = 0.46453135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 332, loss = 0.46426499\n",
      "Iteration 333, loss = 0.46400900\n",
      "Iteration 334, loss = 0.46375132\n",
      "Iteration 335, loss = 0.46349704\n",
      "Iteration 336, loss = 0.46324850\n",
      "Iteration 337, loss = 0.46299676\n",
      "Iteration 338, loss = 0.46275822\n",
      "Iteration 339, loss = 0.46251400\n",
      "Iteration 340, loss = 0.46227670\n",
      "Iteration 341, loss = 0.46203944\n",
      "Iteration 342, loss = 0.46180371\n",
      "Iteration 343, loss = 0.46157370\n",
      "Iteration 344, loss = 0.46134634\n",
      "Iteration 345, loss = 0.46111959\n",
      "Iteration 346, loss = 0.46090204\n",
      "Iteration 347, loss = 0.46067359\n",
      "Iteration 348, loss = 0.46045355\n",
      "Iteration 349, loss = 0.46023905\n",
      "Iteration 350, loss = 0.46002870\n",
      "Iteration 351, loss = 0.45981409\n",
      "Iteration 352, loss = 0.45960930\n",
      "Iteration 353, loss = 0.45939698\n",
      "Iteration 354, loss = 0.45918692\n",
      "Iteration 355, loss = 0.45898485\n",
      "Iteration 356, loss = 0.45878647\n",
      "Iteration 357, loss = 0.45859389\n",
      "Iteration 358, loss = 0.45839731\n",
      "Iteration 359, loss = 0.45819469\n",
      "Iteration 360, loss = 0.45799877\n",
      "Iteration 361, loss = 0.45781405\n",
      "Iteration 362, loss = 0.45761762\n",
      "Iteration 363, loss = 0.45743685\n",
      "Iteration 364, loss = 0.45725281\n",
      "Iteration 365, loss = 0.45706968\n",
      "Iteration 366, loss = 0.45688530\n",
      "Iteration 367, loss = 0.45670484\n",
      "Iteration 368, loss = 0.45652785\n",
      "Iteration 369, loss = 0.45635399\n",
      "Iteration 370, loss = 0.45618143\n",
      "Iteration 371, loss = 0.45601253\n",
      "Iteration 372, loss = 0.45583532\n",
      "Iteration 373, loss = 0.45567217\n",
      "Iteration 374, loss = 0.45549742\n",
      "Iteration 375, loss = 0.45533617\n",
      "Iteration 376, loss = 0.45517322\n",
      "Iteration 377, loss = 0.45500735\n",
      "Iteration 378, loss = 0.45484990\n",
      "Iteration 379, loss = 0.45469879\n",
      "Iteration 380, loss = 0.45453598\n",
      "Iteration 381, loss = 0.45438607\n",
      "Iteration 382, loss = 0.45423318\n",
      "Iteration 383, loss = 0.45408220\n",
      "Iteration 384, loss = 0.45392987\n",
      "Iteration 385, loss = 0.45378845\n",
      "Iteration 386, loss = 0.45364093\n",
      "Iteration 387, loss = 0.45349715\n",
      "Iteration 388, loss = 0.45336182\n",
      "Iteration 389, loss = 0.45322649\n",
      "Iteration 390, loss = 0.45307928\n",
      "Iteration 391, loss = 0.45295115\n",
      "Iteration 392, loss = 0.45281758\n",
      "Iteration 393, loss = 0.45267993\n",
      "Iteration 394, loss = 0.45255490\n",
      "Iteration 395, loss = 0.45241566\n",
      "Iteration 396, loss = 0.45229340\n",
      "Iteration 397, loss = 0.45216351\n",
      "Iteration 398, loss = 0.45204899\n",
      "Iteration 399, loss = 0.45191601\n",
      "Iteration 400, loss = 0.45179509\n",
      "Iteration 401, loss = 0.45167209\n",
      "Iteration 402, loss = 0.45155615\n",
      "Iteration 403, loss = 0.45144110\n",
      "Iteration 404, loss = 0.45132839\n",
      "Iteration 405, loss = 0.45121804\n",
      "Iteration 406, loss = 0.45109978\n",
      "Iteration 407, loss = 0.45098433\n",
      "Iteration 408, loss = 0.45087726\n",
      "Iteration 409, loss = 0.45077151\n",
      "Iteration 410, loss = 0.45066127\n",
      "Iteration 411, loss = 0.45056157\n",
      "Iteration 412, loss = 0.45045449\n",
      "Iteration 413, loss = 0.45035508\n",
      "Iteration 414, loss = 0.45026019\n",
      "Iteration 415, loss = 0.45014884\n",
      "Iteration 416, loss = 0.45005303\n",
      "Iteration 417, loss = 0.44995828\n",
      "Iteration 418, loss = 0.44986488\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72792439\n",
      "Iteration 2, loss = 0.72417703\n",
      "Iteration 3, loss = 0.72059281\n",
      "Iteration 4, loss = 0.71718127\n",
      "Iteration 5, loss = 0.71395417\n",
      "Iteration 6, loss = 0.71091295\n",
      "Iteration 7, loss = 0.70803094\n",
      "Iteration 8, loss = 0.70532948\n",
      "Iteration 9, loss = 0.70276149\n",
      "Iteration 10, loss = 0.70035212\n",
      "Iteration 11, loss = 0.69807762\n",
      "Iteration 12, loss = 0.69595507\n",
      "Iteration 13, loss = 0.69396643\n",
      "Iteration 14, loss = 0.69209065\n",
      "Iteration 15, loss = 0.69033056\n",
      "Iteration 16, loss = 0.68868209\n",
      "Iteration 17, loss = 0.68711329\n",
      "Iteration 18, loss = 0.68565241\n",
      "Iteration 19, loss = 0.68427941\n",
      "Iteration 20, loss = 0.68298161\n",
      "Iteration 21, loss = 0.68176378\n",
      "Iteration 22, loss = 0.68060398\n",
      "Iteration 23, loss = 0.67950294\n",
      "Iteration 24, loss = 0.67845810\n",
      "Iteration 25, loss = 0.67746573\n",
      "Iteration 26, loss = 0.67649967\n",
      "Iteration 27, loss = 0.67557298\n",
      "Iteration 28, loss = 0.67469098\n",
      "Iteration 29, loss = 0.67383387\n",
      "Iteration 30, loss = 0.67299648\n",
      "Iteration 31, loss = 0.67218051\n",
      "Iteration 32, loss = 0.67138285\n",
      "Iteration 33, loss = 0.67059568\n",
      "Iteration 34, loss = 0.66981367\n",
      "Iteration 35, loss = 0.66904504\n",
      "Iteration 36, loss = 0.66828825\n",
      "Iteration 37, loss = 0.66752993\n",
      "Iteration 38, loss = 0.66677686\n",
      "Iteration 39, loss = 0.66602745\n",
      "Iteration 40, loss = 0.66528180\n",
      "Iteration 41, loss = 0.66453108\n",
      "Iteration 42, loss = 0.66378118\n",
      "Iteration 43, loss = 0.66303191\n",
      "Iteration 44, loss = 0.66228187\n",
      "Iteration 45, loss = 0.66152267\n",
      "Iteration 46, loss = 0.66076726\n",
      "Iteration 47, loss = 0.66000992\n",
      "Iteration 48, loss = 0.65925080\n",
      "Iteration 49, loss = 0.65848747\n",
      "Iteration 50, loss = 0.65772629\n",
      "Iteration 51, loss = 0.65695486\n",
      "Iteration 52, loss = 0.65618375\n",
      "Iteration 53, loss = 0.65540794\n",
      "Iteration 54, loss = 0.65463273\n",
      "Iteration 55, loss = 0.65385499\n",
      "Iteration 56, loss = 0.65307345\n",
      "Iteration 57, loss = 0.65229024\n",
      "Iteration 58, loss = 0.65150903\n",
      "Iteration 59, loss = 0.65071668\n",
      "Iteration 60, loss = 0.64992994\n",
      "Iteration 61, loss = 0.64913728\n",
      "Iteration 62, loss = 0.64834081\n",
      "Iteration 63, loss = 0.64754466\n",
      "Iteration 64, loss = 0.64674500\n",
      "Iteration 65, loss = 0.64594613\n",
      "Iteration 66, loss = 0.64514267\n",
      "Iteration 67, loss = 0.64433568\n",
      "Iteration 68, loss = 0.64352621\n",
      "Iteration 69, loss = 0.64271504\n",
      "Iteration 70, loss = 0.64190100\n",
      "Iteration 71, loss = 0.64108775\n",
      "Iteration 72, loss = 0.64027068\n",
      "Iteration 73, loss = 0.63945009\n",
      "Iteration 74, loss = 0.63863432\n",
      "Iteration 75, loss = 0.63780832\n",
      "Iteration 76, loss = 0.63698635\n",
      "Iteration 77, loss = 0.63615933\n",
      "Iteration 78, loss = 0.63533253\n",
      "Iteration 79, loss = 0.63449881\n",
      "Iteration 80, loss = 0.63366895\n",
      "Iteration 81, loss = 0.63283840\n",
      "Iteration 82, loss = 0.63200330\n",
      "Iteration 83, loss = 0.63117119\n",
      "Iteration 84, loss = 0.63034510\n",
      "Iteration 85, loss = 0.62950485\n",
      "Iteration 86, loss = 0.62866920\n",
      "Iteration 87, loss = 0.62783211\n",
      "Iteration 88, loss = 0.62699736\n",
      "Iteration 89, loss = 0.62615718\n",
      "Iteration 90, loss = 0.62531624\n",
      "Iteration 91, loss = 0.62447981\n",
      "Iteration 92, loss = 0.62364396\n",
      "Iteration 93, loss = 0.62280060\n",
      "Iteration 94, loss = 0.62196046\n",
      "Iteration 95, loss = 0.62112290\n",
      "Iteration 96, loss = 0.62027914\n",
      "Iteration 97, loss = 0.61943979\n",
      "Iteration 98, loss = 0.61860118\n",
      "Iteration 99, loss = 0.61776145\n",
      "Iteration 100, loss = 0.61692611\n",
      "Iteration 101, loss = 0.61609095\n",
      "Iteration 102, loss = 0.61525085\n",
      "Iteration 103, loss = 0.61441063\n",
      "Iteration 104, loss = 0.61357229\n",
      "Iteration 105, loss = 0.61273687\n",
      "Iteration 106, loss = 0.61189845\n",
      "Iteration 107, loss = 0.61106114\n",
      "Iteration 108, loss = 0.61022253\n",
      "Iteration 109, loss = 0.60938064\n",
      "Iteration 110, loss = 0.60854016\n",
      "Iteration 111, loss = 0.60769864\n",
      "Iteration 112, loss = 0.60686091\n",
      "Iteration 113, loss = 0.60601701\n",
      "Iteration 114, loss = 0.60517460\n",
      "Iteration 115, loss = 0.60433233\n",
      "Iteration 116, loss = 0.60348945\n",
      "Iteration 117, loss = 0.60265209\n",
      "Iteration 118, loss = 0.60180923\n",
      "Iteration 119, loss = 0.60096666\n",
      "Iteration 120, loss = 0.60011959\n",
      "Iteration 121, loss = 0.59928186\n",
      "Iteration 122, loss = 0.59844369\n",
      "Iteration 123, loss = 0.59760039\n",
      "Iteration 124, loss = 0.59675795\n",
      "Iteration 125, loss = 0.59591461\n",
      "Iteration 126, loss = 0.59507443\n",
      "Iteration 127, loss = 0.59423422\n",
      "Iteration 128, loss = 0.59339432\n",
      "Iteration 129, loss = 0.59255412\n",
      "Iteration 130, loss = 0.59171523\n",
      "Iteration 131, loss = 0.59088009\n",
      "Iteration 132, loss = 0.59003893\n",
      "Iteration 133, loss = 0.58920525\n",
      "Iteration 134, loss = 0.58837258\n",
      "Iteration 135, loss = 0.58754090\n",
      "Iteration 136, loss = 0.58671300\n",
      "Iteration 137, loss = 0.58588274\n",
      "Iteration 138, loss = 0.58505653\n",
      "Iteration 139, loss = 0.58422775\n",
      "Iteration 140, loss = 0.58340443\n",
      "Iteration 141, loss = 0.58258802\n",
      "Iteration 142, loss = 0.58176726\n",
      "Iteration 143, loss = 0.58095224\n",
      "Iteration 144, loss = 0.58013419\n",
      "Iteration 145, loss = 0.57932498\n",
      "Iteration 146, loss = 0.57850794\n",
      "Iteration 147, loss = 0.57770485\n",
      "Iteration 148, loss = 0.57689762\n",
      "Iteration 149, loss = 0.57609080\n",
      "Iteration 150, loss = 0.57528935\n",
      "Iteration 151, loss = 0.57448659\n",
      "Iteration 152, loss = 0.57368864\n",
      "Iteration 153, loss = 0.57290046\n",
      "Iteration 154, loss = 0.57210259\n",
      "Iteration 155, loss = 0.57131348\n",
      "Iteration 156, loss = 0.57053535\n",
      "Iteration 157, loss = 0.56973629\n",
      "Iteration 158, loss = 0.56895290\n",
      "Iteration 159, loss = 0.56816775\n",
      "Iteration 160, loss = 0.56738793\n",
      "Iteration 161, loss = 0.56660835\n",
      "Iteration 162, loss = 0.56583206\n",
      "Iteration 163, loss = 0.56505851\n",
      "Iteration 164, loss = 0.56428854\n",
      "Iteration 165, loss = 0.56352405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 166, loss = 0.56276609\n",
      "Iteration 167, loss = 0.56200637\n",
      "Iteration 168, loss = 0.56124328\n",
      "Iteration 169, loss = 0.56048597\n",
      "Iteration 170, loss = 0.55973906\n",
      "Iteration 171, loss = 0.55898900\n",
      "Iteration 172, loss = 0.55824732\n",
      "Iteration 173, loss = 0.55749984\n",
      "Iteration 174, loss = 0.55676112\n",
      "Iteration 175, loss = 0.55602492\n",
      "Iteration 176, loss = 0.55529423\n",
      "Iteration 177, loss = 0.55456092\n",
      "Iteration 178, loss = 0.55383496\n",
      "Iteration 179, loss = 0.55310929\n",
      "Iteration 180, loss = 0.55239136\n",
      "Iteration 181, loss = 0.55166383\n",
      "Iteration 182, loss = 0.55094726\n",
      "Iteration 183, loss = 0.55023846\n",
      "Iteration 184, loss = 0.54953554\n",
      "Iteration 185, loss = 0.54881658\n",
      "Iteration 186, loss = 0.54812000\n",
      "Iteration 187, loss = 0.54741582\n",
      "Iteration 188, loss = 0.54672391\n",
      "Iteration 189, loss = 0.54603392\n",
      "Iteration 190, loss = 0.54534117\n",
      "Iteration 191, loss = 0.54465735\n",
      "Iteration 192, loss = 0.54398099\n",
      "Iteration 193, loss = 0.54329815\n",
      "Iteration 194, loss = 0.54262629\n",
      "Iteration 195, loss = 0.54195126\n",
      "Iteration 196, loss = 0.54128954\n",
      "Iteration 197, loss = 0.54062012\n",
      "Iteration 198, loss = 0.53996094\n",
      "Iteration 199, loss = 0.53930753\n",
      "Iteration 200, loss = 0.53865237\n",
      "Iteration 201, loss = 0.53800302\n",
      "Iteration 202, loss = 0.53736636\n",
      "Iteration 203, loss = 0.53672019\n",
      "Iteration 204, loss = 0.53607956\n",
      "Iteration 205, loss = 0.53544145\n",
      "Iteration 206, loss = 0.53480649\n",
      "Iteration 207, loss = 0.53417836\n",
      "Iteration 208, loss = 0.53355274\n",
      "Iteration 209, loss = 0.53292998\n",
      "Iteration 210, loss = 0.53231583\n",
      "Iteration 211, loss = 0.53170082\n",
      "Iteration 212, loss = 0.53109535\n",
      "Iteration 213, loss = 0.53049715\n",
      "Iteration 214, loss = 0.52989004\n",
      "Iteration 215, loss = 0.52928686\n",
      "Iteration 216, loss = 0.52869739\n",
      "Iteration 217, loss = 0.52810419\n",
      "Iteration 218, loss = 0.52751756\n",
      "Iteration 219, loss = 0.52693656\n",
      "Iteration 220, loss = 0.52635226\n",
      "Iteration 221, loss = 0.52577523\n",
      "Iteration 222, loss = 0.52520388\n",
      "Iteration 223, loss = 0.52463465\n",
      "Iteration 224, loss = 0.52406635\n",
      "Iteration 225, loss = 0.52350955\n",
      "Iteration 226, loss = 0.52294301\n",
      "Iteration 227, loss = 0.52239196\n",
      "Iteration 228, loss = 0.52183682\n",
      "Iteration 229, loss = 0.52128944\n",
      "Iteration 230, loss = 0.52074517\n",
      "Iteration 231, loss = 0.52021151\n",
      "Iteration 232, loss = 0.51966669\n",
      "Iteration 233, loss = 0.51913223\n",
      "Iteration 234, loss = 0.51860619\n",
      "Iteration 235, loss = 0.51807472\n",
      "Iteration 236, loss = 0.51754820\n",
      "Iteration 237, loss = 0.51703806\n",
      "Iteration 238, loss = 0.51651288\n",
      "Iteration 239, loss = 0.51599747\n",
      "Iteration 240, loss = 0.51549254\n",
      "Iteration 241, loss = 0.51498122\n",
      "Iteration 242, loss = 0.51447927\n",
      "Iteration 243, loss = 0.51398197\n",
      "Iteration 244, loss = 0.51349171\n",
      "Iteration 245, loss = 0.51299277\n",
      "Iteration 246, loss = 0.51250789\n",
      "Iteration 247, loss = 0.51202684\n",
      "Iteration 248, loss = 0.51154752\n",
      "Iteration 249, loss = 0.51107395\n",
      "Iteration 250, loss = 0.51059935\n",
      "Iteration 251, loss = 0.51013312\n",
      "Iteration 252, loss = 0.50966511\n",
      "Iteration 253, loss = 0.50920575\n",
      "Iteration 254, loss = 0.50874746\n",
      "Iteration 255, loss = 0.50829358\n",
      "Iteration 256, loss = 0.50784223\n",
      "Iteration 257, loss = 0.50739764\n",
      "Iteration 258, loss = 0.50695903\n",
      "Iteration 259, loss = 0.50651387\n",
      "Iteration 260, loss = 0.50608253\n",
      "Iteration 261, loss = 0.50564889\n",
      "Iteration 262, loss = 0.50521908\n",
      "Iteration 263, loss = 0.50479374\n",
      "Iteration 264, loss = 0.50438035\n",
      "Iteration 265, loss = 0.50395243\n",
      "Iteration 266, loss = 0.50353966\n",
      "Iteration 267, loss = 0.50313109\n",
      "Iteration 268, loss = 0.50272278\n",
      "Iteration 269, loss = 0.50231998\n",
      "Iteration 270, loss = 0.50191699\n",
      "Iteration 271, loss = 0.50152380\n",
      "Iteration 272, loss = 0.50112916\n",
      "Iteration 273, loss = 0.50074092\n",
      "Iteration 274, loss = 0.50034919\n",
      "Iteration 275, loss = 0.49996249\n",
      "Iteration 276, loss = 0.49958396\n",
      "Iteration 277, loss = 0.49920428\n",
      "Iteration 278, loss = 0.49882903\n",
      "Iteration 279, loss = 0.49846156\n",
      "Iteration 280, loss = 0.49808107\n",
      "Iteration 281, loss = 0.49771658\n",
      "Iteration 282, loss = 0.49736167\n",
      "Iteration 283, loss = 0.49699392\n",
      "Iteration 284, loss = 0.49664187\n",
      "Iteration 285, loss = 0.49628315\n",
      "Iteration 286, loss = 0.49593285\n",
      "Iteration 287, loss = 0.49558166\n",
      "Iteration 288, loss = 0.49523275\n",
      "Iteration 289, loss = 0.49489476\n",
      "Iteration 290, loss = 0.49455417\n",
      "Iteration 291, loss = 0.49421582\n",
      "Iteration 292, loss = 0.49388253\n",
      "Iteration 293, loss = 0.49354786\n",
      "Iteration 294, loss = 0.49321910\n",
      "Iteration 295, loss = 0.49289367\n",
      "Iteration 296, loss = 0.49257374\n",
      "Iteration 297, loss = 0.49225512\n",
      "Iteration 298, loss = 0.49193489\n",
      "Iteration 299, loss = 0.49162554\n",
      "Iteration 300, loss = 0.49131591\n",
      "Iteration 301, loss = 0.49100378\n",
      "Iteration 302, loss = 0.49070066\n",
      "Iteration 303, loss = 0.49040005\n",
      "Iteration 304, loss = 0.49009915\n",
      "Iteration 305, loss = 0.48980549\n",
      "Iteration 306, loss = 0.48951123\n",
      "Iteration 307, loss = 0.48921967\n",
      "Iteration 308, loss = 0.48892859\n",
      "Iteration 309, loss = 0.48864587\n",
      "Iteration 310, loss = 0.48836013\n",
      "Iteration 311, loss = 0.48808612\n",
      "Iteration 312, loss = 0.48780874\n",
      "Iteration 313, loss = 0.48753745\n",
      "Iteration 314, loss = 0.48725874\n",
      "Iteration 315, loss = 0.48699221\n",
      "Iteration 316, loss = 0.48672630\n",
      "Iteration 317, loss = 0.48646397\n",
      "Iteration 318, loss = 0.48620139\n",
      "Iteration 319, loss = 0.48593993\n",
      "Iteration 320, loss = 0.48568630\n",
      "Iteration 321, loss = 0.48543246\n",
      "Iteration 322, loss = 0.48519310\n",
      "Iteration 323, loss = 0.48493530\n",
      "Iteration 324, loss = 0.48468865\n",
      "Iteration 325, loss = 0.48444434\n",
      "Iteration 326, loss = 0.48420535\n",
      "Iteration 327, loss = 0.48396908\n",
      "Iteration 328, loss = 0.48373079\n",
      "Iteration 329, loss = 0.48349752\n",
      "Iteration 330, loss = 0.48326209\n",
      "Iteration 331, loss = 0.48304245\n",
      "Iteration 332, loss = 0.48280995\n",
      "Iteration 333, loss = 0.48258933\n",
      "Iteration 334, loss = 0.48236387\n",
      "Iteration 335, loss = 0.48214620\n",
      "Iteration 336, loss = 0.48192626\n",
      "Iteration 337, loss = 0.48171265\n",
      "Iteration 338, loss = 0.48150808\n",
      "Iteration 339, loss = 0.48129466\n",
      "Iteration 340, loss = 0.48109148\n",
      "Iteration 341, loss = 0.48089114\n",
      "Iteration 342, loss = 0.48068480\n",
      "Iteration 343, loss = 0.48049076\n",
      "Iteration 344, loss = 0.48029237\n",
      "Iteration 345, loss = 0.48009595\n",
      "Iteration 346, loss = 0.47991046\n",
      "Iteration 347, loss = 0.47971385\n",
      "Iteration 348, loss = 0.47952575\n",
      "Iteration 349, loss = 0.47933817\n",
      "Iteration 350, loss = 0.47915686\n",
      "Iteration 351, loss = 0.47897689\n",
      "Iteration 352, loss = 0.47879837\n",
      "Iteration 353, loss = 0.47861876\n",
      "Iteration 354, loss = 0.47843861\n",
      "Iteration 355, loss = 0.47826848\n",
      "Iteration 356, loss = 0.47810492\n",
      "Iteration 357, loss = 0.47793617\n",
      "Iteration 358, loss = 0.47777042\n",
      "Iteration 359, loss = 0.47759954\n",
      "Iteration 360, loss = 0.47743344\n",
      "Iteration 361, loss = 0.47727782\n",
      "Iteration 362, loss = 0.47711549\n",
      "Iteration 363, loss = 0.47695660\n",
      "Iteration 364, loss = 0.47680280\n",
      "Iteration 365, loss = 0.47664963\n",
      "Iteration 366, loss = 0.47649304\n",
      "Iteration 367, loss = 0.47634441\n",
      "Iteration 368, loss = 0.47619921\n",
      "Iteration 369, loss = 0.47605171\n",
      "Iteration 370, loss = 0.47590547\n",
      "Iteration 371, loss = 0.47576235\n",
      "Iteration 372, loss = 0.47562026\n",
      "Iteration 373, loss = 0.47548382\n",
      "Iteration 374, loss = 0.47534017\n",
      "Iteration 375, loss = 0.47520555\n",
      "Iteration 376, loss = 0.47506847\n",
      "Iteration 377, loss = 0.47493492\n",
      "Iteration 378, loss = 0.47480595\n",
      "Iteration 379, loss = 0.47467821\n",
      "Iteration 380, loss = 0.47454421\n",
      "Iteration 381, loss = 0.47442716\n",
      "Iteration 382, loss = 0.47430303\n",
      "Iteration 383, loss = 0.47417139\n",
      "Iteration 384, loss = 0.47405055\n",
      "Iteration 385, loss = 0.47393528\n",
      "Iteration 386, loss = 0.47381751\n",
      "Iteration 387, loss = 0.47370015\n",
      "Iteration 388, loss = 0.47358780\n",
      "Iteration 389, loss = 0.47348005\n",
      "Iteration 390, loss = 0.47336254\n",
      "Iteration 391, loss = 0.47325457\n",
      "Iteration 392, loss = 0.47315601\n",
      "Iteration 393, loss = 0.47304057\n",
      "Iteration 394, loss = 0.47293573\n",
      "Iteration 395, loss = 0.47282635\n",
      "Iteration 396, loss = 0.47272587\n",
      "Iteration 397, loss = 0.47262682\n",
      "Iteration 398, loss = 0.47253928\n",
      "Iteration 399, loss = 0.47242848\n",
      "Iteration 400, loss = 0.47232939\n",
      "Iteration 401, loss = 0.47223274\n",
      "Iteration 402, loss = 0.47214219\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72401596\n",
      "Iteration 2, loss = 0.72046618\n",
      "Iteration 3, loss = 0.71707892\n",
      "Iteration 4, loss = 0.71386719\n",
      "Iteration 5, loss = 0.71083706\n",
      "Iteration 6, loss = 0.70798586\n",
      "Iteration 7, loss = 0.70529318\n",
      "Iteration 8, loss = 0.70276967\n",
      "Iteration 9, loss = 0.70037629\n",
      "Iteration 10, loss = 0.69813649\n",
      "Iteration 11, loss = 0.69602396\n",
      "Iteration 12, loss = 0.69405751\n",
      "Iteration 13, loss = 0.69221389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.69047769\n",
      "Iteration 15, loss = 0.68884527\n",
      "Iteration 16, loss = 0.68731655\n",
      "Iteration 17, loss = 0.68585864\n",
      "Iteration 18, loss = 0.68449821\n",
      "Iteration 19, loss = 0.68321365\n",
      "Iteration 20, loss = 0.68199553\n",
      "Iteration 21, loss = 0.68084805\n",
      "Iteration 22, loss = 0.67974751\n",
      "Iteration 23, loss = 0.67869779\n",
      "Iteration 24, loss = 0.67769363\n",
      "Iteration 25, loss = 0.67673491\n",
      "Iteration 26, loss = 0.67579458\n",
      "Iteration 27, loss = 0.67488645\n",
      "Iteration 28, loss = 0.67401507\n",
      "Iteration 29, loss = 0.67316103\n",
      "Iteration 30, loss = 0.67232157\n",
      "Iteration 31, loss = 0.67149912\n",
      "Iteration 32, loss = 0.67068869\n",
      "Iteration 33, loss = 0.66988775\n",
      "Iteration 34, loss = 0.66908776\n",
      "Iteration 35, loss = 0.66829917\n",
      "Iteration 36, loss = 0.66751986\n",
      "Iteration 37, loss = 0.66673546\n",
      "Iteration 38, loss = 0.66595463\n",
      "Iteration 39, loss = 0.66517657\n",
      "Iteration 40, loss = 0.66440127\n",
      "Iteration 41, loss = 0.66361930\n",
      "Iteration 42, loss = 0.66283809\n",
      "Iteration 43, loss = 0.66205591\n",
      "Iteration 44, loss = 0.66127271\n",
      "Iteration 45, loss = 0.66048014\n",
      "Iteration 46, loss = 0.65969248\n",
      "Iteration 47, loss = 0.65890143\n",
      "Iteration 48, loss = 0.65810933\n",
      "Iteration 49, loss = 0.65731361\n",
      "Iteration 50, loss = 0.65651886\n",
      "Iteration 51, loss = 0.65571542\n",
      "Iteration 52, loss = 0.65491275\n",
      "Iteration 53, loss = 0.65410268\n",
      "Iteration 54, loss = 0.65329504\n",
      "Iteration 55, loss = 0.65248504\n",
      "Iteration 56, loss = 0.65167310\n",
      "Iteration 57, loss = 0.65085816\n",
      "Iteration 58, loss = 0.65004600\n",
      "Iteration 59, loss = 0.64922384\n",
      "Iteration 60, loss = 0.64840777\n",
      "Iteration 61, loss = 0.64758495\n",
      "Iteration 62, loss = 0.64675801\n",
      "Iteration 63, loss = 0.64593370\n",
      "Iteration 64, loss = 0.64510490\n",
      "Iteration 65, loss = 0.64427792\n",
      "Iteration 66, loss = 0.64344712\n",
      "Iteration 67, loss = 0.64261312\n",
      "Iteration 68, loss = 0.64177691\n",
      "Iteration 69, loss = 0.64093991\n",
      "Iteration 70, loss = 0.64010131\n",
      "Iteration 71, loss = 0.63926195\n",
      "Iteration 72, loss = 0.63842039\n",
      "Iteration 73, loss = 0.63757606\n",
      "Iteration 74, loss = 0.63673716\n",
      "Iteration 75, loss = 0.63588913\n",
      "Iteration 76, loss = 0.63504502\n",
      "Iteration 77, loss = 0.63419635\n",
      "Iteration 78, loss = 0.63334946\n",
      "Iteration 79, loss = 0.63249457\n",
      "Iteration 80, loss = 0.63164656\n",
      "Iteration 81, loss = 0.63079781\n",
      "Iteration 82, loss = 0.62994357\n",
      "Iteration 83, loss = 0.62909528\n",
      "Iteration 84, loss = 0.62825178\n",
      "Iteration 85, loss = 0.62739645\n",
      "Iteration 86, loss = 0.62654601\n",
      "Iteration 87, loss = 0.62569431\n",
      "Iteration 88, loss = 0.62484502\n",
      "Iteration 89, loss = 0.62399218\n",
      "Iteration 90, loss = 0.62313900\n",
      "Iteration 91, loss = 0.62229105\n",
      "Iteration 92, loss = 0.62144283\n",
      "Iteration 93, loss = 0.62059129\n",
      "Iteration 94, loss = 0.61974221\n",
      "Iteration 95, loss = 0.61889564\n",
      "Iteration 96, loss = 0.61804347\n",
      "Iteration 97, loss = 0.61719765\n",
      "Iteration 98, loss = 0.61635206\n",
      "Iteration 99, loss = 0.61550618\n",
      "Iteration 100, loss = 0.61466612\n",
      "Iteration 101, loss = 0.61382722\n",
      "Iteration 102, loss = 0.61298237\n",
      "Iteration 103, loss = 0.61213879\n",
      "Iteration 104, loss = 0.61129876\n",
      "Iteration 105, loss = 0.61046261\n",
      "Iteration 106, loss = 0.60962371\n",
      "Iteration 107, loss = 0.60878649\n",
      "Iteration 108, loss = 0.60794886\n",
      "Iteration 109, loss = 0.60711183\n",
      "Iteration 110, loss = 0.60627571\n",
      "Iteration 111, loss = 0.60543952\n",
      "Iteration 112, loss = 0.60460752\n",
      "Iteration 113, loss = 0.60377190\n",
      "Iteration 114, loss = 0.60293774\n",
      "Iteration 115, loss = 0.60210621\n",
      "Iteration 116, loss = 0.60127201\n",
      "Iteration 117, loss = 0.60044621\n",
      "Iteration 118, loss = 0.59961464\n",
      "Iteration 119, loss = 0.59878792\n",
      "Iteration 120, loss = 0.59795532\n",
      "Iteration 121, loss = 0.59713474\n",
      "Iteration 122, loss = 0.59631393\n",
      "Iteration 123, loss = 0.59548886\n",
      "Iteration 124, loss = 0.59466276\n",
      "Iteration 125, loss = 0.59384058\n",
      "Iteration 126, loss = 0.59302197\n",
      "Iteration 127, loss = 0.59220364\n",
      "Iteration 128, loss = 0.59138697\n",
      "Iteration 129, loss = 0.59057191\n",
      "Iteration 130, loss = 0.58975811\n",
      "Iteration 131, loss = 0.58895084\n",
      "Iteration 132, loss = 0.58813598\n",
      "Iteration 133, loss = 0.58733125\n",
      "Iteration 134, loss = 0.58652881\n",
      "Iteration 135, loss = 0.58572722\n",
      "Iteration 136, loss = 0.58493104\n",
      "Iteration 137, loss = 0.58413303\n",
      "Iteration 138, loss = 0.58334232\n",
      "Iteration 139, loss = 0.58254628\n",
      "Iteration 140, loss = 0.58176015\n",
      "Iteration 141, loss = 0.58097963\n",
      "Iteration 142, loss = 0.58019678\n",
      "Iteration 143, loss = 0.57942028\n",
      "Iteration 144, loss = 0.57864038\n",
      "Iteration 145, loss = 0.57787045\n",
      "Iteration 146, loss = 0.57709440\n",
      "Iteration 147, loss = 0.57633232\n",
      "Iteration 148, loss = 0.57556699\n",
      "Iteration 149, loss = 0.57480543\n",
      "Iteration 150, loss = 0.57405017\n",
      "Iteration 151, loss = 0.57329462\n",
      "Iteration 152, loss = 0.57254719\n",
      "Iteration 153, loss = 0.57180821\n",
      "Iteration 154, loss = 0.57106256\n",
      "Iteration 155, loss = 0.57032525\n",
      "Iteration 156, loss = 0.56959739\n",
      "Iteration 157, loss = 0.56885416\n",
      "Iteration 158, loss = 0.56812396\n",
      "Iteration 159, loss = 0.56739334\n",
      "Iteration 160, loss = 0.56666845\n",
      "Iteration 161, loss = 0.56594611\n",
      "Iteration 162, loss = 0.56522667\n",
      "Iteration 163, loss = 0.56450897\n",
      "Iteration 164, loss = 0.56379665\n",
      "Iteration 165, loss = 0.56309075\n",
      "Iteration 166, loss = 0.56239159\n",
      "Iteration 167, loss = 0.56169131\n",
      "Iteration 168, loss = 0.56099012\n",
      "Iteration 169, loss = 0.56029367\n",
      "Iteration 170, loss = 0.55960783\n",
      "Iteration 171, loss = 0.55892174\n",
      "Iteration 172, loss = 0.55824359\n",
      "Iteration 173, loss = 0.55756035\n",
      "Iteration 174, loss = 0.55688722\n",
      "Iteration 175, loss = 0.55621560\n",
      "Iteration 176, loss = 0.55555087\n",
      "Iteration 177, loss = 0.55488607\n",
      "Iteration 178, loss = 0.55422649\n",
      "Iteration 179, loss = 0.55356670\n",
      "Iteration 180, loss = 0.55291818\n",
      "Iteration 181, loss = 0.55225999\n",
      "Iteration 182, loss = 0.55161518\n",
      "Iteration 183, loss = 0.55097617\n",
      "Iteration 184, loss = 0.55034615\n",
      "Iteration 185, loss = 0.54969692\n",
      "Iteration 186, loss = 0.54906965\n",
      "Iteration 187, loss = 0.54843860\n",
      "Iteration 188, loss = 0.54782070\n",
      "Iteration 189, loss = 0.54720332\n",
      "Iteration 190, loss = 0.54658459\n",
      "Iteration 191, loss = 0.54597502\n",
      "Iteration 192, loss = 0.54537667\n",
      "Iteration 193, loss = 0.54476848\n",
      "Iteration 194, loss = 0.54417271\n",
      "Iteration 195, loss = 0.54357465\n",
      "Iteration 196, loss = 0.54299038\n",
      "Iteration 197, loss = 0.54239960\n",
      "Iteration 198, loss = 0.54181908\n",
      "Iteration 199, loss = 0.54124359\n",
      "Iteration 200, loss = 0.54066809\n",
      "Iteration 201, loss = 0.54009823\n",
      "Iteration 202, loss = 0.53953967\n",
      "Iteration 203, loss = 0.53897345\n",
      "Iteration 204, loss = 0.53841372\n",
      "Iteration 205, loss = 0.53785766\n",
      "Iteration 206, loss = 0.53730332\n",
      "Iteration 207, loss = 0.53675628\n",
      "Iteration 208, loss = 0.53621401\n",
      "Iteration 209, loss = 0.53567027\n",
      "Iteration 210, loss = 0.53513755\n",
      "Iteration 211, loss = 0.53460575\n",
      "Iteration 212, loss = 0.53408208\n",
      "Iteration 213, loss = 0.53356741\n",
      "Iteration 214, loss = 0.53304201\n",
      "Iteration 215, loss = 0.53252157\n",
      "Iteration 216, loss = 0.53201626\n",
      "Iteration 217, loss = 0.53150760\n",
      "Iteration 218, loss = 0.53100516\n",
      "Iteration 219, loss = 0.53050839\n",
      "Iteration 220, loss = 0.53000700\n",
      "Iteration 221, loss = 0.52951502\n",
      "Iteration 222, loss = 0.52902703\n",
      "Iteration 223, loss = 0.52854148\n",
      "Iteration 224, loss = 0.52805975\n",
      "Iteration 225, loss = 0.52758904\n",
      "Iteration 226, loss = 0.52710925\n",
      "Iteration 227, loss = 0.52664123\n",
      "Iteration 228, loss = 0.52617436\n",
      "Iteration 229, loss = 0.52571636\n",
      "Iteration 230, loss = 0.52525687\n",
      "Iteration 231, loss = 0.52480871\n",
      "Iteration 232, loss = 0.52435270\n",
      "Iteration 233, loss = 0.52390583\n",
      "Iteration 234, loss = 0.52346512\n",
      "Iteration 235, loss = 0.52302010\n",
      "Iteration 236, loss = 0.52258222\n",
      "Iteration 237, loss = 0.52215763\n",
      "Iteration 238, loss = 0.52171847\n",
      "Iteration 239, loss = 0.52129467\n",
      "Iteration 240, loss = 0.52087828\n",
      "Iteration 241, loss = 0.52045062\n",
      "Iteration 242, loss = 0.52003748\n",
      "Iteration 243, loss = 0.51962545\n",
      "Iteration 244, loss = 0.51922633\n",
      "Iteration 245, loss = 0.51881098\n",
      "Iteration 246, loss = 0.51841297\n",
      "Iteration 247, loss = 0.51801717\n",
      "Iteration 248, loss = 0.51762465\n",
      "Iteration 249, loss = 0.51723546\n",
      "Iteration 250, loss = 0.51684721\n",
      "Iteration 251, loss = 0.51646713\n",
      "Iteration 252, loss = 0.51608529\n",
      "Iteration 253, loss = 0.51571116\n",
      "Iteration 254, loss = 0.51533857\n",
      "Iteration 255, loss = 0.51497054\n",
      "Iteration 256, loss = 0.51460115\n",
      "Iteration 257, loss = 0.51424306\n",
      "Iteration 258, loss = 0.51388467\n",
      "Iteration 259, loss = 0.51352458\n",
      "Iteration 260, loss = 0.51317484\n",
      "Iteration 261, loss = 0.51282634\n",
      "Iteration 262, loss = 0.51247819\n",
      "Iteration 263, loss = 0.51213509\n",
      "Iteration 264, loss = 0.51180418\n",
      "Iteration 265, loss = 0.51146060\n",
      "Iteration 266, loss = 0.51112703\n",
      "Iteration 267, loss = 0.51080007\n",
      "Iteration 268, loss = 0.51047248\n",
      "Iteration 269, loss = 0.51015723\n",
      "Iteration 270, loss = 0.50983206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 271, loss = 0.50951568\n",
      "Iteration 272, loss = 0.50920462\n",
      "Iteration 273, loss = 0.50889470\n",
      "Iteration 274, loss = 0.50858046\n",
      "Iteration 275, loss = 0.50827558\n",
      "Iteration 276, loss = 0.50797544\n",
      "Iteration 277, loss = 0.50767660\n",
      "Iteration 278, loss = 0.50737999\n",
      "Iteration 279, loss = 0.50708932\n",
      "Iteration 280, loss = 0.50679348\n",
      "Iteration 281, loss = 0.50650590\n",
      "Iteration 282, loss = 0.50622797\n",
      "Iteration 283, loss = 0.50594023\n",
      "Iteration 284, loss = 0.50566930\n",
      "Iteration 285, loss = 0.50538514\n",
      "Iteration 286, loss = 0.50511706\n",
      "Iteration 287, loss = 0.50483820\n",
      "Iteration 288, loss = 0.50457164\n",
      "Iteration 289, loss = 0.50430577\n",
      "Iteration 290, loss = 0.50404191\n",
      "Iteration 291, loss = 0.50377950\n",
      "Iteration 292, loss = 0.50352075\n",
      "Iteration 293, loss = 0.50326172\n",
      "Iteration 294, loss = 0.50300901\n",
      "Iteration 295, loss = 0.50275952\n",
      "Iteration 296, loss = 0.50251055\n",
      "Iteration 297, loss = 0.50226817\n",
      "Iteration 298, loss = 0.50202156\n",
      "Iteration 299, loss = 0.50178603\n",
      "Iteration 300, loss = 0.50154822\n",
      "Iteration 301, loss = 0.50131061\n",
      "Iteration 302, loss = 0.50107886\n",
      "Iteration 303, loss = 0.50085279\n",
      "Iteration 304, loss = 0.50062280\n",
      "Iteration 305, loss = 0.50039791\n",
      "Iteration 306, loss = 0.50017603\n",
      "Iteration 307, loss = 0.49995456\n",
      "Iteration 308, loss = 0.49973643\n",
      "Iteration 309, loss = 0.49952240\n",
      "Iteration 310, loss = 0.49930636\n",
      "Iteration 311, loss = 0.49910110\n",
      "Iteration 312, loss = 0.49889559\n",
      "Iteration 313, loss = 0.49869325\n",
      "Iteration 314, loss = 0.49848601\n",
      "Iteration 315, loss = 0.49828489\n",
      "Iteration 316, loss = 0.49808485\n",
      "Iteration 317, loss = 0.49789639\n",
      "Iteration 318, loss = 0.49769815\n",
      "Iteration 319, loss = 0.49750418\n",
      "Iteration 320, loss = 0.49732038\n",
      "Iteration 321, loss = 0.49712897\n",
      "Iteration 322, loss = 0.49695342\n",
      "Iteration 323, loss = 0.49676538\n",
      "Iteration 324, loss = 0.49658413\n",
      "Iteration 325, loss = 0.49640526\n",
      "Iteration 326, loss = 0.49623117\n",
      "Iteration 327, loss = 0.49605710\n",
      "Iteration 328, loss = 0.49588764\n",
      "Iteration 329, loss = 0.49571692\n",
      "Iteration 330, loss = 0.49554717\n",
      "Iteration 331, loss = 0.49539182\n",
      "Iteration 332, loss = 0.49522204\n",
      "Iteration 333, loss = 0.49506369\n",
      "Iteration 334, loss = 0.49489708\n",
      "Iteration 335, loss = 0.49474426\n",
      "Iteration 336, loss = 0.49458537\n",
      "Iteration 337, loss = 0.49443355\n",
      "Iteration 338, loss = 0.49428942\n",
      "Iteration 339, loss = 0.49413821\n",
      "Iteration 340, loss = 0.49399195\n",
      "Iteration 341, loss = 0.49385099\n",
      "Iteration 342, loss = 0.49370306\n",
      "Iteration 343, loss = 0.49356729\n",
      "Iteration 344, loss = 0.49342742\n",
      "Iteration 345, loss = 0.49329167\n",
      "Iteration 346, loss = 0.49316104\n",
      "Iteration 347, loss = 0.49302257\n",
      "Iteration 348, loss = 0.49289332\n",
      "Iteration 349, loss = 0.49276146\n",
      "Iteration 350, loss = 0.49263393\n",
      "Iteration 351, loss = 0.49251053\n",
      "Iteration 352, loss = 0.49238698\n",
      "Iteration 353, loss = 0.49226326\n",
      "Iteration 354, loss = 0.49213874\n",
      "Iteration 355, loss = 0.49202214\n",
      "Iteration 356, loss = 0.49190926\n",
      "Iteration 357, loss = 0.49179795\n",
      "Iteration 358, loss = 0.49168571\n",
      "Iteration 359, loss = 0.49156644\n",
      "Iteration 360, loss = 0.49145317\n",
      "Iteration 361, loss = 0.49134900\n",
      "Iteration 362, loss = 0.49124090\n",
      "Iteration 363, loss = 0.49113525\n",
      "Iteration 364, loss = 0.49103228\n",
      "Iteration 365, loss = 0.49092700\n",
      "Iteration 366, loss = 0.49082206\n",
      "Iteration 367, loss = 0.49072602\n",
      "Iteration 368, loss = 0.49062740\n",
      "Iteration 369, loss = 0.49053146\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "y = y.ravel()\n",
    "cv_results = model_selection.cross_val_score(clf, x, y, cv=kfold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado da execução desse algoritmo, logo abaixo, será usado na seção [3 Resultados e Conclusões](https://github.com/GSansigolo/CAP-240-394/blob/master/src/conclusao/conclusao.ipynb) para uma comparação gráfica dos algoritmos de aprendizado de máquina. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74792597 0.9023612  0.720485   0.71410338 0.73261008 0.7626037\n",
      " 0.76451819 0.57115507 0.62667518 0.69687301]\n"
     ]
    }
   ],
   "source": [
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Teste 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próximo passo, será criado duas váriaveis com dados a ser passado ao algoritmo de treinamento. \n",
    "\n",
    "- `x` - Essa variável contém como dados de entrada os índices espectrais **`ndvi`**, **`nbrl`**, suas diferenças o **`dif_ndvi`** e o **`dif_dnbrl`**, e as **medianas das bandas [2,5]**. \n",
    "\n",
    "- `y` - Essa variável contém os dados da coluna **`verifica`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df[['ndvi','nbrl','dif_ndvi','dif_dnbrl','medianb2','medianb3','medianb4','medianb5']])\n",
    "y = np.array(df[['verifica']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será usado as mesmas configurações da rede, realizadas no **Teste 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73338756\n",
      "Iteration 2, loss = 0.72909954\n",
      "Iteration 3, loss = 0.72502381\n",
      "Iteration 4, loss = 0.72112837\n",
      "Iteration 5, loss = 0.71741628\n",
      "Iteration 6, loss = 0.71387032\n",
      "Iteration 7, loss = 0.71051368\n",
      "Iteration 8, loss = 0.70733442\n",
      "Iteration 9, loss = 0.70431646\n",
      "Iteration 10, loss = 0.70144719\n",
      "Iteration 11, loss = 0.69876117\n",
      "Iteration 12, loss = 0.69622205\n",
      "Iteration 13, loss = 0.69384170\n",
      "Iteration 14, loss = 0.69159176\n",
      "Iteration 15, loss = 0.68947375\n",
      "Iteration 16, loss = 0.68750556\n",
      "Iteration 17, loss = 0.68564394\n",
      "Iteration 18, loss = 0.68390339\n",
      "Iteration 19, loss = 0.68227069\n",
      "Iteration 20, loss = 0.68075519\n",
      "Iteration 21, loss = 0.67932506\n",
      "Iteration 22, loss = 0.67799447\n",
      "Iteration 23, loss = 0.67675085\n",
      "Iteration 24, loss = 0.67557754\n",
      "Iteration 25, loss = 0.67447964\n",
      "Iteration 26, loss = 0.67343312\n",
      "Iteration 27, loss = 0.67245627\n",
      "Iteration 28, loss = 0.67153246\n",
      "Iteration 29, loss = 0.67064781\n",
      "Iteration 30, loss = 0.66981018\n",
      "Iteration 31, loss = 0.66901328\n",
      "Iteration 32, loss = 0.66824096\n",
      "Iteration 33, loss = 0.66750175\n",
      "Iteration 34, loss = 0.66677370\n",
      "Iteration 35, loss = 0.66607363\n",
      "Iteration 36, loss = 0.66538965\n",
      "Iteration 37, loss = 0.66471839\n",
      "Iteration 38, loss = 0.66405877\n",
      "Iteration 39, loss = 0.66340614\n",
      "Iteration 40, loss = 0.66276376\n",
      "Iteration 41, loss = 0.66212025\n",
      "Iteration 42, loss = 0.66148147\n",
      "Iteration 43, loss = 0.66084625\n",
      "Iteration 44, loss = 0.66021207\n",
      "Iteration 45, loss = 0.65957882\n",
      "Iteration 46, loss = 0.65894260\n",
      "Iteration 47, loss = 0.65830769\n",
      "Iteration 48, loss = 0.65767210\n",
      "Iteration 49, loss = 0.65703207\n",
      "Iteration 50, loss = 0.65638613\n",
      "Iteration 51, loss = 0.65574057\n",
      "Iteration 52, loss = 0.65509115\n",
      "Iteration 53, loss = 0.65443986\n",
      "Iteration 54, loss = 0.65378411\n",
      "Iteration 55, loss = 0.65312826\n",
      "Iteration 56, loss = 0.65246239\n",
      "Iteration 57, loss = 0.65179689\n",
      "Iteration 58, loss = 0.65112877\n",
      "Iteration 59, loss = 0.65045947\n",
      "Iteration 60, loss = 0.64977781\n",
      "Iteration 61, loss = 0.64910134\n",
      "Iteration 62, loss = 0.64841978\n",
      "Iteration 63, loss = 0.64773941\n",
      "Iteration 64, loss = 0.64705302\n",
      "Iteration 65, loss = 0.64636660\n",
      "Iteration 66, loss = 0.64567882\n",
      "Iteration 67, loss = 0.64498670\n",
      "Iteration 68, loss = 0.64429243\n",
      "Iteration 69, loss = 0.64359520\n",
      "Iteration 70, loss = 0.64289616\n",
      "Iteration 71, loss = 0.64219763\n",
      "Iteration 72, loss = 0.64149474\n",
      "Iteration 73, loss = 0.64079021\n",
      "Iteration 74, loss = 0.64008346\n",
      "Iteration 75, loss = 0.63937298\n",
      "Iteration 76, loss = 0.63865945\n",
      "Iteration 77, loss = 0.63794469\n",
      "Iteration 78, loss = 0.63722597\n",
      "Iteration 79, loss = 0.63650665\n",
      "Iteration 80, loss = 0.63578955\n",
      "Iteration 81, loss = 0.63506507\n",
      "Iteration 82, loss = 0.63434208\n",
      "Iteration 83, loss = 0.63362302\n",
      "Iteration 84, loss = 0.63289845\n",
      "Iteration 85, loss = 0.63217392\n",
      "Iteration 86, loss = 0.63144711\n",
      "Iteration 87, loss = 0.63072478\n",
      "Iteration 88, loss = 0.62999424\n",
      "Iteration 89, loss = 0.62926574\n",
      "Iteration 90, loss = 0.62854176\n",
      "Iteration 91, loss = 0.62781377\n",
      "Iteration 92, loss = 0.62708590\n",
      "Iteration 93, loss = 0.62635712\n",
      "Iteration 94, loss = 0.62562711\n",
      "Iteration 95, loss = 0.62489665\n",
      "Iteration 96, loss = 0.62417729\n",
      "Iteration 97, loss = 0.62344292\n",
      "Iteration 98, loss = 0.62272321\n",
      "Iteration 99, loss = 0.62198786\n",
      "Iteration 100, loss = 0.62126060\n",
      "Iteration 101, loss = 0.62053083\n",
      "Iteration 102, loss = 0.61980233\n",
      "Iteration 103, loss = 0.61906811\n",
      "Iteration 104, loss = 0.61833534\n",
      "Iteration 105, loss = 0.61760476\n",
      "Iteration 106, loss = 0.61687274\n",
      "Iteration 107, loss = 0.61613983\n",
      "Iteration 108, loss = 0.61541267\n",
      "Iteration 109, loss = 0.61467540\n",
      "Iteration 110, loss = 0.61394122\n",
      "Iteration 111, loss = 0.61320408\n",
      "Iteration 112, loss = 0.61246987\n",
      "Iteration 113, loss = 0.61173033\n",
      "Iteration 114, loss = 0.61099257\n",
      "Iteration 115, loss = 0.61025841\n",
      "Iteration 116, loss = 0.60952064\n",
      "Iteration 117, loss = 0.60877761\n",
      "Iteration 118, loss = 0.60804157\n",
      "Iteration 119, loss = 0.60729849\n",
      "Iteration 120, loss = 0.60656051\n",
      "Iteration 121, loss = 0.60582505\n",
      "Iteration 122, loss = 0.60508624\n",
      "Iteration 123, loss = 0.60435247\n",
      "Iteration 124, loss = 0.60361744\n",
      "Iteration 125, loss = 0.60288129\n",
      "Iteration 126, loss = 0.60215129\n",
      "Iteration 127, loss = 0.60141502\n",
      "Iteration 128, loss = 0.60068251\n",
      "Iteration 129, loss = 0.59995332\n",
      "Iteration 130, loss = 0.59922332\n",
      "Iteration 131, loss = 0.59848606\n",
      "Iteration 132, loss = 0.59775987\n",
      "Iteration 133, loss = 0.59703785\n",
      "Iteration 134, loss = 0.59631032\n",
      "Iteration 135, loss = 0.59559517\n",
      "Iteration 136, loss = 0.59486279\n",
      "Iteration 137, loss = 0.59414044\n",
      "Iteration 138, loss = 0.59341639\n",
      "Iteration 139, loss = 0.59269898\n",
      "Iteration 140, loss = 0.59198079\n",
      "Iteration 141, loss = 0.59126587\n",
      "Iteration 142, loss = 0.59054318\n",
      "Iteration 143, loss = 0.58983115\n",
      "Iteration 144, loss = 0.58911431\n",
      "Iteration 145, loss = 0.58839748\n",
      "Iteration 146, loss = 0.58768648\n",
      "Iteration 147, loss = 0.58697121\n",
      "Iteration 148, loss = 0.58626179\n",
      "Iteration 149, loss = 0.58554920\n",
      "Iteration 150, loss = 0.58484000\n",
      "Iteration 151, loss = 0.58413334\n",
      "Iteration 152, loss = 0.58342541\n",
      "Iteration 153, loss = 0.58272049\n",
      "Iteration 154, loss = 0.58201814\n",
      "Iteration 155, loss = 0.58131557\n",
      "Iteration 156, loss = 0.58061608\n",
      "Iteration 157, loss = 0.57991867\n",
      "Iteration 158, loss = 0.57923269\n",
      "Iteration 159, loss = 0.57853338\n",
      "Iteration 160, loss = 0.57784128\n",
      "Iteration 161, loss = 0.57715462\n",
      "Iteration 162, loss = 0.57646629\n",
      "Iteration 163, loss = 0.57578213\n",
      "Iteration 164, loss = 0.57509743\n",
      "Iteration 165, loss = 0.57441669\n",
      "Iteration 166, loss = 0.57373256\n",
      "Iteration 167, loss = 0.57305258\n",
      "Iteration 168, loss = 0.57238083\n",
      "Iteration 169, loss = 0.57170559\n",
      "Iteration 170, loss = 0.57103073\n",
      "Iteration 171, loss = 0.57035766\n",
      "Iteration 172, loss = 0.56969264\n",
      "Iteration 173, loss = 0.56902251\n",
      "Iteration 174, loss = 0.56835746\n",
      "Iteration 175, loss = 0.56769887\n",
      "Iteration 176, loss = 0.56703357\n",
      "Iteration 177, loss = 0.56637542\n",
      "Iteration 178, loss = 0.56571544\n",
      "Iteration 179, loss = 0.56506123\n",
      "Iteration 180, loss = 0.56441554\n",
      "Iteration 181, loss = 0.56376547\n",
      "Iteration 182, loss = 0.56312296\n",
      "Iteration 183, loss = 0.56248272\n",
      "Iteration 184, loss = 0.56184645\n",
      "Iteration 185, loss = 0.56121501\n",
      "Iteration 186, loss = 0.56059163\n",
      "Iteration 187, loss = 0.55995640\n",
      "Iteration 188, loss = 0.55932774\n",
      "Iteration 189, loss = 0.55870621\n",
      "Iteration 190, loss = 0.55808132\n",
      "Iteration 191, loss = 0.55746508\n",
      "Iteration 192, loss = 0.55684735\n",
      "Iteration 193, loss = 0.55624028\n",
      "Iteration 194, loss = 0.55563442\n",
      "Iteration 195, loss = 0.55502751\n",
      "Iteration 196, loss = 0.55442674\n",
      "Iteration 197, loss = 0.55382790\n",
      "Iteration 198, loss = 0.55323597\n",
      "Iteration 199, loss = 0.55264148\n",
      "Iteration 200, loss = 0.55204916\n",
      "Iteration 201, loss = 0.55146413\n",
      "Iteration 202, loss = 0.55087876\n",
      "Iteration 203, loss = 0.55029963\n",
      "Iteration 204, loss = 0.54972399\n",
      "Iteration 205, loss = 0.54914252\n",
      "Iteration 206, loss = 0.54857087\n",
      "Iteration 207, loss = 0.54801382\n",
      "Iteration 208, loss = 0.54743740\n",
      "Iteration 209, loss = 0.54686607\n",
      "Iteration 210, loss = 0.54630605\n",
      "Iteration 211, loss = 0.54573711\n",
      "Iteration 212, loss = 0.54518477\n",
      "Iteration 213, loss = 0.54463055\n",
      "Iteration 214, loss = 0.54407359\n",
      "Iteration 215, loss = 0.54352707\n",
      "Iteration 216, loss = 0.54298041\n",
      "Iteration 217, loss = 0.54243714\n",
      "Iteration 218, loss = 0.54190032\n",
      "Iteration 219, loss = 0.54135980\n",
      "Iteration 220, loss = 0.54082267\n",
      "Iteration 221, loss = 0.54029422\n",
      "Iteration 222, loss = 0.53976659\n",
      "Iteration 223, loss = 0.53924340\n",
      "Iteration 224, loss = 0.53871908\n",
      "Iteration 225, loss = 0.53819904\n",
      "Iteration 226, loss = 0.53768718\n",
      "Iteration 227, loss = 0.53718299\n",
      "Iteration 228, loss = 0.53667443\n",
      "Iteration 229, loss = 0.53616785\n",
      "Iteration 230, loss = 0.53566385\n",
      "Iteration 231, loss = 0.53516552\n",
      "Iteration 232, loss = 0.53466991\n",
      "Iteration 233, loss = 0.53417839\n",
      "Iteration 234, loss = 0.53368812\n",
      "Iteration 235, loss = 0.53319937\n",
      "Iteration 236, loss = 0.53271854\n",
      "Iteration 237, loss = 0.53223129\n",
      "Iteration 238, loss = 0.53175658\n",
      "Iteration 239, loss = 0.53127655\n",
      "Iteration 240, loss = 0.53080615\n",
      "Iteration 241, loss = 0.53034116\n",
      "Iteration 242, loss = 0.52987806\n",
      "Iteration 243, loss = 0.52940660\n",
      "Iteration 244, loss = 0.52894871\n",
      "Iteration 245, loss = 0.52849291\n",
      "Iteration 246, loss = 0.52803660\n",
      "Iteration 247, loss = 0.52758729\n",
      "Iteration 248, loss = 0.52714684\n",
      "Iteration 249, loss = 0.52669225\n",
      "Iteration 250, loss = 0.52624233\n",
      "Iteration 251, loss = 0.52580414\n",
      "Iteration 252, loss = 0.52536258\n",
      "Iteration 253, loss = 0.52493382\n",
      "Iteration 254, loss = 0.52450143\n",
      "Iteration 255, loss = 0.52407204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.52364907\n",
      "Iteration 257, loss = 0.52322491\n",
      "Iteration 258, loss = 0.52280271\n",
      "Iteration 259, loss = 0.52239321\n",
      "Iteration 260, loss = 0.52197383\n",
      "Iteration 261, loss = 0.52156048\n",
      "Iteration 262, loss = 0.52115411\n",
      "Iteration 263, loss = 0.52074453\n",
      "Iteration 264, loss = 0.52034110\n",
      "Iteration 265, loss = 0.51994641\n",
      "Iteration 266, loss = 0.51954688\n",
      "Iteration 267, loss = 0.51915686\n",
      "Iteration 268, loss = 0.51876520\n",
      "Iteration 269, loss = 0.51837896\n",
      "Iteration 270, loss = 0.51798852\n",
      "Iteration 271, loss = 0.51760888\n",
      "Iteration 272, loss = 0.51724083\n",
      "Iteration 273, loss = 0.51685823\n",
      "Iteration 274, loss = 0.51648390\n",
      "Iteration 275, loss = 0.51611295\n",
      "Iteration 276, loss = 0.51574784\n",
      "Iteration 277, loss = 0.51537834\n",
      "Iteration 278, loss = 0.51502192\n",
      "Iteration 279, loss = 0.51466546\n",
      "Iteration 280, loss = 0.51430002\n",
      "Iteration 281, loss = 0.51395076\n",
      "Iteration 282, loss = 0.51360461\n",
      "Iteration 283, loss = 0.51324991\n",
      "Iteration 284, loss = 0.51291453\n",
      "Iteration 285, loss = 0.51256665\n",
      "Iteration 286, loss = 0.51222519\n",
      "Iteration 287, loss = 0.51189360\n",
      "Iteration 288, loss = 0.51155924\n",
      "Iteration 289, loss = 0.51123629\n",
      "Iteration 290, loss = 0.51091711\n",
      "Iteration 291, loss = 0.51058229\n",
      "Iteration 292, loss = 0.51027779\n",
      "Iteration 293, loss = 0.50995230\n",
      "Iteration 294, loss = 0.50963014\n",
      "Iteration 295, loss = 0.50932050\n",
      "Iteration 296, loss = 0.50901635\n",
      "Iteration 297, loss = 0.50870586\n",
      "Iteration 298, loss = 0.50840533\n",
      "Iteration 299, loss = 0.50810387\n",
      "Iteration 300, loss = 0.50780848\n",
      "Iteration 301, loss = 0.50750877\n",
      "Iteration 302, loss = 0.50721095\n",
      "Iteration 303, loss = 0.50692236\n",
      "Iteration 304, loss = 0.50663001\n",
      "Iteration 305, loss = 0.50634563\n",
      "Iteration 306, loss = 0.50606040\n",
      "Iteration 307, loss = 0.50578078\n",
      "Iteration 308, loss = 0.50549844\n",
      "Iteration 309, loss = 0.50522243\n",
      "Iteration 310, loss = 0.50494997\n",
      "Iteration 311, loss = 0.50467302\n",
      "Iteration 312, loss = 0.50440302\n",
      "Iteration 313, loss = 0.50413833\n",
      "Iteration 314, loss = 0.50386879\n",
      "Iteration 315, loss = 0.50360846\n",
      "Iteration 316, loss = 0.50334517\n",
      "Iteration 317, loss = 0.50309038\n",
      "Iteration 318, loss = 0.50283722\n",
      "Iteration 319, loss = 0.50258930\n",
      "Iteration 320, loss = 0.50233093\n",
      "Iteration 321, loss = 0.50208506\n",
      "Iteration 322, loss = 0.50183919\n",
      "Iteration 323, loss = 0.50158972\n",
      "Iteration 324, loss = 0.50134942\n",
      "Iteration 325, loss = 0.50110988\n",
      "Iteration 326, loss = 0.50087317\n",
      "Iteration 327, loss = 0.50063801\n",
      "Iteration 328, loss = 0.50040482\n",
      "Iteration 329, loss = 0.50017335\n",
      "Iteration 330, loss = 0.49994411\n",
      "Iteration 331, loss = 0.49971904\n",
      "Iteration 332, loss = 0.49949407\n",
      "Iteration 333, loss = 0.49927273\n",
      "Iteration 334, loss = 0.49905501\n",
      "Iteration 335, loss = 0.49883924\n",
      "Iteration 336, loss = 0.49861818\n",
      "Iteration 337, loss = 0.49840458\n",
      "Iteration 338, loss = 0.49819162\n",
      "Iteration 339, loss = 0.49799068\n",
      "Iteration 340, loss = 0.49777794\n",
      "Iteration 341, loss = 0.49757589\n",
      "Iteration 342, loss = 0.49737048\n",
      "Iteration 343, loss = 0.49717064\n",
      "Iteration 344, loss = 0.49697330\n",
      "Iteration 345, loss = 0.49678206\n",
      "Iteration 346, loss = 0.49658554\n",
      "Iteration 347, loss = 0.49639133\n",
      "Iteration 348, loss = 0.49620370\n",
      "Iteration 349, loss = 0.49601584\n",
      "Iteration 350, loss = 0.49582892\n",
      "Iteration 351, loss = 0.49565122\n",
      "Iteration 352, loss = 0.49546945\n",
      "Iteration 353, loss = 0.49528916\n",
      "Iteration 354, loss = 0.49510839\n",
      "Iteration 355, loss = 0.49495039\n",
      "Iteration 356, loss = 0.49476210\n",
      "Iteration 357, loss = 0.49458556\n",
      "Iteration 358, loss = 0.49441828\n",
      "Iteration 359, loss = 0.49424669\n",
      "Iteration 360, loss = 0.49408325\n",
      "Iteration 361, loss = 0.49392197\n",
      "Iteration 362, loss = 0.49375895\n",
      "Iteration 363, loss = 0.49359842\n",
      "Iteration 364, loss = 0.49343291\n",
      "Iteration 365, loss = 0.49328052\n",
      "Iteration 366, loss = 0.49312828\n",
      "Iteration 367, loss = 0.49297237\n",
      "Iteration 368, loss = 0.49282172\n",
      "Iteration 369, loss = 0.49267214\n",
      "Iteration 370, loss = 0.49252554\n",
      "Iteration 371, loss = 0.49238905\n",
      "Iteration 372, loss = 0.49223769\n",
      "Iteration 373, loss = 0.49209557\n",
      "Iteration 374, loss = 0.49195681\n",
      "Iteration 375, loss = 0.49181643\n",
      "Iteration 376, loss = 0.49168021\n",
      "Iteration 377, loss = 0.49153998\n",
      "Iteration 378, loss = 0.49140084\n",
      "Iteration 379, loss = 0.49127244\n",
      "Iteration 380, loss = 0.49114459\n",
      "Iteration 381, loss = 0.49101384\n",
      "Iteration 382, loss = 0.49088947\n",
      "Iteration 383, loss = 0.49075999\n",
      "Iteration 384, loss = 0.49063656\n",
      "Iteration 385, loss = 0.49051984\n",
      "Iteration 386, loss = 0.49039309\n",
      "Iteration 387, loss = 0.49028144\n",
      "Iteration 388, loss = 0.49016187\n",
      "Iteration 389, loss = 0.49004270\n",
      "Iteration 390, loss = 0.48992922\n",
      "Iteration 391, loss = 0.48982229\n",
      "Iteration 392, loss = 0.48970760\n",
      "Iteration 393, loss = 0.48959242\n",
      "Iteration 394, loss = 0.48948454\n",
      "Iteration 395, loss = 0.48938083\n",
      "Iteration 396, loss = 0.48927196\n",
      "Iteration 397, loss = 0.48917281\n",
      "Iteration 398, loss = 0.48906892\n",
      "Iteration 399, loss = 0.48897472\n",
      "Iteration 400, loss = 0.48886270\n",
      "Iteration 401, loss = 0.48878120\n",
      "Iteration 402, loss = 0.48867181\n",
      "Iteration 403, loss = 0.48857547\n",
      "Iteration 404, loss = 0.48848076\n",
      "Iteration 405, loss = 0.48838596\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72687167\n",
      "Iteration 2, loss = 0.72300264\n",
      "Iteration 3, loss = 0.71934873\n",
      "Iteration 4, loss = 0.71587137\n",
      "Iteration 5, loss = 0.71256994\n",
      "Iteration 6, loss = 0.70943217\n",
      "Iteration 7, loss = 0.70647326\n",
      "Iteration 8, loss = 0.70368659\n",
      "Iteration 9, loss = 0.70105273\n",
      "Iteration 10, loss = 0.69856720\n",
      "Iteration 11, loss = 0.69625248\n",
      "Iteration 12, loss = 0.69407997\n",
      "Iteration 13, loss = 0.69205624\n",
      "Iteration 14, loss = 0.69015318\n",
      "Iteration 15, loss = 0.68836943\n",
      "Iteration 16, loss = 0.68672055\n",
      "Iteration 17, loss = 0.68516862\n",
      "Iteration 18, loss = 0.68372623\n",
      "Iteration 19, loss = 0.68237709\n",
      "Iteration 20, loss = 0.68113278\n",
      "Iteration 21, loss = 0.67996046\n",
      "Iteration 22, loss = 0.67887295\n",
      "Iteration 23, loss = 0.67785637\n",
      "Iteration 24, loss = 0.67690296\n",
      "Iteration 25, loss = 0.67600729\n",
      "Iteration 26, loss = 0.67515094\n",
      "Iteration 27, loss = 0.67434969\n",
      "Iteration 28, loss = 0.67359255\n",
      "Iteration 29, loss = 0.67286203\n",
      "Iteration 30, loss = 0.67216655\n",
      "Iteration 31, loss = 0.67150140\n",
      "Iteration 32, loss = 0.67085376\n",
      "Iteration 33, loss = 0.67022834\n",
      "Iteration 34, loss = 0.66960852\n",
      "Iteration 35, loss = 0.66900710\n",
      "Iteration 36, loss = 0.66841386\n",
      "Iteration 37, loss = 0.66783024\n",
      "Iteration 38, loss = 0.66725224\n",
      "Iteration 39, loss = 0.66667863\n",
      "Iteration 40, loss = 0.66610749\n",
      "Iteration 41, loss = 0.66553732\n",
      "Iteration 42, loss = 0.66496556\n",
      "Iteration 43, loss = 0.66439845\n",
      "Iteration 44, loss = 0.66382837\n",
      "Iteration 45, loss = 0.66326103\n",
      "Iteration 46, loss = 0.66268707\n",
      "Iteration 47, loss = 0.66211478\n",
      "Iteration 48, loss = 0.66153951\n",
      "Iteration 49, loss = 0.66096221\n",
      "Iteration 50, loss = 0.66038069\n",
      "Iteration 51, loss = 0.65979857\n",
      "Iteration 52, loss = 0.65921218\n",
      "Iteration 53, loss = 0.65862416\n",
      "Iteration 54, loss = 0.65803377\n",
      "Iteration 55, loss = 0.65744036\n",
      "Iteration 56, loss = 0.65684205\n",
      "Iteration 57, loss = 0.65624291\n",
      "Iteration 58, loss = 0.65564034\n",
      "Iteration 59, loss = 0.65503841\n",
      "Iteration 60, loss = 0.65442709\n",
      "Iteration 61, loss = 0.65381896\n",
      "Iteration 62, loss = 0.65320834\n",
      "Iteration 63, loss = 0.65259774\n",
      "Iteration 64, loss = 0.65198154\n",
      "Iteration 65, loss = 0.65136884\n",
      "Iteration 66, loss = 0.65075237\n",
      "Iteration 67, loss = 0.65013179\n",
      "Iteration 68, loss = 0.64951313\n",
      "Iteration 69, loss = 0.64888969\n",
      "Iteration 70, loss = 0.64826519\n",
      "Iteration 71, loss = 0.64764050\n",
      "Iteration 72, loss = 0.64701284\n",
      "Iteration 73, loss = 0.64638361\n",
      "Iteration 74, loss = 0.64575459\n",
      "Iteration 75, loss = 0.64512260\n",
      "Iteration 76, loss = 0.64448430\n",
      "Iteration 77, loss = 0.64384858\n",
      "Iteration 78, loss = 0.64320957\n",
      "Iteration 79, loss = 0.64257219\n",
      "Iteration 80, loss = 0.64193513\n",
      "Iteration 81, loss = 0.64129384\n",
      "Iteration 82, loss = 0.64065419\n",
      "Iteration 83, loss = 0.64001654\n",
      "Iteration 84, loss = 0.63937969\n",
      "Iteration 85, loss = 0.63873556\n",
      "Iteration 86, loss = 0.63809298\n",
      "Iteration 87, loss = 0.63745481\n",
      "Iteration 88, loss = 0.63680790\n",
      "Iteration 89, loss = 0.63616378\n",
      "Iteration 90, loss = 0.63552647\n",
      "Iteration 91, loss = 0.63488252\n",
      "Iteration 92, loss = 0.63424061\n",
      "Iteration 93, loss = 0.63359943\n",
      "Iteration 94, loss = 0.63295673\n",
      "Iteration 95, loss = 0.63231181\n",
      "Iteration 96, loss = 0.63167868\n",
      "Iteration 97, loss = 0.63102919\n",
      "Iteration 98, loss = 0.63039964\n",
      "Iteration 99, loss = 0.62974893\n",
      "Iteration 100, loss = 0.62910997\n",
      "Iteration 101, loss = 0.62846835\n",
      "Iteration 102, loss = 0.62782401\n",
      "Iteration 103, loss = 0.62717539\n",
      "Iteration 104, loss = 0.62653188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 105, loss = 0.62588602\n",
      "Iteration 106, loss = 0.62523966\n",
      "Iteration 107, loss = 0.62459470\n",
      "Iteration 108, loss = 0.62395323\n",
      "Iteration 109, loss = 0.62330529\n",
      "Iteration 110, loss = 0.62265883\n",
      "Iteration 111, loss = 0.62201176\n",
      "Iteration 112, loss = 0.62136539\n",
      "Iteration 113, loss = 0.62071554\n",
      "Iteration 114, loss = 0.62006563\n",
      "Iteration 115, loss = 0.61942186\n",
      "Iteration 116, loss = 0.61877461\n",
      "Iteration 117, loss = 0.61811976\n",
      "Iteration 118, loss = 0.61747470\n",
      "Iteration 119, loss = 0.61682079\n",
      "Iteration 120, loss = 0.61617158\n",
      "Iteration 121, loss = 0.61552653\n",
      "Iteration 122, loss = 0.61487765\n",
      "Iteration 123, loss = 0.61423284\n",
      "Iteration 124, loss = 0.61358635\n",
      "Iteration 125, loss = 0.61294143\n",
      "Iteration 126, loss = 0.61230129\n",
      "Iteration 127, loss = 0.61165444\n",
      "Iteration 128, loss = 0.61101199\n",
      "Iteration 129, loss = 0.61037007\n",
      "Iteration 130, loss = 0.60973655\n",
      "Iteration 131, loss = 0.60908939\n",
      "Iteration 132, loss = 0.60845521\n",
      "Iteration 133, loss = 0.60781898\n",
      "Iteration 134, loss = 0.60718280\n",
      "Iteration 135, loss = 0.60655089\n",
      "Iteration 136, loss = 0.60591325\n",
      "Iteration 137, loss = 0.60527829\n",
      "Iteration 138, loss = 0.60464339\n",
      "Iteration 139, loss = 0.60401587\n",
      "Iteration 140, loss = 0.60338471\n",
      "Iteration 141, loss = 0.60276027\n",
      "Iteration 142, loss = 0.60212712\n",
      "Iteration 143, loss = 0.60150411\n",
      "Iteration 144, loss = 0.60087585\n",
      "Iteration 145, loss = 0.60024869\n",
      "Iteration 146, loss = 0.59962272\n",
      "Iteration 147, loss = 0.59899811\n",
      "Iteration 148, loss = 0.59837256\n",
      "Iteration 149, loss = 0.59774652\n",
      "Iteration 150, loss = 0.59712464\n",
      "Iteration 151, loss = 0.59650444\n",
      "Iteration 152, loss = 0.59588479\n",
      "Iteration 153, loss = 0.59526769\n",
      "Iteration 154, loss = 0.59465174\n",
      "Iteration 155, loss = 0.59403824\n",
      "Iteration 156, loss = 0.59342759\n",
      "Iteration 157, loss = 0.59282160\n",
      "Iteration 158, loss = 0.59222018\n",
      "Iteration 159, loss = 0.59160933\n",
      "Iteration 160, loss = 0.59100519\n",
      "Iteration 161, loss = 0.59040568\n",
      "Iteration 162, loss = 0.58980510\n",
      "Iteration 163, loss = 0.58920711\n",
      "Iteration 164, loss = 0.58861075\n",
      "Iteration 165, loss = 0.58801986\n",
      "Iteration 166, loss = 0.58742506\n",
      "Iteration 167, loss = 0.58683083\n",
      "Iteration 168, loss = 0.58624502\n",
      "Iteration 169, loss = 0.58565695\n",
      "Iteration 170, loss = 0.58506838\n",
      "Iteration 171, loss = 0.58448415\n",
      "Iteration 172, loss = 0.58390609\n",
      "Iteration 173, loss = 0.58332196\n",
      "Iteration 174, loss = 0.58274299\n",
      "Iteration 175, loss = 0.58217039\n",
      "Iteration 176, loss = 0.58159223\n",
      "Iteration 177, loss = 0.58101868\n",
      "Iteration 178, loss = 0.58044560\n",
      "Iteration 179, loss = 0.57987889\n",
      "Iteration 180, loss = 0.57931568\n",
      "Iteration 181, loss = 0.57875287\n",
      "Iteration 182, loss = 0.57819254\n",
      "Iteration 183, loss = 0.57763746\n",
      "Iteration 184, loss = 0.57708294\n",
      "Iteration 185, loss = 0.57653376\n",
      "Iteration 186, loss = 0.57599682\n",
      "Iteration 187, loss = 0.57544466\n",
      "Iteration 188, loss = 0.57489663\n",
      "Iteration 189, loss = 0.57435659\n",
      "Iteration 190, loss = 0.57381424\n",
      "Iteration 191, loss = 0.57327951\n",
      "Iteration 192, loss = 0.57274181\n",
      "Iteration 193, loss = 0.57221388\n",
      "Iteration 194, loss = 0.57168990\n",
      "Iteration 195, loss = 0.57115777\n",
      "Iteration 196, loss = 0.57063621\n",
      "Iteration 197, loss = 0.57011836\n",
      "Iteration 198, loss = 0.56960044\n",
      "Iteration 199, loss = 0.56908271\n",
      "Iteration 200, loss = 0.56856694\n",
      "Iteration 201, loss = 0.56805877\n",
      "Iteration 202, loss = 0.56754985\n",
      "Iteration 203, loss = 0.56704642\n",
      "Iteration 204, loss = 0.56654693\n",
      "Iteration 205, loss = 0.56604276\n",
      "Iteration 206, loss = 0.56554305\n",
      "Iteration 207, loss = 0.56506512\n",
      "Iteration 208, loss = 0.56456168\n",
      "Iteration 209, loss = 0.56406609\n",
      "Iteration 210, loss = 0.56358189\n",
      "Iteration 211, loss = 0.56308891\n",
      "Iteration 212, loss = 0.56260991\n",
      "Iteration 213, loss = 0.56212877\n",
      "Iteration 214, loss = 0.56164499\n",
      "Iteration 215, loss = 0.56117015\n",
      "Iteration 216, loss = 0.56069983\n",
      "Iteration 217, loss = 0.56022463\n",
      "Iteration 218, loss = 0.55976029\n",
      "Iteration 219, loss = 0.55928997\n",
      "Iteration 220, loss = 0.55882244\n",
      "Iteration 221, loss = 0.55836229\n",
      "Iteration 222, loss = 0.55790458\n",
      "Iteration 223, loss = 0.55744621\n",
      "Iteration 224, loss = 0.55699140\n",
      "Iteration 225, loss = 0.55653915\n",
      "Iteration 226, loss = 0.55609367\n",
      "Iteration 227, loss = 0.55565905\n",
      "Iteration 228, loss = 0.55521639\n",
      "Iteration 229, loss = 0.55477813\n",
      "Iteration 230, loss = 0.55433983\n",
      "Iteration 231, loss = 0.55390943\n",
      "Iteration 232, loss = 0.55347987\n",
      "Iteration 233, loss = 0.55305428\n",
      "Iteration 234, loss = 0.55263427\n",
      "Iteration 235, loss = 0.55220703\n",
      "Iteration 236, loss = 0.55179088\n",
      "Iteration 237, loss = 0.55137132\n",
      "Iteration 238, loss = 0.55096118\n",
      "Iteration 239, loss = 0.55054320\n",
      "Iteration 240, loss = 0.55013757\n",
      "Iteration 241, loss = 0.54973556\n",
      "Iteration 242, loss = 0.54933093\n",
      "Iteration 243, loss = 0.54892721\n",
      "Iteration 244, loss = 0.54853017\n",
      "Iteration 245, loss = 0.54813642\n",
      "Iteration 246, loss = 0.54774071\n",
      "Iteration 247, loss = 0.54735294\n",
      "Iteration 248, loss = 0.54696926\n",
      "Iteration 249, loss = 0.54657809\n",
      "Iteration 250, loss = 0.54618975\n",
      "Iteration 251, loss = 0.54581175\n",
      "Iteration 252, loss = 0.54542801\n",
      "Iteration 253, loss = 0.54506141\n",
      "Iteration 254, loss = 0.54468049\n",
      "Iteration 255, loss = 0.54431039\n",
      "Iteration 256, loss = 0.54394409\n",
      "Iteration 257, loss = 0.54357682\n",
      "Iteration 258, loss = 0.54321124\n",
      "Iteration 259, loss = 0.54285286\n",
      "Iteration 260, loss = 0.54249253\n",
      "Iteration 261, loss = 0.54213253\n",
      "Iteration 262, loss = 0.54177944\n",
      "Iteration 263, loss = 0.54142434\n",
      "Iteration 264, loss = 0.54107667\n",
      "Iteration 265, loss = 0.54073400\n",
      "Iteration 266, loss = 0.54038782\n",
      "Iteration 267, loss = 0.54004883\n",
      "Iteration 268, loss = 0.53970981\n",
      "Iteration 269, loss = 0.53937256\n",
      "Iteration 270, loss = 0.53903376\n",
      "Iteration 271, loss = 0.53870257\n",
      "Iteration 272, loss = 0.53838742\n",
      "Iteration 273, loss = 0.53805102\n",
      "Iteration 274, loss = 0.53772478\n",
      "Iteration 275, loss = 0.53740253\n",
      "Iteration 276, loss = 0.53708777\n",
      "Iteration 277, loss = 0.53676496\n",
      "Iteration 278, loss = 0.53645562\n",
      "Iteration 279, loss = 0.53614575\n",
      "Iteration 280, loss = 0.53582906\n",
      "Iteration 281, loss = 0.53552756\n",
      "Iteration 282, loss = 0.53522621\n",
      "Iteration 283, loss = 0.53491716\n",
      "Iteration 284, loss = 0.53462823\n",
      "Iteration 285, loss = 0.53432660\n",
      "Iteration 286, loss = 0.53402777\n",
      "Iteration 287, loss = 0.53374237\n",
      "Iteration 288, loss = 0.53345269\n",
      "Iteration 289, loss = 0.53317671\n",
      "Iteration 290, loss = 0.53289975\n",
      "Iteration 291, loss = 0.53260374\n",
      "Iteration 292, loss = 0.53234486\n",
      "Iteration 293, loss = 0.53205838\n",
      "Iteration 294, loss = 0.53177947\n",
      "Iteration 295, loss = 0.53151002\n",
      "Iteration 296, loss = 0.53124726\n",
      "Iteration 297, loss = 0.53097877\n",
      "Iteration 298, loss = 0.53071464\n",
      "Iteration 299, loss = 0.53045747\n",
      "Iteration 300, loss = 0.53019567\n",
      "Iteration 301, loss = 0.52994055\n",
      "Iteration 302, loss = 0.52967779\n",
      "Iteration 303, loss = 0.52942840\n",
      "Iteration 304, loss = 0.52917504\n",
      "Iteration 305, loss = 0.52892521\n",
      "Iteration 306, loss = 0.52867898\n",
      "Iteration 307, loss = 0.52843286\n",
      "Iteration 308, loss = 0.52818903\n",
      "Iteration 309, loss = 0.52794979\n",
      "Iteration 310, loss = 0.52771044\n",
      "Iteration 311, loss = 0.52747215\n",
      "Iteration 312, loss = 0.52723606\n",
      "Iteration 313, loss = 0.52700560\n",
      "Iteration 314, loss = 0.52677000\n",
      "Iteration 315, loss = 0.52654338\n",
      "Iteration 316, loss = 0.52631495\n",
      "Iteration 317, loss = 0.52609638\n",
      "Iteration 318, loss = 0.52587133\n",
      "Iteration 319, loss = 0.52565608\n",
      "Iteration 320, loss = 0.52543074\n",
      "Iteration 321, loss = 0.52521896\n",
      "Iteration 322, loss = 0.52500062\n",
      "Iteration 323, loss = 0.52478451\n",
      "Iteration 324, loss = 0.52457592\n",
      "Iteration 325, loss = 0.52436622\n",
      "Iteration 326, loss = 0.52415955\n",
      "Iteration 327, loss = 0.52395457\n",
      "Iteration 328, loss = 0.52375129\n",
      "Iteration 329, loss = 0.52354916\n",
      "Iteration 330, loss = 0.52335070\n",
      "Iteration 331, loss = 0.52315300\n",
      "Iteration 332, loss = 0.52296123\n",
      "Iteration 333, loss = 0.52276809\n",
      "Iteration 334, loss = 0.52257737\n",
      "Iteration 335, loss = 0.52238691\n",
      "Iteration 336, loss = 0.52219173\n",
      "Iteration 337, loss = 0.52200730\n",
      "Iteration 338, loss = 0.52182243\n",
      "Iteration 339, loss = 0.52165158\n",
      "Iteration 340, loss = 0.52146110\n",
      "Iteration 341, loss = 0.52128468\n",
      "Iteration 342, loss = 0.52111019\n",
      "Iteration 343, loss = 0.52093463\n",
      "Iteration 344, loss = 0.52076211\n",
      "Iteration 345, loss = 0.52059571\n",
      "Iteration 346, loss = 0.52042391\n",
      "Iteration 347, loss = 0.52025706\n",
      "Iteration 348, loss = 0.52009085\n",
      "Iteration 349, loss = 0.51992888\n",
      "Iteration 350, loss = 0.51976855\n",
      "Iteration 351, loss = 0.51961211\n",
      "Iteration 352, loss = 0.51945584\n",
      "Iteration 353, loss = 0.51929643\n",
      "Iteration 354, loss = 0.51913506\n",
      "Iteration 355, loss = 0.51900073\n",
      "Iteration 356, loss = 0.51883315\n",
      "Iteration 357, loss = 0.51868620\n",
      "Iteration 358, loss = 0.51853891\n",
      "Iteration 359, loss = 0.51838844\n",
      "Iteration 360, loss = 0.51824477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 361, loss = 0.51810301\n",
      "Iteration 362, loss = 0.51796054\n",
      "Iteration 363, loss = 0.51782104\n",
      "Iteration 364, loss = 0.51767932\n",
      "Iteration 365, loss = 0.51754652\n",
      "Iteration 366, loss = 0.51741248\n",
      "Iteration 367, loss = 0.51727647\n",
      "Iteration 368, loss = 0.51714312\n",
      "Iteration 369, loss = 0.51701398\n",
      "Iteration 370, loss = 0.51688656\n",
      "Iteration 371, loss = 0.51676924\n",
      "Iteration 372, loss = 0.51663361\n",
      "Iteration 373, loss = 0.51650634\n",
      "Iteration 374, loss = 0.51638481\n",
      "Iteration 375, loss = 0.51626340\n",
      "Iteration 376, loss = 0.51614998\n",
      "Iteration 377, loss = 0.51602197\n",
      "Iteration 378, loss = 0.51590120\n",
      "Iteration 379, loss = 0.51578832\n",
      "Iteration 380, loss = 0.51567825\n",
      "Iteration 381, loss = 0.51556206\n",
      "Iteration 382, loss = 0.51545283\n",
      "Iteration 383, loss = 0.51533933\n",
      "Iteration 384, loss = 0.51522838\n",
      "Iteration 385, loss = 0.51512644\n",
      "Iteration 386, loss = 0.51501330\n",
      "Iteration 387, loss = 0.51492010\n",
      "Iteration 388, loss = 0.51481413\n",
      "Iteration 389, loss = 0.51471131\n",
      "Iteration 390, loss = 0.51461087\n",
      "Iteration 391, loss = 0.51451566\n",
      "Iteration 392, loss = 0.51441183\n",
      "Iteration 393, loss = 0.51431060\n",
      "Iteration 394, loss = 0.51421622\n",
      "Iteration 395, loss = 0.51412548\n",
      "Iteration 396, loss = 0.51403111\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74339030\n",
      "Iteration 2, loss = 0.73846569\n",
      "Iteration 3, loss = 0.73374518\n",
      "Iteration 4, loss = 0.72920913\n",
      "Iteration 5, loss = 0.72486091\n",
      "Iteration 6, loss = 0.72069122\n",
      "Iteration 7, loss = 0.71670416\n",
      "Iteration 8, loss = 0.71290883\n",
      "Iteration 9, loss = 0.70927662\n",
      "Iteration 10, loss = 0.70580851\n",
      "Iteration 11, loss = 0.70252646\n",
      "Iteration 12, loss = 0.69941300\n",
      "Iteration 13, loss = 0.69646349\n",
      "Iteration 14, loss = 0.69365998\n",
      "Iteration 15, loss = 0.69100265\n",
      "Iteration 16, loss = 0.68850573\n",
      "Iteration 17, loss = 0.68613031\n",
      "Iteration 18, loss = 0.68390248\n",
      "Iteration 19, loss = 0.68180063\n",
      "Iteration 20, loss = 0.67983209\n",
      "Iteration 21, loss = 0.67797144\n",
      "Iteration 22, loss = 0.67623019\n",
      "Iteration 23, loss = 0.67459998\n",
      "Iteration 24, loss = 0.67306668\n",
      "Iteration 25, loss = 0.67162741\n",
      "Iteration 26, loss = 0.67025282\n",
      "Iteration 27, loss = 0.66897218\n",
      "Iteration 28, loss = 0.66777277\n",
      "Iteration 29, loss = 0.66662816\n",
      "Iteration 30, loss = 0.66555543\n",
      "Iteration 31, loss = 0.66454116\n",
      "Iteration 32, loss = 0.66357108\n",
      "Iteration 33, loss = 0.66264966\n",
      "Iteration 34, loss = 0.66175399\n",
      "Iteration 35, loss = 0.66090328\n",
      "Iteration 36, loss = 0.66008110\n",
      "Iteration 37, loss = 0.65928199\n",
      "Iteration 38, loss = 0.65850886\n",
      "Iteration 39, loss = 0.65775073\n",
      "Iteration 40, loss = 0.65701269\n",
      "Iteration 41, loss = 0.65628184\n",
      "Iteration 42, loss = 0.65556335\n",
      "Iteration 43, loss = 0.65485385\n",
      "Iteration 44, loss = 0.65414842\n",
      "Iteration 45, loss = 0.65345037\n",
      "Iteration 46, loss = 0.65274901\n",
      "Iteration 47, loss = 0.65205525\n",
      "Iteration 48, loss = 0.65135935\n",
      "Iteration 49, loss = 0.65066160\n",
      "Iteration 50, loss = 0.64996325\n",
      "Iteration 51, loss = 0.64926256\n",
      "Iteration 52, loss = 0.64855727\n",
      "Iteration 53, loss = 0.64785018\n",
      "Iteration 54, loss = 0.64714127\n",
      "Iteration 55, loss = 0.64642722\n",
      "Iteration 56, loss = 0.64570727\n",
      "Iteration 57, loss = 0.64498571\n",
      "Iteration 58, loss = 0.64426060\n",
      "Iteration 59, loss = 0.64353281\n",
      "Iteration 60, loss = 0.64279682\n",
      "Iteration 61, loss = 0.64206147\n",
      "Iteration 62, loss = 0.64131869\n",
      "Iteration 63, loss = 0.64057565\n",
      "Iteration 64, loss = 0.63982716\n",
      "Iteration 65, loss = 0.63908092\n",
      "Iteration 66, loss = 0.63832864\n",
      "Iteration 67, loss = 0.63757122\n",
      "Iteration 68, loss = 0.63681243\n",
      "Iteration 69, loss = 0.63605070\n",
      "Iteration 70, loss = 0.63528478\n",
      "Iteration 71, loss = 0.63451717\n",
      "Iteration 72, loss = 0.63374723\n",
      "Iteration 73, loss = 0.63297303\n",
      "Iteration 74, loss = 0.63219685\n",
      "Iteration 75, loss = 0.63141583\n",
      "Iteration 76, loss = 0.63062817\n",
      "Iteration 77, loss = 0.62984445\n",
      "Iteration 78, loss = 0.62905164\n",
      "Iteration 79, loss = 0.62825978\n",
      "Iteration 80, loss = 0.62746714\n",
      "Iteration 81, loss = 0.62667121\n",
      "Iteration 82, loss = 0.62587508\n",
      "Iteration 83, loss = 0.62508220\n",
      "Iteration 84, loss = 0.62428992\n",
      "Iteration 85, loss = 0.62348651\n",
      "Iteration 86, loss = 0.62268750\n",
      "Iteration 87, loss = 0.62188910\n",
      "Iteration 88, loss = 0.62108623\n",
      "Iteration 89, loss = 0.62028076\n",
      "Iteration 90, loss = 0.61948084\n",
      "Iteration 91, loss = 0.61867771\n",
      "Iteration 92, loss = 0.61787472\n",
      "Iteration 93, loss = 0.61707255\n",
      "Iteration 94, loss = 0.61627098\n",
      "Iteration 95, loss = 0.61546272\n",
      "Iteration 96, loss = 0.61466985\n",
      "Iteration 97, loss = 0.61386124\n",
      "Iteration 98, loss = 0.61306618\n",
      "Iteration 99, loss = 0.61225669\n",
      "Iteration 100, loss = 0.61145822\n",
      "Iteration 101, loss = 0.61065443\n",
      "Iteration 102, loss = 0.60984749\n",
      "Iteration 103, loss = 0.60903888\n",
      "Iteration 104, loss = 0.60823416\n",
      "Iteration 105, loss = 0.60742678\n",
      "Iteration 106, loss = 0.60661861\n",
      "Iteration 107, loss = 0.60581328\n",
      "Iteration 108, loss = 0.60500901\n",
      "Iteration 109, loss = 0.60420139\n",
      "Iteration 110, loss = 0.60339413\n",
      "Iteration 111, loss = 0.60258627\n",
      "Iteration 112, loss = 0.60177766\n",
      "Iteration 113, loss = 0.60096763\n",
      "Iteration 114, loss = 0.60015875\n",
      "Iteration 115, loss = 0.59935461\n",
      "Iteration 116, loss = 0.59854932\n",
      "Iteration 117, loss = 0.59773563\n",
      "Iteration 118, loss = 0.59693456\n",
      "Iteration 119, loss = 0.59611943\n",
      "Iteration 120, loss = 0.59531262\n",
      "Iteration 121, loss = 0.59450561\n",
      "Iteration 122, loss = 0.59370026\n",
      "Iteration 123, loss = 0.59289292\n",
      "Iteration 124, loss = 0.59208759\n",
      "Iteration 125, loss = 0.59128523\n",
      "Iteration 126, loss = 0.59048613\n",
      "Iteration 127, loss = 0.58968177\n",
      "Iteration 128, loss = 0.58888382\n",
      "Iteration 129, loss = 0.58808327\n",
      "Iteration 130, loss = 0.58729477\n",
      "Iteration 131, loss = 0.58649558\n",
      "Iteration 132, loss = 0.58570568\n",
      "Iteration 133, loss = 0.58491226\n",
      "Iteration 134, loss = 0.58412275\n",
      "Iteration 135, loss = 0.58333558\n",
      "Iteration 136, loss = 0.58254549\n",
      "Iteration 137, loss = 0.58175441\n",
      "Iteration 138, loss = 0.58096772\n",
      "Iteration 139, loss = 0.58018484\n",
      "Iteration 140, loss = 0.57940563\n",
      "Iteration 141, loss = 0.57862806\n",
      "Iteration 142, loss = 0.57784715\n",
      "Iteration 143, loss = 0.57707518\n",
      "Iteration 144, loss = 0.57629702\n",
      "Iteration 145, loss = 0.57552381\n",
      "Iteration 146, loss = 0.57475015\n",
      "Iteration 147, loss = 0.57398085\n",
      "Iteration 148, loss = 0.57320921\n",
      "Iteration 149, loss = 0.57243953\n",
      "Iteration 150, loss = 0.57167113\n",
      "Iteration 151, loss = 0.57090963\n",
      "Iteration 152, loss = 0.57014648\n",
      "Iteration 153, loss = 0.56938948\n",
      "Iteration 154, loss = 0.56863727\n",
      "Iteration 155, loss = 0.56788631\n",
      "Iteration 156, loss = 0.56713923\n",
      "Iteration 157, loss = 0.56639668\n",
      "Iteration 158, loss = 0.56566091\n",
      "Iteration 159, loss = 0.56491820\n",
      "Iteration 160, loss = 0.56418193\n",
      "Iteration 161, loss = 0.56344921\n",
      "Iteration 162, loss = 0.56271861\n",
      "Iteration 163, loss = 0.56199080\n",
      "Iteration 164, loss = 0.56126687\n",
      "Iteration 165, loss = 0.56054636\n",
      "Iteration 166, loss = 0.55982681\n",
      "Iteration 167, loss = 0.55910638\n",
      "Iteration 168, loss = 0.55839181\n",
      "Iteration 169, loss = 0.55767703\n",
      "Iteration 170, loss = 0.55696447\n",
      "Iteration 171, loss = 0.55625659\n",
      "Iteration 172, loss = 0.55555365\n",
      "Iteration 173, loss = 0.55485015\n",
      "Iteration 174, loss = 0.55415125\n",
      "Iteration 175, loss = 0.55346158\n",
      "Iteration 176, loss = 0.55276685\n",
      "Iteration 177, loss = 0.55208034\n",
      "Iteration 178, loss = 0.55139420\n",
      "Iteration 179, loss = 0.55071729\n",
      "Iteration 180, loss = 0.55004188\n",
      "Iteration 181, loss = 0.54936827\n",
      "Iteration 182, loss = 0.54869723\n",
      "Iteration 183, loss = 0.54803406\n",
      "Iteration 184, loss = 0.54737098\n",
      "Iteration 185, loss = 0.54671908\n",
      "Iteration 186, loss = 0.54607469\n",
      "Iteration 187, loss = 0.54542321\n",
      "Iteration 188, loss = 0.54476592\n",
      "Iteration 189, loss = 0.54412518\n",
      "Iteration 190, loss = 0.54347989\n",
      "Iteration 191, loss = 0.54284464\n",
      "Iteration 192, loss = 0.54220728\n",
      "Iteration 193, loss = 0.54158301\n",
      "Iteration 194, loss = 0.54095853\n",
      "Iteration 195, loss = 0.54032938\n",
      "Iteration 196, loss = 0.53971168\n",
      "Iteration 197, loss = 0.53909955\n",
      "Iteration 198, loss = 0.53848900\n",
      "Iteration 199, loss = 0.53787841\n",
      "Iteration 200, loss = 0.53727436\n",
      "Iteration 201, loss = 0.53667668\n",
      "Iteration 202, loss = 0.53608016\n",
      "Iteration 203, loss = 0.53548998\n",
      "Iteration 204, loss = 0.53490376\n",
      "Iteration 205, loss = 0.53431546\n",
      "Iteration 206, loss = 0.53373331\n",
      "Iteration 207, loss = 0.53317093\n",
      "Iteration 208, loss = 0.53259341\n",
      "Iteration 209, loss = 0.53201797\n",
      "Iteration 210, loss = 0.53145638\n",
      "Iteration 211, loss = 0.53088924\n",
      "Iteration 212, loss = 0.53033334\n",
      "Iteration 213, loss = 0.52977752\n",
      "Iteration 214, loss = 0.52922301\n",
      "Iteration 215, loss = 0.52867481\n",
      "Iteration 216, loss = 0.52813558\n",
      "Iteration 217, loss = 0.52759529\n",
      "Iteration 218, loss = 0.52706498\n",
      "Iteration 219, loss = 0.52652777\n",
      "Iteration 220, loss = 0.52600221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 221, loss = 0.52547520\n",
      "Iteration 222, loss = 0.52495516\n",
      "Iteration 223, loss = 0.52443487\n",
      "Iteration 224, loss = 0.52392120\n",
      "Iteration 225, loss = 0.52341056\n",
      "Iteration 226, loss = 0.52290760\n",
      "Iteration 227, loss = 0.52241429\n",
      "Iteration 228, loss = 0.52191728\n",
      "Iteration 229, loss = 0.52142572\n",
      "Iteration 230, loss = 0.52093713\n",
      "Iteration 231, loss = 0.52045056\n",
      "Iteration 232, loss = 0.51996925\n",
      "Iteration 233, loss = 0.51949530\n",
      "Iteration 234, loss = 0.51902562\n",
      "Iteration 235, loss = 0.51855177\n",
      "Iteration 236, loss = 0.51809117\n",
      "Iteration 237, loss = 0.51762510\n",
      "Iteration 238, loss = 0.51717115\n",
      "Iteration 239, loss = 0.51670979\n",
      "Iteration 240, loss = 0.51626625\n",
      "Iteration 241, loss = 0.51582010\n",
      "Iteration 242, loss = 0.51537696\n",
      "Iteration 243, loss = 0.51493876\n",
      "Iteration 244, loss = 0.51450382\n",
      "Iteration 245, loss = 0.51407452\n",
      "Iteration 246, loss = 0.51364532\n",
      "Iteration 247, loss = 0.51322294\n",
      "Iteration 248, loss = 0.51280348\n",
      "Iteration 249, loss = 0.51238475\n",
      "Iteration 250, loss = 0.51196338\n",
      "Iteration 251, loss = 0.51155360\n",
      "Iteration 252, loss = 0.51114341\n",
      "Iteration 253, loss = 0.51074544\n",
      "Iteration 254, loss = 0.51034021\n",
      "Iteration 255, loss = 0.50994508\n",
      "Iteration 256, loss = 0.50955062\n",
      "Iteration 257, loss = 0.50915922\n",
      "Iteration 258, loss = 0.50876795\n",
      "Iteration 259, loss = 0.50838441\n",
      "Iteration 260, loss = 0.50800079\n",
      "Iteration 261, loss = 0.50762098\n",
      "Iteration 262, loss = 0.50724575\n",
      "Iteration 263, loss = 0.50687188\n",
      "Iteration 264, loss = 0.50650423\n",
      "Iteration 265, loss = 0.50614119\n",
      "Iteration 266, loss = 0.50577620\n",
      "Iteration 267, loss = 0.50542295\n",
      "Iteration 268, loss = 0.50506734\n",
      "Iteration 269, loss = 0.50471585\n",
      "Iteration 270, loss = 0.50436413\n",
      "Iteration 271, loss = 0.50401771\n",
      "Iteration 272, loss = 0.50368427\n",
      "Iteration 273, loss = 0.50334092\n",
      "Iteration 274, loss = 0.50300143\n",
      "Iteration 275, loss = 0.50267088\n",
      "Iteration 276, loss = 0.50234182\n",
      "Iteration 277, loss = 0.50201289\n",
      "Iteration 278, loss = 0.50169356\n",
      "Iteration 279, loss = 0.50137622\n",
      "Iteration 280, loss = 0.50105427\n",
      "Iteration 281, loss = 0.50074189\n",
      "Iteration 282, loss = 0.50043260\n",
      "Iteration 283, loss = 0.50011693\n",
      "Iteration 284, loss = 0.49982468\n",
      "Iteration 285, loss = 0.49951732\n",
      "Iteration 286, loss = 0.49921465\n",
      "Iteration 287, loss = 0.49892334\n",
      "Iteration 288, loss = 0.49862824\n",
      "Iteration 289, loss = 0.49835052\n",
      "Iteration 290, loss = 0.49807625\n",
      "Iteration 291, loss = 0.49777326\n",
      "Iteration 292, loss = 0.49750797\n",
      "Iteration 293, loss = 0.49722586\n",
      "Iteration 294, loss = 0.49694626\n",
      "Iteration 295, loss = 0.49667617\n",
      "Iteration 296, loss = 0.49641693\n",
      "Iteration 297, loss = 0.49614782\n",
      "Iteration 298, loss = 0.49588654\n",
      "Iteration 299, loss = 0.49563089\n",
      "Iteration 300, loss = 0.49537191\n",
      "Iteration 301, loss = 0.49511787\n",
      "Iteration 302, loss = 0.49486259\n",
      "Iteration 303, loss = 0.49461384\n",
      "Iteration 304, loss = 0.49436880\n",
      "Iteration 305, loss = 0.49412137\n",
      "Iteration 306, loss = 0.49388274\n",
      "Iteration 307, loss = 0.49363913\n",
      "Iteration 308, loss = 0.49340329\n",
      "Iteration 309, loss = 0.49317289\n",
      "Iteration 310, loss = 0.49293488\n",
      "Iteration 311, loss = 0.49270829\n",
      "Iteration 312, loss = 0.49247762\n",
      "Iteration 313, loss = 0.49225458\n",
      "Iteration 314, loss = 0.49202588\n",
      "Iteration 315, loss = 0.49180894\n",
      "Iteration 316, loss = 0.49158906\n",
      "Iteration 317, loss = 0.49138170\n",
      "Iteration 318, loss = 0.49116324\n",
      "Iteration 319, loss = 0.49095537\n",
      "Iteration 320, loss = 0.49074285\n",
      "Iteration 321, loss = 0.49053668\n",
      "Iteration 322, loss = 0.49033111\n",
      "Iteration 323, loss = 0.49012612\n",
      "Iteration 324, loss = 0.48992682\n",
      "Iteration 325, loss = 0.48972758\n",
      "Iteration 326, loss = 0.48953277\n",
      "Iteration 327, loss = 0.48933950\n",
      "Iteration 328, loss = 0.48914591\n",
      "Iteration 329, loss = 0.48896141\n",
      "Iteration 330, loss = 0.48876997\n",
      "Iteration 331, loss = 0.48858601\n",
      "Iteration 332, loss = 0.48840300\n",
      "Iteration 333, loss = 0.48822258\n",
      "Iteration 334, loss = 0.48804691\n",
      "Iteration 335, loss = 0.48787300\n",
      "Iteration 336, loss = 0.48768691\n",
      "Iteration 337, loss = 0.48751893\n",
      "Iteration 338, loss = 0.48734632\n",
      "Iteration 339, loss = 0.48718960\n",
      "Iteration 340, loss = 0.48701334\n",
      "Iteration 341, loss = 0.48684863\n",
      "Iteration 342, loss = 0.48668800\n",
      "Iteration 343, loss = 0.48652704\n",
      "Iteration 344, loss = 0.48637099\n",
      "Iteration 345, loss = 0.48621697\n",
      "Iteration 346, loss = 0.48605892\n",
      "Iteration 347, loss = 0.48590728\n",
      "Iteration 348, loss = 0.48575804\n",
      "Iteration 349, loss = 0.48560801\n",
      "Iteration 350, loss = 0.48546261\n",
      "Iteration 351, loss = 0.48532222\n",
      "Iteration 352, loss = 0.48518293\n",
      "Iteration 353, loss = 0.48504296\n",
      "Iteration 354, loss = 0.48489603\n",
      "Iteration 355, loss = 0.48477558\n",
      "Iteration 356, loss = 0.48462437\n",
      "Iteration 357, loss = 0.48449306\n",
      "Iteration 358, loss = 0.48435929\n",
      "Iteration 359, loss = 0.48422731\n",
      "Iteration 360, loss = 0.48409800\n",
      "Iteration 361, loss = 0.48397463\n",
      "Iteration 362, loss = 0.48384403\n",
      "Iteration 363, loss = 0.48372155\n",
      "Iteration 364, loss = 0.48359575\n",
      "Iteration 365, loss = 0.48347729\n",
      "Iteration 366, loss = 0.48335833\n",
      "Iteration 367, loss = 0.48324415\n",
      "Iteration 368, loss = 0.48312413\n",
      "Iteration 369, loss = 0.48301514\n",
      "Iteration 370, loss = 0.48290139\n",
      "Iteration 371, loss = 0.48280473\n",
      "Iteration 372, loss = 0.48267830\n",
      "Iteration 373, loss = 0.48256791\n",
      "Iteration 374, loss = 0.48246220\n",
      "Iteration 375, loss = 0.48235648\n",
      "Iteration 376, loss = 0.48226187\n",
      "Iteration 377, loss = 0.48214954\n",
      "Iteration 378, loss = 0.48204314\n",
      "Iteration 379, loss = 0.48194354\n",
      "Iteration 380, loss = 0.48185227\n",
      "Iteration 381, loss = 0.48175139\n",
      "Iteration 382, loss = 0.48165663\n",
      "Iteration 383, loss = 0.48155882\n",
      "Iteration 384, loss = 0.48146530\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76068217\n",
      "Iteration 2, loss = 0.75488248\n",
      "Iteration 3, loss = 0.74929223\n",
      "Iteration 4, loss = 0.74389689\n",
      "Iteration 5, loss = 0.73869301\n",
      "Iteration 6, loss = 0.73367219\n",
      "Iteration 7, loss = 0.72884155\n",
      "Iteration 8, loss = 0.72420452\n",
      "Iteration 9, loss = 0.71974346\n",
      "Iteration 10, loss = 0.71545407\n",
      "Iteration 11, loss = 0.71135412\n",
      "Iteration 12, loss = 0.70743254\n",
      "Iteration 13, loss = 0.70369060\n",
      "Iteration 14, loss = 0.70010483\n",
      "Iteration 15, loss = 0.69667818\n",
      "Iteration 16, loss = 0.69342164\n",
      "Iteration 17, loss = 0.69030517\n",
      "Iteration 18, loss = 0.68736539\n",
      "Iteration 19, loss = 0.68457148\n",
      "Iteration 20, loss = 0.68193979\n",
      "Iteration 21, loss = 0.67943264\n",
      "Iteration 22, loss = 0.67707053\n",
      "Iteration 23, loss = 0.67484526\n",
      "Iteration 24, loss = 0.67274679\n",
      "Iteration 25, loss = 0.67076645\n",
      "Iteration 26, loss = 0.66888820\n",
      "Iteration 27, loss = 0.66712910\n",
      "Iteration 28, loss = 0.66548185\n",
      "Iteration 29, loss = 0.66392256\n",
      "Iteration 30, loss = 0.66246360\n",
      "Iteration 31, loss = 0.66109569\n",
      "Iteration 32, loss = 0.65980397\n",
      "Iteration 33, loss = 0.65858655\n",
      "Iteration 34, loss = 0.65742398\n",
      "Iteration 35, loss = 0.65633533\n",
      "Iteration 36, loss = 0.65530056\n",
      "Iteration 37, loss = 0.65431416\n",
      "Iteration 38, loss = 0.65338263\n",
      "Iteration 39, loss = 0.65248501\n",
      "Iteration 40, loss = 0.65163179\n",
      "Iteration 41, loss = 0.65080224\n",
      "Iteration 42, loss = 0.65000391\n",
      "Iteration 43, loss = 0.64923147\n",
      "Iteration 44, loss = 0.64847241\n",
      "Iteration 45, loss = 0.64773716\n",
      "Iteration 46, loss = 0.64700761\n",
      "Iteration 47, loss = 0.64629485\n",
      "Iteration 48, loss = 0.64559132\n",
      "Iteration 49, loss = 0.64489024\n",
      "Iteration 50, loss = 0.64419704\n",
      "Iteration 51, loss = 0.64350594\n",
      "Iteration 52, loss = 0.64281399\n",
      "Iteration 53, loss = 0.64212321\n",
      "Iteration 54, loss = 0.64143450\n",
      "Iteration 55, loss = 0.64074523\n",
      "Iteration 56, loss = 0.64005020\n",
      "Iteration 57, loss = 0.63935596\n",
      "Iteration 58, loss = 0.63865738\n",
      "Iteration 59, loss = 0.63795645\n",
      "Iteration 60, loss = 0.63725018\n",
      "Iteration 61, loss = 0.63654233\n",
      "Iteration 62, loss = 0.63582769\n",
      "Iteration 63, loss = 0.63511218\n",
      "Iteration 64, loss = 0.63439171\n",
      "Iteration 65, loss = 0.63366824\n",
      "Iteration 66, loss = 0.63293883\n",
      "Iteration 67, loss = 0.63220692\n",
      "Iteration 68, loss = 0.63146945\n",
      "Iteration 69, loss = 0.63072808\n",
      "Iteration 70, loss = 0.62998250\n",
      "Iteration 71, loss = 0.62923352\n",
      "Iteration 72, loss = 0.62847826\n",
      "Iteration 73, loss = 0.62772156\n",
      "Iteration 74, loss = 0.62695955\n",
      "Iteration 75, loss = 0.62619225\n",
      "Iteration 76, loss = 0.62542324\n",
      "Iteration 77, loss = 0.62465129\n",
      "Iteration 78, loss = 0.62387267\n",
      "Iteration 79, loss = 0.62309432\n",
      "Iteration 80, loss = 0.62231162\n",
      "Iteration 81, loss = 0.62153024\n",
      "Iteration 82, loss = 0.62074341\n",
      "Iteration 83, loss = 0.61995837\n",
      "Iteration 84, loss = 0.61917752\n",
      "Iteration 85, loss = 0.61838336\n",
      "Iteration 86, loss = 0.61759471\n",
      "Iteration 87, loss = 0.61680718\n",
      "Iteration 88, loss = 0.61601209\n",
      "Iteration 89, loss = 0.61521501\n",
      "Iteration 90, loss = 0.61442243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91, loss = 0.61362488\n",
      "Iteration 92, loss = 0.61282856\n",
      "Iteration 93, loss = 0.61203136\n",
      "Iteration 94, loss = 0.61123055\n",
      "Iteration 95, loss = 0.61042334\n",
      "Iteration 96, loss = 0.60963248\n",
      "Iteration 97, loss = 0.60882556\n",
      "Iteration 98, loss = 0.60803198\n",
      "Iteration 99, loss = 0.60722234\n",
      "Iteration 100, loss = 0.60642460\n",
      "Iteration 101, loss = 0.60562004\n",
      "Iteration 102, loss = 0.60481666\n",
      "Iteration 103, loss = 0.60400855\n",
      "Iteration 104, loss = 0.60320311\n",
      "Iteration 105, loss = 0.60239900\n",
      "Iteration 106, loss = 0.60159315\n",
      "Iteration 107, loss = 0.60078840\n",
      "Iteration 108, loss = 0.59998515\n",
      "Iteration 109, loss = 0.59917826\n",
      "Iteration 110, loss = 0.59837248\n",
      "Iteration 111, loss = 0.59756193\n",
      "Iteration 112, loss = 0.59675520\n",
      "Iteration 113, loss = 0.59595225\n",
      "Iteration 114, loss = 0.59514273\n",
      "Iteration 115, loss = 0.59434057\n",
      "Iteration 116, loss = 0.59353538\n",
      "Iteration 117, loss = 0.59273238\n",
      "Iteration 118, loss = 0.59193414\n",
      "Iteration 119, loss = 0.59112452\n",
      "Iteration 120, loss = 0.59031748\n",
      "Iteration 121, loss = 0.58951635\n",
      "Iteration 122, loss = 0.58871859\n",
      "Iteration 123, loss = 0.58791747\n",
      "Iteration 124, loss = 0.58711470\n",
      "Iteration 125, loss = 0.58631418\n",
      "Iteration 126, loss = 0.58551936\n",
      "Iteration 127, loss = 0.58471640\n",
      "Iteration 128, loss = 0.58392245\n",
      "Iteration 129, loss = 0.58312073\n",
      "Iteration 130, loss = 0.58233032\n",
      "Iteration 131, loss = 0.58153307\n",
      "Iteration 132, loss = 0.58074052\n",
      "Iteration 133, loss = 0.57994299\n",
      "Iteration 134, loss = 0.57915337\n",
      "Iteration 135, loss = 0.57836234\n",
      "Iteration 136, loss = 0.57756999\n",
      "Iteration 137, loss = 0.57677604\n",
      "Iteration 138, loss = 0.57598499\n",
      "Iteration 139, loss = 0.57519954\n",
      "Iteration 140, loss = 0.57441723\n",
      "Iteration 141, loss = 0.57363520\n",
      "Iteration 142, loss = 0.57285131\n",
      "Iteration 143, loss = 0.57207485\n",
      "Iteration 144, loss = 0.57129423\n",
      "Iteration 145, loss = 0.57052032\n",
      "Iteration 146, loss = 0.56974228\n",
      "Iteration 147, loss = 0.56897435\n",
      "Iteration 148, loss = 0.56819985\n",
      "Iteration 149, loss = 0.56743076\n",
      "Iteration 150, loss = 0.56666649\n",
      "Iteration 151, loss = 0.56590431\n",
      "Iteration 152, loss = 0.56514041\n",
      "Iteration 153, loss = 0.56438294\n",
      "Iteration 154, loss = 0.56362716\n",
      "Iteration 155, loss = 0.56287583\n",
      "Iteration 156, loss = 0.56212750\n",
      "Iteration 157, loss = 0.56138063\n",
      "Iteration 158, loss = 0.56063720\n",
      "Iteration 159, loss = 0.55989278\n",
      "Iteration 160, loss = 0.55914949\n",
      "Iteration 161, loss = 0.55841103\n",
      "Iteration 162, loss = 0.55767742\n",
      "Iteration 163, loss = 0.55693942\n",
      "Iteration 164, loss = 0.55620532\n",
      "Iteration 165, loss = 0.55547393\n",
      "Iteration 166, loss = 0.55474693\n",
      "Iteration 167, loss = 0.55402038\n",
      "Iteration 168, loss = 0.55329121\n",
      "Iteration 169, loss = 0.55256885\n",
      "Iteration 170, loss = 0.55184809\n",
      "Iteration 171, loss = 0.55113110\n",
      "Iteration 172, loss = 0.55042343\n",
      "Iteration 173, loss = 0.54970792\n",
      "Iteration 174, loss = 0.54900020\n",
      "Iteration 175, loss = 0.54829860\n",
      "Iteration 176, loss = 0.54759301\n",
      "Iteration 177, loss = 0.54689766\n",
      "Iteration 178, loss = 0.54620111\n",
      "Iteration 179, loss = 0.54551015\n",
      "Iteration 180, loss = 0.54482854\n",
      "Iteration 181, loss = 0.54413927\n",
      "Iteration 182, loss = 0.54345799\n",
      "Iteration 183, loss = 0.54278601\n",
      "Iteration 184, loss = 0.54211029\n",
      "Iteration 185, loss = 0.54144217\n",
      "Iteration 186, loss = 0.54078770\n",
      "Iteration 187, loss = 0.54012294\n",
      "Iteration 188, loss = 0.53946384\n",
      "Iteration 189, loss = 0.53880947\n",
      "Iteration 190, loss = 0.53815414\n",
      "Iteration 191, loss = 0.53750690\n",
      "Iteration 192, loss = 0.53685591\n",
      "Iteration 193, loss = 0.53621776\n",
      "Iteration 194, loss = 0.53558551\n",
      "Iteration 195, loss = 0.53494794\n",
      "Iteration 196, loss = 0.53432016\n",
      "Iteration 197, loss = 0.53369658\n",
      "Iteration 198, loss = 0.53307587\n",
      "Iteration 199, loss = 0.53245875\n",
      "Iteration 200, loss = 0.53183967\n",
      "Iteration 201, loss = 0.53123041\n",
      "Iteration 202, loss = 0.53062196\n",
      "Iteration 203, loss = 0.53001788\n",
      "Iteration 204, loss = 0.52941819\n",
      "Iteration 205, loss = 0.52881873\n",
      "Iteration 206, loss = 0.52822252\n",
      "Iteration 207, loss = 0.52763828\n",
      "Iteration 208, loss = 0.52705170\n",
      "Iteration 209, loss = 0.52646452\n",
      "Iteration 210, loss = 0.52588956\n",
      "Iteration 211, loss = 0.52531070\n",
      "Iteration 212, loss = 0.52474473\n",
      "Iteration 213, loss = 0.52416974\n",
      "Iteration 214, loss = 0.52360331\n",
      "Iteration 215, loss = 0.52303978\n",
      "Iteration 216, loss = 0.52248743\n",
      "Iteration 217, loss = 0.52193192\n",
      "Iteration 218, loss = 0.52138406\n",
      "Iteration 219, loss = 0.52083288\n",
      "Iteration 220, loss = 0.52028797\n",
      "Iteration 221, loss = 0.51974727\n",
      "Iteration 222, loss = 0.51920907\n",
      "Iteration 223, loss = 0.51866846\n",
      "Iteration 224, loss = 0.51813432\n",
      "Iteration 225, loss = 0.51760831\n",
      "Iteration 226, loss = 0.51708545\n",
      "Iteration 227, loss = 0.51657065\n",
      "Iteration 228, loss = 0.51605466\n",
      "Iteration 229, loss = 0.51554242\n",
      "Iteration 230, loss = 0.51503989\n",
      "Iteration 231, loss = 0.51453411\n",
      "Iteration 232, loss = 0.51403018\n",
      "Iteration 233, loss = 0.51353870\n",
      "Iteration 234, loss = 0.51304772\n",
      "Iteration 235, loss = 0.51255424\n",
      "Iteration 236, loss = 0.51207569\n",
      "Iteration 237, loss = 0.51159457\n",
      "Iteration 238, loss = 0.51111815\n",
      "Iteration 239, loss = 0.51063978\n",
      "Iteration 240, loss = 0.51017920\n",
      "Iteration 241, loss = 0.50971448\n",
      "Iteration 242, loss = 0.50925312\n",
      "Iteration 243, loss = 0.50879691\n",
      "Iteration 244, loss = 0.50834447\n",
      "Iteration 245, loss = 0.50789819\n",
      "Iteration 246, loss = 0.50744914\n",
      "Iteration 247, loss = 0.50700528\n",
      "Iteration 248, loss = 0.50656412\n",
      "Iteration 249, loss = 0.50613486\n",
      "Iteration 250, loss = 0.50569711\n",
      "Iteration 251, loss = 0.50526558\n",
      "Iteration 252, loss = 0.50483830\n",
      "Iteration 253, loss = 0.50442196\n",
      "Iteration 254, loss = 0.50399958\n",
      "Iteration 255, loss = 0.50359050\n",
      "Iteration 256, loss = 0.50317614\n",
      "Iteration 257, loss = 0.50276309\n",
      "Iteration 258, loss = 0.50235500\n",
      "Iteration 259, loss = 0.50195382\n",
      "Iteration 260, loss = 0.50155495\n",
      "Iteration 261, loss = 0.50115449\n",
      "Iteration 262, loss = 0.50076208\n",
      "Iteration 263, loss = 0.50037442\n",
      "Iteration 264, loss = 0.49998800\n",
      "Iteration 265, loss = 0.49960765\n",
      "Iteration 266, loss = 0.49922554\n",
      "Iteration 267, loss = 0.49885334\n",
      "Iteration 268, loss = 0.49847995\n",
      "Iteration 269, loss = 0.49811101\n",
      "Iteration 270, loss = 0.49774296\n",
      "Iteration 271, loss = 0.49737880\n",
      "Iteration 272, loss = 0.49703033\n",
      "Iteration 273, loss = 0.49666794\n",
      "Iteration 274, loss = 0.49631008\n",
      "Iteration 275, loss = 0.49596149\n",
      "Iteration 276, loss = 0.49561860\n",
      "Iteration 277, loss = 0.49527067\n",
      "Iteration 278, loss = 0.49493072\n",
      "Iteration 279, loss = 0.49459588\n",
      "Iteration 280, loss = 0.49425740\n",
      "Iteration 281, loss = 0.49393131\n",
      "Iteration 282, loss = 0.49360456\n",
      "Iteration 283, loss = 0.49327140\n",
      "Iteration 284, loss = 0.49295816\n",
      "Iteration 285, loss = 0.49263817\n",
      "Iteration 286, loss = 0.49231193\n",
      "Iteration 287, loss = 0.49200644\n",
      "Iteration 288, loss = 0.49169617\n",
      "Iteration 289, loss = 0.49139359\n",
      "Iteration 290, loss = 0.49110312\n",
      "Iteration 291, loss = 0.49078778\n",
      "Iteration 292, loss = 0.49050479\n",
      "Iteration 293, loss = 0.49020475\n",
      "Iteration 294, loss = 0.48990961\n",
      "Iteration 295, loss = 0.48962379\n",
      "Iteration 296, loss = 0.48934362\n",
      "Iteration 297, loss = 0.48906243\n",
      "Iteration 298, loss = 0.48877838\n",
      "Iteration 299, loss = 0.48850286\n",
      "Iteration 300, loss = 0.48823345\n",
      "Iteration 301, loss = 0.48795987\n",
      "Iteration 302, loss = 0.48769355\n",
      "Iteration 303, loss = 0.48742634\n",
      "Iteration 304, loss = 0.48716855\n",
      "Iteration 305, loss = 0.48690519\n",
      "Iteration 306, loss = 0.48665311\n",
      "Iteration 307, loss = 0.48639422\n",
      "Iteration 308, loss = 0.48614374\n",
      "Iteration 309, loss = 0.48589961\n",
      "Iteration 310, loss = 0.48564642\n",
      "Iteration 311, loss = 0.48540543\n",
      "Iteration 312, loss = 0.48515722\n",
      "Iteration 313, loss = 0.48492845\n",
      "Iteration 314, loss = 0.48468222\n",
      "Iteration 315, loss = 0.48445010\n",
      "Iteration 316, loss = 0.48422112\n",
      "Iteration 317, loss = 0.48399966\n",
      "Iteration 318, loss = 0.48376765\n",
      "Iteration 319, loss = 0.48354646\n",
      "Iteration 320, loss = 0.48332458\n",
      "Iteration 321, loss = 0.48310293\n",
      "Iteration 322, loss = 0.48288854\n",
      "Iteration 323, loss = 0.48267493\n",
      "Iteration 324, loss = 0.48246235\n",
      "Iteration 325, loss = 0.48225222\n",
      "Iteration 326, loss = 0.48204617\n",
      "Iteration 327, loss = 0.48184506\n",
      "Iteration 328, loss = 0.48163809\n",
      "Iteration 329, loss = 0.48143552\n",
      "Iteration 330, loss = 0.48123918\n",
      "Iteration 331, loss = 0.48104420\n",
      "Iteration 332, loss = 0.48084989\n",
      "Iteration 333, loss = 0.48066252\n",
      "Iteration 334, loss = 0.48047343\n",
      "Iteration 335, loss = 0.48028779\n",
      "Iteration 336, loss = 0.48009096\n",
      "Iteration 337, loss = 0.47991551\n",
      "Iteration 338, loss = 0.47973266\n",
      "Iteration 339, loss = 0.47955769\n",
      "Iteration 340, loss = 0.47938056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 341, loss = 0.47920533\n",
      "Iteration 342, loss = 0.47903781\n",
      "Iteration 343, loss = 0.47886474\n",
      "Iteration 344, loss = 0.47870097\n",
      "Iteration 345, loss = 0.47853939\n",
      "Iteration 346, loss = 0.47837297\n",
      "Iteration 347, loss = 0.47821287\n",
      "Iteration 348, loss = 0.47805355\n",
      "Iteration 349, loss = 0.47789549\n",
      "Iteration 350, loss = 0.47774247\n",
      "Iteration 351, loss = 0.47759450\n",
      "Iteration 352, loss = 0.47744427\n",
      "Iteration 353, loss = 0.47729585\n",
      "Iteration 354, loss = 0.47714451\n",
      "Iteration 355, loss = 0.47701022\n",
      "Iteration 356, loss = 0.47685949\n",
      "Iteration 357, loss = 0.47672301\n",
      "Iteration 358, loss = 0.47658145\n",
      "Iteration 359, loss = 0.47644181\n",
      "Iteration 360, loss = 0.47630909\n",
      "Iteration 361, loss = 0.47617677\n",
      "Iteration 362, loss = 0.47603872\n",
      "Iteration 363, loss = 0.47591194\n",
      "Iteration 364, loss = 0.47578176\n",
      "Iteration 365, loss = 0.47565512\n",
      "Iteration 366, loss = 0.47553307\n",
      "Iteration 367, loss = 0.47541172\n",
      "Iteration 368, loss = 0.47528571\n",
      "Iteration 369, loss = 0.47517363\n",
      "Iteration 370, loss = 0.47505340\n",
      "Iteration 371, loss = 0.47494736\n",
      "Iteration 372, loss = 0.47481738\n",
      "Iteration 373, loss = 0.47470295\n",
      "Iteration 374, loss = 0.47459394\n",
      "Iteration 375, loss = 0.47448407\n",
      "Iteration 376, loss = 0.47437794\n",
      "Iteration 377, loss = 0.47426481\n",
      "Iteration 378, loss = 0.47415706\n",
      "Iteration 379, loss = 0.47405563\n",
      "Iteration 380, loss = 0.47395938\n",
      "Iteration 381, loss = 0.47385354\n",
      "Iteration 382, loss = 0.47375441\n",
      "Iteration 383, loss = 0.47365519\n",
      "Iteration 384, loss = 0.47355600\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76720267\n",
      "Iteration 2, loss = 0.76111389\n",
      "Iteration 3, loss = 0.75522088\n",
      "Iteration 4, loss = 0.74952128\n",
      "Iteration 5, loss = 0.74401776\n",
      "Iteration 6, loss = 0.73870936\n",
      "Iteration 7, loss = 0.73358193\n",
      "Iteration 8, loss = 0.72865796\n",
      "Iteration 9, loss = 0.72390317\n",
      "Iteration 10, loss = 0.71932335\n",
      "Iteration 11, loss = 0.71493109\n",
      "Iteration 12, loss = 0.71072797\n",
      "Iteration 13, loss = 0.70669972\n",
      "Iteration 14, loss = 0.70283782\n",
      "Iteration 15, loss = 0.69914003\n",
      "Iteration 16, loss = 0.69562011\n",
      "Iteration 17, loss = 0.69223483\n",
      "Iteration 18, loss = 0.68903258\n",
      "Iteration 19, loss = 0.68597980\n",
      "Iteration 20, loss = 0.68309440\n",
      "Iteration 21, loss = 0.68035069\n",
      "Iteration 22, loss = 0.67775730\n",
      "Iteration 23, loss = 0.67530087\n",
      "Iteration 24, loss = 0.67298363\n",
      "Iteration 25, loss = 0.67079038\n",
      "Iteration 26, loss = 0.66871299\n",
      "Iteration 27, loss = 0.66676170\n",
      "Iteration 28, loss = 0.66493571\n",
      "Iteration 29, loss = 0.66320468\n",
      "Iteration 30, loss = 0.66158769\n",
      "Iteration 31, loss = 0.66007570\n",
      "Iteration 32, loss = 0.65864464\n",
      "Iteration 33, loss = 0.65730164\n",
      "Iteration 34, loss = 0.65602385\n",
      "Iteration 35, loss = 0.65483280\n",
      "Iteration 36, loss = 0.65370860\n",
      "Iteration 37, loss = 0.65263975\n",
      "Iteration 38, loss = 0.65164130\n",
      "Iteration 39, loss = 0.65068566\n",
      "Iteration 40, loss = 0.64978166\n",
      "Iteration 41, loss = 0.64891216\n",
      "Iteration 42, loss = 0.64808040\n",
      "Iteration 43, loss = 0.64728038\n",
      "Iteration 44, loss = 0.64650666\n",
      "Iteration 45, loss = 0.64576176\n",
      "Iteration 46, loss = 0.64502606\n",
      "Iteration 47, loss = 0.64431402\n",
      "Iteration 48, loss = 0.64361768\n",
      "Iteration 49, loss = 0.64292686\n",
      "Iteration 50, loss = 0.64224800\n",
      "Iteration 51, loss = 0.64157428\n",
      "Iteration 52, loss = 0.64090594\n",
      "Iteration 53, loss = 0.64023905\n",
      "Iteration 54, loss = 0.63957805\n",
      "Iteration 55, loss = 0.63891980\n",
      "Iteration 56, loss = 0.63825506\n",
      "Iteration 57, loss = 0.63759169\n",
      "Iteration 58, loss = 0.63692617\n",
      "Iteration 59, loss = 0.63625982\n",
      "Iteration 60, loss = 0.63558700\n",
      "Iteration 61, loss = 0.63491599\n",
      "Iteration 62, loss = 0.63423452\n",
      "Iteration 63, loss = 0.63355311\n",
      "Iteration 64, loss = 0.63286819\n",
      "Iteration 65, loss = 0.63218193\n",
      "Iteration 66, loss = 0.63148714\n",
      "Iteration 67, loss = 0.63079068\n",
      "Iteration 68, loss = 0.63008671\n",
      "Iteration 69, loss = 0.62938245\n",
      "Iteration 70, loss = 0.62867084\n",
      "Iteration 71, loss = 0.62795602\n",
      "Iteration 72, loss = 0.62723542\n",
      "Iteration 73, loss = 0.62651220\n",
      "Iteration 74, loss = 0.62578694\n",
      "Iteration 75, loss = 0.62505329\n",
      "Iteration 76, loss = 0.62431780\n",
      "Iteration 77, loss = 0.62358024\n",
      "Iteration 78, loss = 0.62283663\n",
      "Iteration 79, loss = 0.62208853\n",
      "Iteration 80, loss = 0.62134244\n",
      "Iteration 81, loss = 0.62059177\n",
      "Iteration 82, loss = 0.61984087\n",
      "Iteration 83, loss = 0.61908556\n",
      "Iteration 84, loss = 0.61833750\n",
      "Iteration 85, loss = 0.61757538\n",
      "Iteration 86, loss = 0.61682117\n",
      "Iteration 87, loss = 0.61606550\n",
      "Iteration 88, loss = 0.61530290\n",
      "Iteration 89, loss = 0.61453926\n",
      "Iteration 90, loss = 0.61377751\n",
      "Iteration 91, loss = 0.61301461\n",
      "Iteration 92, loss = 0.61224706\n",
      "Iteration 93, loss = 0.61148075\n",
      "Iteration 94, loss = 0.61071107\n",
      "Iteration 95, loss = 0.60993805\n",
      "Iteration 96, loss = 0.60917906\n",
      "Iteration 97, loss = 0.60840154\n",
      "Iteration 98, loss = 0.60763864\n",
      "Iteration 99, loss = 0.60686177\n",
      "Iteration 100, loss = 0.60609624\n",
      "Iteration 101, loss = 0.60532431\n",
      "Iteration 102, loss = 0.60455440\n",
      "Iteration 103, loss = 0.60378024\n",
      "Iteration 104, loss = 0.60300704\n",
      "Iteration 105, loss = 0.60223253\n",
      "Iteration 106, loss = 0.60145890\n",
      "Iteration 107, loss = 0.60068625\n",
      "Iteration 108, loss = 0.59991196\n",
      "Iteration 109, loss = 0.59913605\n",
      "Iteration 110, loss = 0.59836101\n",
      "Iteration 111, loss = 0.59758119\n",
      "Iteration 112, loss = 0.59680430\n",
      "Iteration 113, loss = 0.59603446\n",
      "Iteration 114, loss = 0.59525148\n",
      "Iteration 115, loss = 0.59447922\n",
      "Iteration 116, loss = 0.59370267\n",
      "Iteration 117, loss = 0.59292736\n",
      "Iteration 118, loss = 0.59216102\n",
      "Iteration 119, loss = 0.59137745\n",
      "Iteration 120, loss = 0.59060214\n",
      "Iteration 121, loss = 0.58983019\n",
      "Iteration 122, loss = 0.58905851\n",
      "Iteration 123, loss = 0.58829115\n",
      "Iteration 124, loss = 0.58751697\n",
      "Iteration 125, loss = 0.58674736\n",
      "Iteration 126, loss = 0.58597925\n",
      "Iteration 127, loss = 0.58520864\n",
      "Iteration 128, loss = 0.58444482\n",
      "Iteration 129, loss = 0.58367138\n",
      "Iteration 130, loss = 0.58290959\n",
      "Iteration 131, loss = 0.58214210\n",
      "Iteration 132, loss = 0.58137544\n",
      "Iteration 133, loss = 0.58060897\n",
      "Iteration 134, loss = 0.57984293\n",
      "Iteration 135, loss = 0.57908143\n",
      "Iteration 136, loss = 0.57831592\n",
      "Iteration 137, loss = 0.57754804\n",
      "Iteration 138, loss = 0.57678468\n",
      "Iteration 139, loss = 0.57602202\n",
      "Iteration 140, loss = 0.57526452\n",
      "Iteration 141, loss = 0.57450517\n",
      "Iteration 142, loss = 0.57374677\n",
      "Iteration 143, loss = 0.57299491\n",
      "Iteration 144, loss = 0.57224065\n",
      "Iteration 145, loss = 0.57149280\n",
      "Iteration 146, loss = 0.57073786\n",
      "Iteration 147, loss = 0.56999264\n",
      "Iteration 148, loss = 0.56924574\n",
      "Iteration 149, loss = 0.56850127\n",
      "Iteration 150, loss = 0.56776101\n",
      "Iteration 151, loss = 0.56702508\n",
      "Iteration 152, loss = 0.56628865\n",
      "Iteration 153, loss = 0.56555850\n",
      "Iteration 154, loss = 0.56482814\n",
      "Iteration 155, loss = 0.56410058\n",
      "Iteration 156, loss = 0.56337624\n",
      "Iteration 157, loss = 0.56265128\n",
      "Iteration 158, loss = 0.56192640\n",
      "Iteration 159, loss = 0.56120327\n",
      "Iteration 160, loss = 0.56048197\n",
      "Iteration 161, loss = 0.55976078\n",
      "Iteration 162, loss = 0.55905159\n",
      "Iteration 163, loss = 0.55832984\n",
      "Iteration 164, loss = 0.55761642\n",
      "Iteration 165, loss = 0.55690738\n",
      "Iteration 166, loss = 0.55619957\n",
      "Iteration 167, loss = 0.55549839\n",
      "Iteration 168, loss = 0.55478949\n",
      "Iteration 169, loss = 0.55408897\n",
      "Iteration 170, loss = 0.55339066\n",
      "Iteration 171, loss = 0.55269623\n",
      "Iteration 172, loss = 0.55201227\n",
      "Iteration 173, loss = 0.55131594\n",
      "Iteration 174, loss = 0.55063213\n",
      "Iteration 175, loss = 0.54995023\n",
      "Iteration 176, loss = 0.54926643\n",
      "Iteration 177, loss = 0.54859083\n",
      "Iteration 178, loss = 0.54792039\n",
      "Iteration 179, loss = 0.54724791\n",
      "Iteration 180, loss = 0.54658714\n",
      "Iteration 181, loss = 0.54591819\n",
      "Iteration 182, loss = 0.54525799\n",
      "Iteration 183, loss = 0.54460700\n",
      "Iteration 184, loss = 0.54394943\n",
      "Iteration 185, loss = 0.54330073\n",
      "Iteration 186, loss = 0.54266371\n",
      "Iteration 187, loss = 0.54201729\n",
      "Iteration 188, loss = 0.54138119\n",
      "Iteration 189, loss = 0.54074343\n",
      "Iteration 190, loss = 0.54010798\n",
      "Iteration 191, loss = 0.53947737\n",
      "Iteration 192, loss = 0.53884559\n",
      "Iteration 193, loss = 0.53822583\n",
      "Iteration 194, loss = 0.53760568\n",
      "Iteration 195, loss = 0.53698469\n",
      "Iteration 196, loss = 0.53637440\n",
      "Iteration 197, loss = 0.53576481\n",
      "Iteration 198, loss = 0.53515810\n",
      "Iteration 199, loss = 0.53455642\n",
      "Iteration 200, loss = 0.53395082\n",
      "Iteration 201, loss = 0.53335331\n",
      "Iteration 202, loss = 0.53275686\n",
      "Iteration 203, loss = 0.53216692\n",
      "Iteration 204, loss = 0.53157651\n",
      "Iteration 205, loss = 0.53098995\n",
      "Iteration 206, loss = 0.53040534\n",
      "Iteration 207, loss = 0.52983001\n",
      "Iteration 208, loss = 0.52925876\n",
      "Iteration 209, loss = 0.52868058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 210, loss = 0.52811881\n",
      "Iteration 211, loss = 0.52755160\n",
      "Iteration 212, loss = 0.52699275\n",
      "Iteration 213, loss = 0.52643398\n",
      "Iteration 214, loss = 0.52587962\n",
      "Iteration 215, loss = 0.52532182\n",
      "Iteration 216, loss = 0.52477939\n",
      "Iteration 217, loss = 0.52423174\n",
      "Iteration 218, loss = 0.52369055\n",
      "Iteration 219, loss = 0.52315111\n",
      "Iteration 220, loss = 0.52261406\n",
      "Iteration 221, loss = 0.52208489\n",
      "Iteration 222, loss = 0.52155604\n",
      "Iteration 223, loss = 0.52102001\n",
      "Iteration 224, loss = 0.52049367\n",
      "Iteration 225, loss = 0.51997985\n",
      "Iteration 226, loss = 0.51946017\n",
      "Iteration 227, loss = 0.51895318\n",
      "Iteration 228, loss = 0.51844394\n",
      "Iteration 229, loss = 0.51794052\n",
      "Iteration 230, loss = 0.51744151\n",
      "Iteration 231, loss = 0.51694530\n",
      "Iteration 232, loss = 0.51644787\n",
      "Iteration 233, loss = 0.51596221\n",
      "Iteration 234, loss = 0.51547812\n",
      "Iteration 235, loss = 0.51499100\n",
      "Iteration 236, loss = 0.51451590\n",
      "Iteration 237, loss = 0.51404946\n",
      "Iteration 238, loss = 0.51357245\n",
      "Iteration 239, loss = 0.51310076\n",
      "Iteration 240, loss = 0.51264874\n",
      "Iteration 241, loss = 0.51218156\n",
      "Iteration 242, loss = 0.51172949\n",
      "Iteration 243, loss = 0.51127856\n",
      "Iteration 244, loss = 0.51082947\n",
      "Iteration 245, loss = 0.51038276\n",
      "Iteration 246, loss = 0.50993775\n",
      "Iteration 247, loss = 0.50949837\n",
      "Iteration 248, loss = 0.50905969\n",
      "Iteration 249, loss = 0.50863420\n",
      "Iteration 250, loss = 0.50819791\n",
      "Iteration 251, loss = 0.50776689\n",
      "Iteration 252, loss = 0.50734211\n",
      "Iteration 253, loss = 0.50692510\n",
      "Iteration 254, loss = 0.50650327\n",
      "Iteration 255, loss = 0.50609616\n",
      "Iteration 256, loss = 0.50568429\n",
      "Iteration 257, loss = 0.50526881\n",
      "Iteration 258, loss = 0.50486612\n",
      "Iteration 259, loss = 0.50446447\n",
      "Iteration 260, loss = 0.50406367\n",
      "Iteration 261, loss = 0.50366339\n",
      "Iteration 262, loss = 0.50327175\n",
      "Iteration 263, loss = 0.50288481\n",
      "Iteration 264, loss = 0.50249981\n",
      "Iteration 265, loss = 0.50211239\n",
      "Iteration 266, loss = 0.50173284\n",
      "Iteration 267, loss = 0.50136133\n",
      "Iteration 268, loss = 0.50098665\n",
      "Iteration 269, loss = 0.50061282\n",
      "Iteration 270, loss = 0.50024424\n",
      "Iteration 271, loss = 0.49987976\n",
      "Iteration 272, loss = 0.49952595\n",
      "Iteration 273, loss = 0.49916525\n",
      "Iteration 274, loss = 0.49880486\n",
      "Iteration 275, loss = 0.49845336\n",
      "Iteration 276, loss = 0.49810641\n",
      "Iteration 277, loss = 0.49775837\n",
      "Iteration 278, loss = 0.49741597\n",
      "Iteration 279, loss = 0.49707747\n",
      "Iteration 280, loss = 0.49673487\n",
      "Iteration 281, loss = 0.49640237\n",
      "Iteration 282, loss = 0.49607677\n",
      "Iteration 283, loss = 0.49573742\n",
      "Iteration 284, loss = 0.49541484\n",
      "Iteration 285, loss = 0.49509092\n",
      "Iteration 286, loss = 0.49476944\n",
      "Iteration 287, loss = 0.49445384\n",
      "Iteration 288, loss = 0.49414027\n",
      "Iteration 289, loss = 0.49382763\n",
      "Iteration 290, loss = 0.49353362\n",
      "Iteration 291, loss = 0.49321088\n",
      "Iteration 292, loss = 0.49291346\n",
      "Iteration 293, loss = 0.49261697\n",
      "Iteration 294, loss = 0.49231377\n",
      "Iteration 295, loss = 0.49201877\n",
      "Iteration 296, loss = 0.49172929\n",
      "Iteration 297, loss = 0.49144616\n",
      "Iteration 298, loss = 0.49115442\n",
      "Iteration 299, loss = 0.49087234\n",
      "Iteration 300, loss = 0.49059438\n",
      "Iteration 301, loss = 0.49031548\n",
      "Iteration 302, loss = 0.49004028\n",
      "Iteration 303, loss = 0.48976798\n",
      "Iteration 304, loss = 0.48950119\n",
      "Iteration 305, loss = 0.48923172\n",
      "Iteration 306, loss = 0.48897206\n",
      "Iteration 307, loss = 0.48871097\n",
      "Iteration 308, loss = 0.48844999\n",
      "Iteration 309, loss = 0.48819590\n",
      "Iteration 310, loss = 0.48793798\n",
      "Iteration 311, loss = 0.48769054\n",
      "Iteration 312, loss = 0.48743509\n",
      "Iteration 313, loss = 0.48719923\n",
      "Iteration 314, loss = 0.48694542\n",
      "Iteration 315, loss = 0.48670596\n",
      "Iteration 316, loss = 0.48646874\n",
      "Iteration 317, loss = 0.48623704\n",
      "Iteration 318, loss = 0.48600223\n",
      "Iteration 319, loss = 0.48576853\n",
      "Iteration 320, loss = 0.48554396\n",
      "Iteration 321, loss = 0.48531245\n",
      "Iteration 322, loss = 0.48508829\n",
      "Iteration 323, loss = 0.48486690\n",
      "Iteration 324, loss = 0.48464499\n",
      "Iteration 325, loss = 0.48442674\n",
      "Iteration 326, loss = 0.48421200\n",
      "Iteration 327, loss = 0.48400058\n",
      "Iteration 328, loss = 0.48379045\n",
      "Iteration 329, loss = 0.48357820\n",
      "Iteration 330, loss = 0.48337097\n",
      "Iteration 331, loss = 0.48316512\n",
      "Iteration 332, loss = 0.48296012\n",
      "Iteration 333, loss = 0.48276477\n",
      "Iteration 334, loss = 0.48257258\n",
      "Iteration 335, loss = 0.48237180\n",
      "Iteration 336, loss = 0.48216724\n",
      "Iteration 337, loss = 0.48198069\n",
      "Iteration 338, loss = 0.48179168\n",
      "Iteration 339, loss = 0.48160862\n",
      "Iteration 340, loss = 0.48142355\n",
      "Iteration 341, loss = 0.48123941\n",
      "Iteration 342, loss = 0.48106626\n",
      "Iteration 343, loss = 0.48088470\n",
      "Iteration 344, loss = 0.48070935\n",
      "Iteration 345, loss = 0.48053970\n",
      "Iteration 346, loss = 0.48036539\n",
      "Iteration 347, loss = 0.48019496\n",
      "Iteration 348, loss = 0.48002588\n",
      "Iteration 349, loss = 0.47985953\n",
      "Iteration 350, loss = 0.47970425\n",
      "Iteration 351, loss = 0.47953716\n",
      "Iteration 352, loss = 0.47937894\n",
      "Iteration 353, loss = 0.47922393\n",
      "Iteration 354, loss = 0.47906174\n",
      "Iteration 355, loss = 0.47891714\n",
      "Iteration 356, loss = 0.47875902\n",
      "Iteration 357, loss = 0.47861614\n",
      "Iteration 358, loss = 0.47847274\n",
      "Iteration 359, loss = 0.47831420\n",
      "Iteration 360, loss = 0.47817218\n",
      "Iteration 361, loss = 0.47803091\n",
      "Iteration 362, loss = 0.47788193\n",
      "Iteration 363, loss = 0.47774852\n",
      "Iteration 364, loss = 0.47761044\n",
      "Iteration 365, loss = 0.47747031\n",
      "Iteration 366, loss = 0.47733774\n",
      "Iteration 367, loss = 0.47720700\n",
      "Iteration 368, loss = 0.47707470\n",
      "Iteration 369, loss = 0.47695011\n",
      "Iteration 370, loss = 0.47682298\n",
      "Iteration 371, loss = 0.47670249\n",
      "Iteration 372, loss = 0.47657102\n",
      "Iteration 373, loss = 0.47644572\n",
      "Iteration 374, loss = 0.47632344\n",
      "Iteration 375, loss = 0.47620287\n",
      "Iteration 376, loss = 0.47609020\n",
      "Iteration 377, loss = 0.47596688\n",
      "Iteration 378, loss = 0.47584880\n",
      "Iteration 379, loss = 0.47573768\n",
      "Iteration 380, loss = 0.47562409\n",
      "Iteration 381, loss = 0.47551596\n",
      "Iteration 382, loss = 0.47540771\n",
      "Iteration 383, loss = 0.47530004\n",
      "Iteration 384, loss = 0.47518921\n",
      "Iteration 385, loss = 0.47508852\n",
      "Iteration 386, loss = 0.47498051\n",
      "Iteration 387, loss = 0.47488616\n",
      "Iteration 388, loss = 0.47478950\n",
      "Iteration 389, loss = 0.47468245\n",
      "Iteration 390, loss = 0.47458274\n",
      "Iteration 391, loss = 0.47448851\n",
      "Iteration 392, loss = 0.47440738\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77418369\n",
      "Iteration 2, loss = 0.76780474\n",
      "Iteration 3, loss = 0.76161855\n",
      "Iteration 4, loss = 0.75561803\n",
      "Iteration 5, loss = 0.74980963\n",
      "Iteration 6, loss = 0.74420041\n",
      "Iteration 7, loss = 0.73876826\n",
      "Iteration 8, loss = 0.73354538\n",
      "Iteration 9, loss = 0.72848337\n",
      "Iteration 10, loss = 0.72360518\n",
      "Iteration 11, loss = 0.71891153\n",
      "Iteration 12, loss = 0.71441032\n",
      "Iteration 13, loss = 0.71009065\n",
      "Iteration 14, loss = 0.70594226\n",
      "Iteration 15, loss = 0.70196318\n",
      "Iteration 16, loss = 0.69815991\n",
      "Iteration 17, loss = 0.69450001\n",
      "Iteration 18, loss = 0.69101858\n",
      "Iteration 19, loss = 0.68769924\n",
      "Iteration 20, loss = 0.68454627\n",
      "Iteration 21, loss = 0.68154786\n",
      "Iteration 22, loss = 0.67869919\n",
      "Iteration 23, loss = 0.67599839\n",
      "Iteration 24, loss = 0.67343963\n",
      "Iteration 25, loss = 0.67101514\n",
      "Iteration 26, loss = 0.66871363\n",
      "Iteration 27, loss = 0.66654643\n",
      "Iteration 28, loss = 0.66452015\n",
      "Iteration 29, loss = 0.66259404\n",
      "Iteration 30, loss = 0.66079454\n",
      "Iteration 31, loss = 0.65910615\n",
      "Iteration 32, loss = 0.65751694\n",
      "Iteration 33, loss = 0.65602520\n",
      "Iteration 34, loss = 0.65460557\n",
      "Iteration 35, loss = 0.65328814\n",
      "Iteration 36, loss = 0.65204681\n",
      "Iteration 37, loss = 0.65087305\n",
      "Iteration 38, loss = 0.64977915\n",
      "Iteration 39, loss = 0.64874189\n",
      "Iteration 40, loss = 0.64777001\n",
      "Iteration 41, loss = 0.64684018\n",
      "Iteration 42, loss = 0.64596180\n",
      "Iteration 43, loss = 0.64512111\n",
      "Iteration 44, loss = 0.64431633\n",
      "Iteration 45, loss = 0.64354766\n",
      "Iteration 46, loss = 0.64279901\n",
      "Iteration 47, loss = 0.64207816\n",
      "Iteration 48, loss = 0.64138153\n",
      "Iteration 49, loss = 0.64069689\n",
      "Iteration 50, loss = 0.64003073\n",
      "Iteration 51, loss = 0.63937366\n",
      "Iteration 52, loss = 0.63872523\n",
      "Iteration 53, loss = 0.63808280\n",
      "Iteration 54, loss = 0.63745051\n",
      "Iteration 55, loss = 0.63682213\n",
      "Iteration 56, loss = 0.63619262\n",
      "Iteration 57, loss = 0.63556424\n",
      "Iteration 58, loss = 0.63493625\n",
      "Iteration 59, loss = 0.63430779\n",
      "Iteration 60, loss = 0.63367621\n",
      "Iteration 61, loss = 0.63304681\n",
      "Iteration 62, loss = 0.63240756\n",
      "Iteration 63, loss = 0.63176926\n",
      "Iteration 64, loss = 0.63112724\n",
      "Iteration 65, loss = 0.63048388\n",
      "Iteration 66, loss = 0.62983207\n",
      "Iteration 67, loss = 0.62917812\n",
      "Iteration 68, loss = 0.62851713\n",
      "Iteration 69, loss = 0.62785463\n",
      "Iteration 70, loss = 0.62718694\n",
      "Iteration 71, loss = 0.62651688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.62583984\n",
      "Iteration 73, loss = 0.62516004\n",
      "Iteration 74, loss = 0.62447919\n",
      "Iteration 75, loss = 0.62378784\n",
      "Iteration 76, loss = 0.62309624\n",
      "Iteration 77, loss = 0.62240163\n",
      "Iteration 78, loss = 0.62170196\n",
      "Iteration 79, loss = 0.62099696\n",
      "Iteration 80, loss = 0.62029376\n",
      "Iteration 81, loss = 0.61958632\n",
      "Iteration 82, loss = 0.61887997\n",
      "Iteration 83, loss = 0.61816579\n",
      "Iteration 84, loss = 0.61746224\n",
      "Iteration 85, loss = 0.61673931\n",
      "Iteration 86, loss = 0.61602538\n",
      "Iteration 87, loss = 0.61530761\n",
      "Iteration 88, loss = 0.61458723\n",
      "Iteration 89, loss = 0.61386121\n",
      "Iteration 90, loss = 0.61313650\n",
      "Iteration 91, loss = 0.61241088\n",
      "Iteration 92, loss = 0.61168300\n",
      "Iteration 93, loss = 0.61094803\n",
      "Iteration 94, loss = 0.61022023\n",
      "Iteration 95, loss = 0.60948468\n",
      "Iteration 96, loss = 0.60876047\n",
      "Iteration 97, loss = 0.60802211\n",
      "Iteration 98, loss = 0.60729443\n",
      "Iteration 99, loss = 0.60655931\n",
      "Iteration 100, loss = 0.60583195\n",
      "Iteration 101, loss = 0.60509953\n",
      "Iteration 102, loss = 0.60436739\n",
      "Iteration 103, loss = 0.60363436\n",
      "Iteration 104, loss = 0.60290207\n",
      "Iteration 105, loss = 0.60216897\n",
      "Iteration 106, loss = 0.60143227\n",
      "Iteration 107, loss = 0.60070008\n",
      "Iteration 108, loss = 0.59996577\n",
      "Iteration 109, loss = 0.59922754\n",
      "Iteration 110, loss = 0.59849091\n",
      "Iteration 111, loss = 0.59775113\n",
      "Iteration 112, loss = 0.59701427\n",
      "Iteration 113, loss = 0.59627876\n",
      "Iteration 114, loss = 0.59553514\n",
      "Iteration 115, loss = 0.59480092\n",
      "Iteration 116, loss = 0.59406094\n",
      "Iteration 117, loss = 0.59332485\n",
      "Iteration 118, loss = 0.59259555\n",
      "Iteration 119, loss = 0.59185128\n",
      "Iteration 120, loss = 0.59111236\n",
      "Iteration 121, loss = 0.59037932\n",
      "Iteration 122, loss = 0.58964498\n",
      "Iteration 123, loss = 0.58891350\n",
      "Iteration 124, loss = 0.58817825\n",
      "Iteration 125, loss = 0.58744584\n",
      "Iteration 126, loss = 0.58671580\n",
      "Iteration 127, loss = 0.58598024\n",
      "Iteration 128, loss = 0.58525238\n",
      "Iteration 129, loss = 0.58451774\n",
      "Iteration 130, loss = 0.58379278\n",
      "Iteration 131, loss = 0.58306305\n",
      "Iteration 132, loss = 0.58233512\n",
      "Iteration 133, loss = 0.58160779\n",
      "Iteration 134, loss = 0.58088007\n",
      "Iteration 135, loss = 0.58015589\n",
      "Iteration 136, loss = 0.57943131\n",
      "Iteration 137, loss = 0.57870269\n",
      "Iteration 138, loss = 0.57798033\n",
      "Iteration 139, loss = 0.57725581\n",
      "Iteration 140, loss = 0.57653663\n",
      "Iteration 141, loss = 0.57581791\n",
      "Iteration 142, loss = 0.57509949\n",
      "Iteration 143, loss = 0.57438883\n",
      "Iteration 144, loss = 0.57367594\n",
      "Iteration 145, loss = 0.57296561\n",
      "Iteration 146, loss = 0.57225383\n",
      "Iteration 147, loss = 0.57155174\n",
      "Iteration 148, loss = 0.57084247\n",
      "Iteration 149, loss = 0.57013846\n",
      "Iteration 150, loss = 0.56943805\n",
      "Iteration 151, loss = 0.56874004\n",
      "Iteration 152, loss = 0.56804325\n",
      "Iteration 153, loss = 0.56735202\n",
      "Iteration 154, loss = 0.56665843\n",
      "Iteration 155, loss = 0.56596892\n",
      "Iteration 156, loss = 0.56528282\n",
      "Iteration 157, loss = 0.56459637\n",
      "Iteration 158, loss = 0.56390653\n",
      "Iteration 159, loss = 0.56322226\n",
      "Iteration 160, loss = 0.56253701\n",
      "Iteration 161, loss = 0.56185261\n",
      "Iteration 162, loss = 0.56117717\n",
      "Iteration 163, loss = 0.56049043\n",
      "Iteration 164, loss = 0.55981290\n",
      "Iteration 165, loss = 0.55913750\n",
      "Iteration 166, loss = 0.55846729\n",
      "Iteration 167, loss = 0.55779753\n",
      "Iteration 168, loss = 0.55712306\n",
      "Iteration 169, loss = 0.55645679\n",
      "Iteration 170, loss = 0.55579478\n",
      "Iteration 171, loss = 0.55513307\n",
      "Iteration 172, loss = 0.55448621\n",
      "Iteration 173, loss = 0.55382048\n",
      "Iteration 174, loss = 0.55316896\n",
      "Iteration 175, loss = 0.55252260\n",
      "Iteration 176, loss = 0.55187214\n",
      "Iteration 177, loss = 0.55123006\n",
      "Iteration 178, loss = 0.55059150\n",
      "Iteration 179, loss = 0.54995037\n",
      "Iteration 180, loss = 0.54932011\n",
      "Iteration 181, loss = 0.54868548\n",
      "Iteration 182, loss = 0.54805635\n",
      "Iteration 183, loss = 0.54743542\n",
      "Iteration 184, loss = 0.54681156\n",
      "Iteration 185, loss = 0.54619360\n",
      "Iteration 186, loss = 0.54558753\n",
      "Iteration 187, loss = 0.54496925\n",
      "Iteration 188, loss = 0.54436424\n",
      "Iteration 189, loss = 0.54375813\n",
      "Iteration 190, loss = 0.54315242\n",
      "Iteration 191, loss = 0.54255503\n",
      "Iteration 192, loss = 0.54195631\n",
      "Iteration 193, loss = 0.54135776\n",
      "Iteration 194, loss = 0.54076790\n",
      "Iteration 195, loss = 0.54017417\n",
      "Iteration 196, loss = 0.53959375\n",
      "Iteration 197, loss = 0.53900795\n",
      "Iteration 198, loss = 0.53842703\n",
      "Iteration 199, loss = 0.53784974\n",
      "Iteration 200, loss = 0.53727275\n",
      "Iteration 201, loss = 0.53670186\n",
      "Iteration 202, loss = 0.53612920\n",
      "Iteration 203, loss = 0.53556902\n",
      "Iteration 204, loss = 0.53500295\n",
      "Iteration 205, loss = 0.53444376\n",
      "Iteration 206, loss = 0.53388465\n",
      "Iteration 207, loss = 0.53333683\n",
      "Iteration 208, loss = 0.53279198\n",
      "Iteration 209, loss = 0.53224051\n",
      "Iteration 210, loss = 0.53171115\n",
      "Iteration 211, loss = 0.53116463\n",
      "Iteration 212, loss = 0.53063256\n",
      "Iteration 213, loss = 0.53010353\n",
      "Iteration 214, loss = 0.52957311\n",
      "Iteration 215, loss = 0.52904117\n",
      "Iteration 216, loss = 0.52852368\n",
      "Iteration 217, loss = 0.52800047\n",
      "Iteration 218, loss = 0.52748252\n",
      "Iteration 219, loss = 0.52696744\n",
      "Iteration 220, loss = 0.52645333\n",
      "Iteration 221, loss = 0.52594756\n",
      "Iteration 222, loss = 0.52544354\n",
      "Iteration 223, loss = 0.52493256\n",
      "Iteration 224, loss = 0.52442877\n",
      "Iteration 225, loss = 0.52393888\n",
      "Iteration 226, loss = 0.52344100\n",
      "Iteration 227, loss = 0.52295631\n",
      "Iteration 228, loss = 0.52247064\n",
      "Iteration 229, loss = 0.52198916\n",
      "Iteration 230, loss = 0.52151182\n",
      "Iteration 231, loss = 0.52103667\n",
      "Iteration 232, loss = 0.52055837\n",
      "Iteration 233, loss = 0.52009351\n",
      "Iteration 234, loss = 0.51962553\n",
      "Iteration 235, loss = 0.51915797\n",
      "Iteration 236, loss = 0.51869792\n",
      "Iteration 237, loss = 0.51825525\n",
      "Iteration 238, loss = 0.51779157\n",
      "Iteration 239, loss = 0.51733785\n",
      "Iteration 240, loss = 0.51689987\n",
      "Iteration 241, loss = 0.51644929\n",
      "Iteration 242, loss = 0.51601328\n",
      "Iteration 243, loss = 0.51557646\n",
      "Iteration 244, loss = 0.51513855\n",
      "Iteration 245, loss = 0.51470697\n",
      "Iteration 246, loss = 0.51427580\n",
      "Iteration 247, loss = 0.51385118\n",
      "Iteration 248, loss = 0.51342511\n",
      "Iteration 249, loss = 0.51301123\n",
      "Iteration 250, loss = 0.51258995\n",
      "Iteration 251, loss = 0.51217075\n",
      "Iteration 252, loss = 0.51175874\n",
      "Iteration 253, loss = 0.51135707\n",
      "Iteration 254, loss = 0.51094491\n",
      "Iteration 255, loss = 0.51054841\n",
      "Iteration 256, loss = 0.51014937\n",
      "Iteration 257, loss = 0.50974561\n",
      "Iteration 258, loss = 0.50935701\n",
      "Iteration 259, loss = 0.50896941\n",
      "Iteration 260, loss = 0.50857856\n",
      "Iteration 261, loss = 0.50818881\n",
      "Iteration 262, loss = 0.50781059\n",
      "Iteration 263, loss = 0.50743386\n",
      "Iteration 264, loss = 0.50706686\n",
      "Iteration 265, loss = 0.50668473\n",
      "Iteration 266, loss = 0.50631352\n",
      "Iteration 267, loss = 0.50595283\n",
      "Iteration 268, loss = 0.50558931\n",
      "Iteration 269, loss = 0.50522649\n",
      "Iteration 270, loss = 0.50486950\n",
      "Iteration 271, loss = 0.50451761\n",
      "Iteration 272, loss = 0.50417207\n",
      "Iteration 273, loss = 0.50382620\n",
      "Iteration 274, loss = 0.50347532\n",
      "Iteration 275, loss = 0.50313120\n",
      "Iteration 276, loss = 0.50279565\n",
      "Iteration 277, loss = 0.50245697\n",
      "Iteration 278, loss = 0.50212473\n",
      "Iteration 279, loss = 0.50179194\n",
      "Iteration 280, loss = 0.50146177\n",
      "Iteration 281, loss = 0.50113648\n",
      "Iteration 282, loss = 0.50082134\n",
      "Iteration 283, loss = 0.50049208\n",
      "Iteration 284, loss = 0.50017998\n",
      "Iteration 285, loss = 0.49986152\n",
      "Iteration 286, loss = 0.49954796\n",
      "Iteration 287, loss = 0.49923487\n",
      "Iteration 288, loss = 0.49893064\n",
      "Iteration 289, loss = 0.49862770\n",
      "Iteration 290, loss = 0.49833116\n",
      "Iteration 291, loss = 0.49802296\n",
      "Iteration 292, loss = 0.49772869\n",
      "Iteration 293, loss = 0.49743761\n",
      "Iteration 294, loss = 0.49714322\n",
      "Iteration 295, loss = 0.49685399\n",
      "Iteration 296, loss = 0.49656981\n",
      "Iteration 297, loss = 0.49629229\n",
      "Iteration 298, loss = 0.49600364\n",
      "Iteration 299, loss = 0.49573064\n",
      "Iteration 300, loss = 0.49545602\n",
      "Iteration 301, loss = 0.49518096\n",
      "Iteration 302, loss = 0.49490865\n",
      "Iteration 303, loss = 0.49464297\n",
      "Iteration 304, loss = 0.49437766\n",
      "Iteration 305, loss = 0.49411483\n",
      "Iteration 306, loss = 0.49386013\n",
      "Iteration 307, loss = 0.49360087\n",
      "Iteration 308, loss = 0.49334245\n",
      "Iteration 309, loss = 0.49309474\n",
      "Iteration 310, loss = 0.49283738\n",
      "Iteration 311, loss = 0.49259529\n",
      "Iteration 312, loss = 0.49234311\n",
      "Iteration 313, loss = 0.49211031\n",
      "Iteration 314, loss = 0.49185728\n",
      "Iteration 315, loss = 0.49162009\n",
      "Iteration 316, loss = 0.49138411\n",
      "Iteration 317, loss = 0.49115502\n",
      "Iteration 318, loss = 0.49092489\n",
      "Iteration 319, loss = 0.49069022\n",
      "Iteration 320, loss = 0.49046835\n",
      "Iteration 321, loss = 0.49023690\n",
      "Iteration 322, loss = 0.49001531\n",
      "Iteration 323, loss = 0.48979564\n",
      "Iteration 324, loss = 0.48957413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 325, loss = 0.48935673\n",
      "Iteration 326, loss = 0.48914336\n",
      "Iteration 327, loss = 0.48893353\n",
      "Iteration 328, loss = 0.48872314\n",
      "Iteration 329, loss = 0.48851300\n",
      "Iteration 330, loss = 0.48830611\n",
      "Iteration 331, loss = 0.48810522\n",
      "Iteration 332, loss = 0.48790329\n",
      "Iteration 333, loss = 0.48770724\n",
      "Iteration 334, loss = 0.48751605\n",
      "Iteration 335, loss = 0.48731478\n",
      "Iteration 336, loss = 0.48711738\n",
      "Iteration 337, loss = 0.48692783\n",
      "Iteration 338, loss = 0.48674362\n",
      "Iteration 339, loss = 0.48656050\n",
      "Iteration 340, loss = 0.48637768\n",
      "Iteration 341, loss = 0.48619459\n",
      "Iteration 342, loss = 0.48601946\n",
      "Iteration 343, loss = 0.48584072\n",
      "Iteration 344, loss = 0.48566855\n",
      "Iteration 345, loss = 0.48550015\n",
      "Iteration 346, loss = 0.48532607\n",
      "Iteration 347, loss = 0.48515261\n",
      "Iteration 348, loss = 0.48498590\n",
      "Iteration 349, loss = 0.48482050\n",
      "Iteration 350, loss = 0.48466055\n",
      "Iteration 351, loss = 0.48450042\n",
      "Iteration 352, loss = 0.48434018\n",
      "Iteration 353, loss = 0.48418101\n",
      "Iteration 354, loss = 0.48402100\n",
      "Iteration 355, loss = 0.48387043\n",
      "Iteration 356, loss = 0.48371476\n",
      "Iteration 357, loss = 0.48356889\n",
      "Iteration 358, loss = 0.48342335\n",
      "Iteration 359, loss = 0.48326752\n",
      "Iteration 360, loss = 0.48312141\n",
      "Iteration 361, loss = 0.48298342\n",
      "Iteration 362, loss = 0.48283437\n",
      "Iteration 363, loss = 0.48269777\n",
      "Iteration 364, loss = 0.48255912\n",
      "Iteration 365, loss = 0.48241915\n",
      "Iteration 366, loss = 0.48228377\n",
      "Iteration 367, loss = 0.48215433\n",
      "Iteration 368, loss = 0.48201807\n",
      "Iteration 369, loss = 0.48189027\n",
      "Iteration 370, loss = 0.48175806\n",
      "Iteration 371, loss = 0.48163501\n",
      "Iteration 372, loss = 0.48150535\n",
      "Iteration 373, loss = 0.48137921\n",
      "Iteration 374, loss = 0.48125469\n",
      "Iteration 375, loss = 0.48113158\n",
      "Iteration 376, loss = 0.48101475\n",
      "Iteration 377, loss = 0.48088904\n",
      "Iteration 378, loss = 0.48077328\n",
      "Iteration 379, loss = 0.48066298\n",
      "Iteration 380, loss = 0.48054466\n",
      "Iteration 381, loss = 0.48043295\n",
      "Iteration 382, loss = 0.48032066\n",
      "Iteration 383, loss = 0.48021185\n",
      "Iteration 384, loss = 0.48009721\n",
      "Iteration 385, loss = 0.47999503\n",
      "Iteration 386, loss = 0.47988514\n",
      "Iteration 387, loss = 0.47978717\n",
      "Iteration 388, loss = 0.47968750\n",
      "Iteration 389, loss = 0.47958104\n",
      "Iteration 390, loss = 0.47947884\n",
      "Iteration 391, loss = 0.47938032\n",
      "Iteration 392, loss = 0.47929276\n",
      "Iteration 393, loss = 0.47918525\n",
      "Iteration 394, loss = 0.47909748\n",
      "Iteration 395, loss = 0.47900374\n",
      "Iteration 396, loss = 0.47891234\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77594435\n",
      "Iteration 2, loss = 0.76946757\n",
      "Iteration 3, loss = 0.76318801\n",
      "Iteration 4, loss = 0.75709825\n",
      "Iteration 5, loss = 0.75120338\n",
      "Iteration 6, loss = 0.74550503\n",
      "Iteration 7, loss = 0.73998461\n",
      "Iteration 8, loss = 0.73467315\n",
      "Iteration 9, loss = 0.72952337\n",
      "Iteration 10, loss = 0.72455708\n",
      "Iteration 11, loss = 0.71977500\n",
      "Iteration 12, loss = 0.71518468\n",
      "Iteration 13, loss = 0.71077945\n",
      "Iteration 14, loss = 0.70654086\n",
      "Iteration 15, loss = 0.70247797\n",
      "Iteration 16, loss = 0.69858834\n",
      "Iteration 17, loss = 0.69484482\n",
      "Iteration 18, loss = 0.69127805\n",
      "Iteration 19, loss = 0.68787548\n",
      "Iteration 20, loss = 0.68463996\n",
      "Iteration 21, loss = 0.68156514\n",
      "Iteration 22, loss = 0.67863663\n",
      "Iteration 23, loss = 0.67586048\n",
      "Iteration 24, loss = 0.67323223\n",
      "Iteration 25, loss = 0.67073435\n",
      "Iteration 26, loss = 0.66836474\n",
      "Iteration 27, loss = 0.66612802\n",
      "Iteration 28, loss = 0.66403699\n",
      "Iteration 29, loss = 0.66205270\n",
      "Iteration 30, loss = 0.66019893\n",
      "Iteration 31, loss = 0.65845764\n",
      "Iteration 32, loss = 0.65681971\n",
      "Iteration 33, loss = 0.65528300\n",
      "Iteration 34, loss = 0.65382100\n",
      "Iteration 35, loss = 0.65246482\n",
      "Iteration 36, loss = 0.65119114\n",
      "Iteration 37, loss = 0.64998589\n",
      "Iteration 38, loss = 0.64886570\n",
      "Iteration 39, loss = 0.64780509\n",
      "Iteration 40, loss = 0.64681392\n",
      "Iteration 41, loss = 0.64586802\n",
      "Iteration 42, loss = 0.64497400\n",
      "Iteration 43, loss = 0.64412385\n",
      "Iteration 44, loss = 0.64331358\n",
      "Iteration 45, loss = 0.64254087\n",
      "Iteration 46, loss = 0.64179230\n",
      "Iteration 47, loss = 0.64107285\n",
      "Iteration 48, loss = 0.64038193\n",
      "Iteration 49, loss = 0.63970542\n",
      "Iteration 50, loss = 0.63904926\n",
      "Iteration 51, loss = 0.63840371\n",
      "Iteration 52, loss = 0.63776903\n",
      "Iteration 53, loss = 0.63714094\n",
      "Iteration 54, loss = 0.63652311\n",
      "Iteration 55, loss = 0.63591094\n",
      "Iteration 56, loss = 0.63529938\n",
      "Iteration 57, loss = 0.63468919\n",
      "Iteration 58, loss = 0.63408010\n",
      "Iteration 59, loss = 0.63346982\n",
      "Iteration 60, loss = 0.63285921\n",
      "Iteration 61, loss = 0.63224850\n",
      "Iteration 62, loss = 0.63162955\n",
      "Iteration 63, loss = 0.63101157\n",
      "Iteration 64, loss = 0.63039011\n",
      "Iteration 65, loss = 0.62976520\n",
      "Iteration 66, loss = 0.62913513\n",
      "Iteration 67, loss = 0.62850005\n",
      "Iteration 68, loss = 0.62785956\n",
      "Iteration 69, loss = 0.62721753\n",
      "Iteration 70, loss = 0.62656903\n",
      "Iteration 71, loss = 0.62591866\n",
      "Iteration 72, loss = 0.62526268\n",
      "Iteration 73, loss = 0.62460217\n",
      "Iteration 74, loss = 0.62394115\n",
      "Iteration 75, loss = 0.62326955\n",
      "Iteration 76, loss = 0.62259761\n",
      "Iteration 77, loss = 0.62192159\n",
      "Iteration 78, loss = 0.62124163\n",
      "Iteration 79, loss = 0.62055613\n",
      "Iteration 80, loss = 0.61987145\n",
      "Iteration 81, loss = 0.61918341\n",
      "Iteration 82, loss = 0.61849509\n",
      "Iteration 83, loss = 0.61780057\n",
      "Iteration 84, loss = 0.61711651\n",
      "Iteration 85, loss = 0.61641139\n",
      "Iteration 86, loss = 0.61571585\n",
      "Iteration 87, loss = 0.61501454\n",
      "Iteration 88, loss = 0.61431286\n",
      "Iteration 89, loss = 0.61360484\n",
      "Iteration 90, loss = 0.61289825\n",
      "Iteration 91, loss = 0.61219257\n",
      "Iteration 92, loss = 0.61148052\n",
      "Iteration 93, loss = 0.61076294\n",
      "Iteration 94, loss = 0.61005025\n",
      "Iteration 95, loss = 0.60933334\n",
      "Iteration 96, loss = 0.60862367\n",
      "Iteration 97, loss = 0.60790266\n",
      "Iteration 98, loss = 0.60718710\n",
      "Iteration 99, loss = 0.60647236\n",
      "Iteration 100, loss = 0.60576088\n",
      "Iteration 101, loss = 0.60504549\n",
      "Iteration 102, loss = 0.60432858\n",
      "Iteration 103, loss = 0.60361325\n",
      "Iteration 104, loss = 0.60289574\n",
      "Iteration 105, loss = 0.60217865\n",
      "Iteration 106, loss = 0.60146086\n",
      "Iteration 107, loss = 0.60074574\n",
      "Iteration 108, loss = 0.60002813\n",
      "Iteration 109, loss = 0.59930689\n",
      "Iteration 110, loss = 0.59858892\n",
      "Iteration 111, loss = 0.59786756\n",
      "Iteration 112, loss = 0.59714854\n",
      "Iteration 113, loss = 0.59642987\n",
      "Iteration 114, loss = 0.59570557\n",
      "Iteration 115, loss = 0.59499039\n",
      "Iteration 116, loss = 0.59426819\n",
      "Iteration 117, loss = 0.59355114\n",
      "Iteration 118, loss = 0.59283789\n",
      "Iteration 119, loss = 0.59211264\n",
      "Iteration 120, loss = 0.59139252\n",
      "Iteration 121, loss = 0.59067749\n",
      "Iteration 122, loss = 0.58996155\n",
      "Iteration 123, loss = 0.58924700\n",
      "Iteration 124, loss = 0.58852793\n",
      "Iteration 125, loss = 0.58781184\n",
      "Iteration 126, loss = 0.58709902\n",
      "Iteration 127, loss = 0.58638063\n",
      "Iteration 128, loss = 0.58566845\n",
      "Iteration 129, loss = 0.58495032\n",
      "Iteration 130, loss = 0.58423916\n",
      "Iteration 131, loss = 0.58352745\n",
      "Iteration 132, loss = 0.58281458\n",
      "Iteration 133, loss = 0.58210321\n",
      "Iteration 134, loss = 0.58139079\n",
      "Iteration 135, loss = 0.58068531\n",
      "Iteration 136, loss = 0.57997528\n",
      "Iteration 137, loss = 0.57926315\n",
      "Iteration 138, loss = 0.57855772\n",
      "Iteration 139, loss = 0.57784863\n",
      "Iteration 140, loss = 0.57714566\n",
      "Iteration 141, loss = 0.57644352\n",
      "Iteration 142, loss = 0.57574224\n",
      "Iteration 143, loss = 0.57504771\n",
      "Iteration 144, loss = 0.57435220\n",
      "Iteration 145, loss = 0.57366005\n",
      "Iteration 146, loss = 0.57296326\n",
      "Iteration 147, loss = 0.57227620\n",
      "Iteration 148, loss = 0.57158355\n",
      "Iteration 149, loss = 0.57089410\n",
      "Iteration 150, loss = 0.57020908\n",
      "Iteration 151, loss = 0.56952527\n",
      "Iteration 152, loss = 0.56884286\n",
      "Iteration 153, loss = 0.56816659\n",
      "Iteration 154, loss = 0.56748720\n",
      "Iteration 155, loss = 0.56681229\n",
      "Iteration 156, loss = 0.56614363\n",
      "Iteration 157, loss = 0.56546989\n",
      "Iteration 158, loss = 0.56479527\n",
      "Iteration 159, loss = 0.56412566\n",
      "Iteration 160, loss = 0.56345624\n",
      "Iteration 161, loss = 0.56278861\n",
      "Iteration 162, loss = 0.56213014\n",
      "Iteration 163, loss = 0.56145869\n",
      "Iteration 164, loss = 0.56079755\n",
      "Iteration 165, loss = 0.56013646\n",
      "Iteration 166, loss = 0.55948137\n",
      "Iteration 167, loss = 0.55882659\n",
      "Iteration 168, loss = 0.55816906\n",
      "Iteration 169, loss = 0.55751712\n",
      "Iteration 170, loss = 0.55687171\n",
      "Iteration 171, loss = 0.55622346\n",
      "Iteration 172, loss = 0.55558945\n",
      "Iteration 173, loss = 0.55494105\n",
      "Iteration 174, loss = 0.55430429\n",
      "Iteration 175, loss = 0.55367685\n",
      "Iteration 176, loss = 0.55303945\n",
      "Iteration 177, loss = 0.55241128\n",
      "Iteration 178, loss = 0.55178750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 179, loss = 0.55116253\n",
      "Iteration 180, loss = 0.55055014\n",
      "Iteration 181, loss = 0.54992639\n",
      "Iteration 182, loss = 0.54931136\n",
      "Iteration 183, loss = 0.54870504\n",
      "Iteration 184, loss = 0.54809444\n",
      "Iteration 185, loss = 0.54748998\n",
      "Iteration 186, loss = 0.54689412\n",
      "Iteration 187, loss = 0.54629303\n",
      "Iteration 188, loss = 0.54570127\n",
      "Iteration 189, loss = 0.54510782\n",
      "Iteration 190, loss = 0.54451478\n",
      "Iteration 191, loss = 0.54393422\n",
      "Iteration 192, loss = 0.54334854\n",
      "Iteration 193, loss = 0.54276277\n",
      "Iteration 194, loss = 0.54218578\n",
      "Iteration 195, loss = 0.54160653\n",
      "Iteration 196, loss = 0.54104123\n",
      "Iteration 197, loss = 0.54046533\n",
      "Iteration 198, loss = 0.53989746\n",
      "Iteration 199, loss = 0.53933399\n",
      "Iteration 200, loss = 0.53876776\n",
      "Iteration 201, loss = 0.53820892\n",
      "Iteration 202, loss = 0.53765074\n",
      "Iteration 203, loss = 0.53709818\n",
      "Iteration 204, loss = 0.53654478\n",
      "Iteration 205, loss = 0.53599790\n",
      "Iteration 206, loss = 0.53544798\n",
      "Iteration 207, loss = 0.53490771\n",
      "Iteration 208, loss = 0.53437672\n",
      "Iteration 209, loss = 0.53383676\n",
      "Iteration 210, loss = 0.53331939\n",
      "Iteration 211, loss = 0.53278503\n",
      "Iteration 212, loss = 0.53226180\n",
      "Iteration 213, loss = 0.53174469\n",
      "Iteration 214, loss = 0.53122723\n",
      "Iteration 215, loss = 0.53070835\n",
      "Iteration 216, loss = 0.53020167\n",
      "Iteration 217, loss = 0.52969300\n",
      "Iteration 218, loss = 0.52918558\n",
      "Iteration 219, loss = 0.52868260\n",
      "Iteration 220, loss = 0.52817944\n",
      "Iteration 221, loss = 0.52768463\n",
      "Iteration 222, loss = 0.52719337\n",
      "Iteration 223, loss = 0.52669555\n",
      "Iteration 224, loss = 0.52620148\n",
      "Iteration 225, loss = 0.52572361\n",
      "Iteration 226, loss = 0.52523665\n",
      "Iteration 227, loss = 0.52476122\n",
      "Iteration 228, loss = 0.52428691\n",
      "Iteration 229, loss = 0.52381515\n",
      "Iteration 230, loss = 0.52334709\n",
      "Iteration 231, loss = 0.52288180\n",
      "Iteration 232, loss = 0.52241411\n",
      "Iteration 233, loss = 0.52195845\n",
      "Iteration 234, loss = 0.52150085\n",
      "Iteration 235, loss = 0.52104345\n",
      "Iteration 236, loss = 0.52059243\n",
      "Iteration 237, loss = 0.52015832\n",
      "Iteration 238, loss = 0.51970351\n",
      "Iteration 239, loss = 0.51926212\n",
      "Iteration 240, loss = 0.51883342\n",
      "Iteration 241, loss = 0.51839279\n",
      "Iteration 242, loss = 0.51796643\n",
      "Iteration 243, loss = 0.51754095\n",
      "Iteration 244, loss = 0.51711240\n",
      "Iteration 245, loss = 0.51668921\n",
      "Iteration 246, loss = 0.51626792\n",
      "Iteration 247, loss = 0.51585227\n",
      "Iteration 248, loss = 0.51543522\n",
      "Iteration 249, loss = 0.51503045\n",
      "Iteration 250, loss = 0.51461754\n",
      "Iteration 251, loss = 0.51420833\n",
      "Iteration 252, loss = 0.51380468\n",
      "Iteration 253, loss = 0.51340642\n",
      "Iteration 254, loss = 0.51300844\n",
      "Iteration 255, loss = 0.51261705\n",
      "Iteration 256, loss = 0.51222574\n",
      "Iteration 257, loss = 0.51183268\n",
      "Iteration 258, loss = 0.51145058\n",
      "Iteration 259, loss = 0.51107199\n",
      "Iteration 260, loss = 0.51069059\n",
      "Iteration 261, loss = 0.51030952\n",
      "Iteration 262, loss = 0.50993891\n",
      "Iteration 263, loss = 0.50957514\n",
      "Iteration 264, loss = 0.50921184\n",
      "Iteration 265, loss = 0.50883558\n",
      "Iteration 266, loss = 0.50847335\n",
      "Iteration 267, loss = 0.50812036\n",
      "Iteration 268, loss = 0.50776473\n",
      "Iteration 269, loss = 0.50741071\n",
      "Iteration 270, loss = 0.50705938\n",
      "Iteration 271, loss = 0.50671571\n",
      "Iteration 272, loss = 0.50637577\n",
      "Iteration 273, loss = 0.50603738\n",
      "Iteration 274, loss = 0.50569665\n",
      "Iteration 275, loss = 0.50536167\n",
      "Iteration 276, loss = 0.50503328\n",
      "Iteration 277, loss = 0.50470286\n",
      "Iteration 278, loss = 0.50437855\n",
      "Iteration 279, loss = 0.50405317\n",
      "Iteration 280, loss = 0.50373017\n",
      "Iteration 281, loss = 0.50341124\n",
      "Iteration 282, loss = 0.50310243\n",
      "Iteration 283, loss = 0.50278294\n",
      "Iteration 284, loss = 0.50247672\n",
      "Iteration 285, loss = 0.50216439\n",
      "Iteration 286, loss = 0.50186077\n",
      "Iteration 287, loss = 0.50155720\n",
      "Iteration 288, loss = 0.50125684\n",
      "Iteration 289, loss = 0.50096087\n",
      "Iteration 290, loss = 0.50067119\n",
      "Iteration 291, loss = 0.50037372\n",
      "Iteration 292, loss = 0.50008361\n",
      "Iteration 293, loss = 0.49979897\n",
      "Iteration 294, loss = 0.49951314\n",
      "Iteration 295, loss = 0.49923102\n",
      "Iteration 296, loss = 0.49895611\n",
      "Iteration 297, loss = 0.49868081\n",
      "Iteration 298, loss = 0.49840216\n",
      "Iteration 299, loss = 0.49813375\n",
      "Iteration 300, loss = 0.49786897\n",
      "Iteration 301, loss = 0.49759939\n",
      "Iteration 302, loss = 0.49733745\n",
      "Iteration 303, loss = 0.49707822\n",
      "Iteration 304, loss = 0.49682086\n",
      "Iteration 305, loss = 0.49656651\n",
      "Iteration 306, loss = 0.49631311\n",
      "Iteration 307, loss = 0.49606152\n",
      "Iteration 308, loss = 0.49580992\n",
      "Iteration 309, loss = 0.49556607\n",
      "Iteration 310, loss = 0.49531572\n",
      "Iteration 311, loss = 0.49507746\n",
      "Iteration 312, loss = 0.49483458\n",
      "Iteration 313, loss = 0.49460639\n",
      "Iteration 314, loss = 0.49435958\n",
      "Iteration 315, loss = 0.49412875\n",
      "Iteration 316, loss = 0.49389812\n",
      "Iteration 317, loss = 0.49367449\n",
      "Iteration 318, loss = 0.49344930\n",
      "Iteration 319, loss = 0.49321940\n",
      "Iteration 320, loss = 0.49300522\n",
      "Iteration 321, loss = 0.49277732\n",
      "Iteration 322, loss = 0.49256730\n",
      "Iteration 323, loss = 0.49234769\n",
      "Iteration 324, loss = 0.49213322\n",
      "Iteration 325, loss = 0.49192036\n",
      "Iteration 326, loss = 0.49171392\n",
      "Iteration 327, loss = 0.49150487\n",
      "Iteration 328, loss = 0.49129974\n",
      "Iteration 329, loss = 0.49109621\n",
      "Iteration 330, loss = 0.49089473\n",
      "Iteration 331, loss = 0.49069790\n",
      "Iteration 332, loss = 0.49050144\n",
      "Iteration 333, loss = 0.49030833\n",
      "Iteration 334, loss = 0.49011743\n",
      "Iteration 335, loss = 0.48992145\n",
      "Iteration 336, loss = 0.48972870\n",
      "Iteration 337, loss = 0.48954342\n",
      "Iteration 338, loss = 0.48936492\n",
      "Iteration 339, loss = 0.48918151\n",
      "Iteration 340, loss = 0.48900368\n",
      "Iteration 341, loss = 0.48882383\n",
      "Iteration 342, loss = 0.48865240\n",
      "Iteration 343, loss = 0.48847799\n",
      "Iteration 344, loss = 0.48830810\n",
      "Iteration 345, loss = 0.48814308\n",
      "Iteration 346, loss = 0.48797590\n",
      "Iteration 347, loss = 0.48780569\n",
      "Iteration 348, loss = 0.48764580\n",
      "Iteration 349, loss = 0.48748100\n",
      "Iteration 350, loss = 0.48732652\n",
      "Iteration 351, loss = 0.48716612\n",
      "Iteration 352, loss = 0.48701157\n",
      "Iteration 353, loss = 0.48685599\n",
      "Iteration 354, loss = 0.48669953\n",
      "Iteration 355, loss = 0.48655144\n",
      "Iteration 356, loss = 0.48640088\n",
      "Iteration 357, loss = 0.48626015\n",
      "Iteration 358, loss = 0.48611809\n",
      "Iteration 359, loss = 0.48596660\n",
      "Iteration 360, loss = 0.48582222\n",
      "Iteration 361, loss = 0.48568412\n",
      "Iteration 362, loss = 0.48554158\n",
      "Iteration 363, loss = 0.48541065\n",
      "Iteration 364, loss = 0.48527425\n",
      "Iteration 365, loss = 0.48513626\n",
      "Iteration 366, loss = 0.48500399\n",
      "Iteration 367, loss = 0.48487655\n",
      "Iteration 368, loss = 0.48474650\n",
      "Iteration 369, loss = 0.48461862\n",
      "Iteration 370, loss = 0.48449547\n",
      "Iteration 371, loss = 0.48437294\n",
      "Iteration 372, loss = 0.48424351\n",
      "Iteration 373, loss = 0.48412207\n",
      "Iteration 374, loss = 0.48399971\n",
      "Iteration 375, loss = 0.48387863\n",
      "Iteration 376, loss = 0.48376468\n",
      "Iteration 377, loss = 0.48364256\n",
      "Iteration 378, loss = 0.48352953\n",
      "Iteration 379, loss = 0.48342294\n",
      "Iteration 380, loss = 0.48330706\n",
      "Iteration 381, loss = 0.48319574\n",
      "Iteration 382, loss = 0.48308530\n",
      "Iteration 383, loss = 0.48297759\n",
      "Iteration 384, loss = 0.48286701\n",
      "Iteration 385, loss = 0.48276495\n",
      "Iteration 386, loss = 0.48265891\n",
      "Iteration 387, loss = 0.48255838\n",
      "Iteration 388, loss = 0.48246318\n",
      "Iteration 389, loss = 0.48236054\n",
      "Iteration 390, loss = 0.48225960\n",
      "Iteration 391, loss = 0.48216375\n",
      "Iteration 392, loss = 0.48207390\n",
      "Iteration 393, loss = 0.48197281\n",
      "Iteration 394, loss = 0.48188605\n",
      "Iteration 395, loss = 0.48179084\n",
      "Iteration 396, loss = 0.48170218\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75183332\n",
      "Iteration 2, loss = 0.74647427\n",
      "Iteration 3, loss = 0.74130417\n",
      "Iteration 4, loss = 0.73632018\n",
      "Iteration 5, loss = 0.73152883\n",
      "Iteration 6, loss = 0.72693818\n",
      "Iteration 7, loss = 0.72253134\n",
      "Iteration 8, loss = 0.71833170\n",
      "Iteration 9, loss = 0.71428638\n",
      "Iteration 10, loss = 0.71042193\n",
      "Iteration 11, loss = 0.70673604\n",
      "Iteration 12, loss = 0.70322894\n",
      "Iteration 13, loss = 0.69989403\n",
      "Iteration 14, loss = 0.69671323\n",
      "Iteration 15, loss = 0.69369340\n",
      "Iteration 16, loss = 0.69083043\n",
      "Iteration 17, loss = 0.68809387\n",
      "Iteration 18, loss = 0.68552336\n",
      "Iteration 19, loss = 0.68309304\n",
      "Iteration 20, loss = 0.68079645\n",
      "Iteration 21, loss = 0.67863825\n",
      "Iteration 22, loss = 0.67659896\n",
      "Iteration 23, loss = 0.67467895\n",
      "Iteration 24, loss = 0.67287329\n",
      "Iteration 25, loss = 0.67117362\n",
      "Iteration 26, loss = 0.66955979\n",
      "Iteration 27, loss = 0.66804612\n",
      "Iteration 28, loss = 0.66663624\n",
      "Iteration 29, loss = 0.66529632\n",
      "Iteration 30, loss = 0.66403773\n",
      "Iteration 31, loss = 0.66285034\n",
      "Iteration 32, loss = 0.66172442\n",
      "Iteration 33, loss = 0.66065887\n",
      "Iteration 34, loss = 0.65962825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.65865603\n",
      "Iteration 36, loss = 0.65772561\n",
      "Iteration 37, loss = 0.65682472\n",
      "Iteration 38, loss = 0.65596419\n",
      "Iteration 39, loss = 0.65512805\n",
      "Iteration 40, loss = 0.65432143\n",
      "Iteration 41, loss = 0.65352863\n",
      "Iteration 42, loss = 0.65275208\n",
      "Iteration 43, loss = 0.65198973\n",
      "Iteration 44, loss = 0.65123931\n",
      "Iteration 45, loss = 0.65049605\n",
      "Iteration 46, loss = 0.64975788\n",
      "Iteration 47, loss = 0.64902612\n",
      "Iteration 48, loss = 0.64830183\n",
      "Iteration 49, loss = 0.64757438\n",
      "Iteration 50, loss = 0.64685266\n",
      "Iteration 51, loss = 0.64612714\n",
      "Iteration 52, loss = 0.64540145\n",
      "Iteration 53, loss = 0.64467146\n",
      "Iteration 54, loss = 0.64394294\n",
      "Iteration 55, loss = 0.64320876\n",
      "Iteration 56, loss = 0.64247140\n",
      "Iteration 57, loss = 0.64173003\n",
      "Iteration 58, loss = 0.64098724\n",
      "Iteration 59, loss = 0.64023672\n",
      "Iteration 60, loss = 0.63948537\n",
      "Iteration 61, loss = 0.63873008\n",
      "Iteration 62, loss = 0.63796703\n",
      "Iteration 63, loss = 0.63720170\n",
      "Iteration 64, loss = 0.63643144\n",
      "Iteration 65, loss = 0.63565950\n",
      "Iteration 66, loss = 0.63488092\n",
      "Iteration 67, loss = 0.63409819\n",
      "Iteration 68, loss = 0.63330808\n",
      "Iteration 69, loss = 0.63251933\n",
      "Iteration 70, loss = 0.63172415\n",
      "Iteration 71, loss = 0.63092692\n",
      "Iteration 72, loss = 0.63012567\n",
      "Iteration 73, loss = 0.62931896\n",
      "Iteration 74, loss = 0.62851389\n",
      "Iteration 75, loss = 0.62769794\n",
      "Iteration 76, loss = 0.62688339\n",
      "Iteration 77, loss = 0.62606335\n",
      "Iteration 78, loss = 0.62524169\n",
      "Iteration 79, loss = 0.62441373\n",
      "Iteration 80, loss = 0.62358884\n",
      "Iteration 81, loss = 0.62276011\n",
      "Iteration 82, loss = 0.62192936\n",
      "Iteration 83, loss = 0.62110037\n",
      "Iteration 84, loss = 0.62027632\n",
      "Iteration 85, loss = 0.61943532\n",
      "Iteration 86, loss = 0.61860071\n",
      "Iteration 87, loss = 0.61776292\n",
      "Iteration 88, loss = 0.61692605\n",
      "Iteration 89, loss = 0.61608284\n",
      "Iteration 90, loss = 0.61524206\n",
      "Iteration 91, loss = 0.61440086\n",
      "Iteration 92, loss = 0.61356279\n",
      "Iteration 93, loss = 0.61271193\n",
      "Iteration 94, loss = 0.61186721\n",
      "Iteration 95, loss = 0.61102038\n",
      "Iteration 96, loss = 0.61017664\n",
      "Iteration 97, loss = 0.60932796\n",
      "Iteration 98, loss = 0.60847975\n",
      "Iteration 99, loss = 0.60763417\n",
      "Iteration 100, loss = 0.60679088\n",
      "Iteration 101, loss = 0.60594931\n",
      "Iteration 102, loss = 0.60509991\n",
      "Iteration 103, loss = 0.60425409\n",
      "Iteration 104, loss = 0.60340560\n",
      "Iteration 105, loss = 0.60255964\n",
      "Iteration 106, loss = 0.60171183\n",
      "Iteration 107, loss = 0.60086529\n",
      "Iteration 108, loss = 0.60001827\n",
      "Iteration 109, loss = 0.59916734\n",
      "Iteration 110, loss = 0.59831705\n",
      "Iteration 111, loss = 0.59746614\n",
      "Iteration 112, loss = 0.59661559\n",
      "Iteration 113, loss = 0.59576319\n",
      "Iteration 114, loss = 0.59491003\n",
      "Iteration 115, loss = 0.59405934\n",
      "Iteration 116, loss = 0.59320802\n",
      "Iteration 117, loss = 0.59235818\n",
      "Iteration 118, loss = 0.59150777\n",
      "Iteration 119, loss = 0.59065278\n",
      "Iteration 120, loss = 0.58979934\n",
      "Iteration 121, loss = 0.58894942\n",
      "Iteration 122, loss = 0.58810227\n",
      "Iteration 123, loss = 0.58725111\n",
      "Iteration 124, loss = 0.58639620\n",
      "Iteration 125, loss = 0.58554458\n",
      "Iteration 126, loss = 0.58469419\n",
      "Iteration 127, loss = 0.58383982\n",
      "Iteration 128, loss = 0.58298811\n",
      "Iteration 129, loss = 0.58213178\n",
      "Iteration 130, loss = 0.58128285\n",
      "Iteration 131, loss = 0.58042780\n",
      "Iteration 132, loss = 0.57957238\n",
      "Iteration 133, loss = 0.57872347\n",
      "Iteration 134, loss = 0.57787065\n",
      "Iteration 135, loss = 0.57702091\n",
      "Iteration 136, loss = 0.57617455\n",
      "Iteration 137, loss = 0.57532603\n",
      "Iteration 138, loss = 0.57448073\n",
      "Iteration 139, loss = 0.57363371\n",
      "Iteration 140, loss = 0.57278940\n",
      "Iteration 141, loss = 0.57195202\n",
      "Iteration 142, loss = 0.57111323\n",
      "Iteration 143, loss = 0.57028135\n",
      "Iteration 144, loss = 0.56944513\n",
      "Iteration 145, loss = 0.56861600\n",
      "Iteration 146, loss = 0.56778190\n",
      "Iteration 147, loss = 0.56696124\n",
      "Iteration 148, loss = 0.56613461\n",
      "Iteration 149, loss = 0.56530611\n",
      "Iteration 150, loss = 0.56448450\n",
      "Iteration 151, loss = 0.56366345\n",
      "Iteration 152, loss = 0.56284586\n",
      "Iteration 153, loss = 0.56203740\n",
      "Iteration 154, loss = 0.56122036\n",
      "Iteration 155, loss = 0.56041113\n",
      "Iteration 156, loss = 0.55960960\n",
      "Iteration 157, loss = 0.55879740\n",
      "Iteration 158, loss = 0.55799044\n",
      "Iteration 159, loss = 0.55718628\n",
      "Iteration 160, loss = 0.55638361\n",
      "Iteration 161, loss = 0.55558396\n",
      "Iteration 162, loss = 0.55478999\n",
      "Iteration 163, loss = 0.55398673\n",
      "Iteration 164, loss = 0.55319429\n",
      "Iteration 165, loss = 0.55240300\n",
      "Iteration 166, loss = 0.55161709\n",
      "Iteration 167, loss = 0.55083433\n",
      "Iteration 168, loss = 0.55004052\n",
      "Iteration 169, loss = 0.54926037\n",
      "Iteration 170, loss = 0.54848565\n",
      "Iteration 171, loss = 0.54770739\n",
      "Iteration 172, loss = 0.54694193\n",
      "Iteration 173, loss = 0.54616345\n",
      "Iteration 174, loss = 0.54539650\n",
      "Iteration 175, loss = 0.54463008\n",
      "Iteration 176, loss = 0.54386773\n",
      "Iteration 177, loss = 0.54310717\n",
      "Iteration 178, loss = 0.54234787\n",
      "Iteration 179, loss = 0.54159193\n",
      "Iteration 180, loss = 0.54084318\n",
      "Iteration 181, loss = 0.54008360\n",
      "Iteration 182, loss = 0.53933667\n",
      "Iteration 183, loss = 0.53859534\n",
      "Iteration 184, loss = 0.53785537\n",
      "Iteration 185, loss = 0.53711082\n",
      "Iteration 186, loss = 0.53638469\n",
      "Iteration 187, loss = 0.53564099\n",
      "Iteration 188, loss = 0.53491856\n",
      "Iteration 189, loss = 0.53419257\n",
      "Iteration 190, loss = 0.53346749\n",
      "Iteration 191, loss = 0.53274802\n",
      "Iteration 192, loss = 0.53203438\n",
      "Iteration 193, loss = 0.53131578\n",
      "Iteration 194, loss = 0.53060853\n",
      "Iteration 195, loss = 0.52989825\n",
      "Iteration 196, loss = 0.52920120\n",
      "Iteration 197, loss = 0.52849304\n",
      "Iteration 198, loss = 0.52779651\n",
      "Iteration 199, loss = 0.52710705\n",
      "Iteration 200, loss = 0.52641216\n",
      "Iteration 201, loss = 0.52572678\n",
      "Iteration 202, loss = 0.52504428\n",
      "Iteration 203, loss = 0.52436292\n",
      "Iteration 204, loss = 0.52368350\n",
      "Iteration 205, loss = 0.52300912\n",
      "Iteration 206, loss = 0.52233240\n",
      "Iteration 207, loss = 0.52166430\n",
      "Iteration 208, loss = 0.52100561\n",
      "Iteration 209, loss = 0.52034208\n",
      "Iteration 210, loss = 0.51969670\n",
      "Iteration 211, loss = 0.51903690\n",
      "Iteration 212, loss = 0.51839272\n",
      "Iteration 213, loss = 0.51775575\n",
      "Iteration 214, loss = 0.51711261\n",
      "Iteration 215, loss = 0.51646963\n",
      "Iteration 216, loss = 0.51584059\n",
      "Iteration 217, loss = 0.51520870\n",
      "Iteration 218, loss = 0.51458279\n",
      "Iteration 219, loss = 0.51396075\n",
      "Iteration 220, loss = 0.51333643\n",
      "Iteration 221, loss = 0.51271850\n",
      "Iteration 222, loss = 0.51210762\n",
      "Iteration 223, loss = 0.51149376\n",
      "Iteration 224, loss = 0.51088329\n",
      "Iteration 225, loss = 0.51028755\n",
      "Iteration 226, loss = 0.50968144\n",
      "Iteration 227, loss = 0.50908941\n",
      "Iteration 228, loss = 0.50849533\n",
      "Iteration 229, loss = 0.50790564\n",
      "Iteration 230, loss = 0.50731652\n",
      "Iteration 231, loss = 0.50673707\n",
      "Iteration 232, loss = 0.50615010\n",
      "Iteration 233, loss = 0.50557674\n",
      "Iteration 234, loss = 0.50500595\n",
      "Iteration 235, loss = 0.50443186\n",
      "Iteration 236, loss = 0.50386493\n",
      "Iteration 237, loss = 0.50331715\n",
      "Iteration 238, loss = 0.50274960\n",
      "Iteration 239, loss = 0.50219378\n",
      "Iteration 240, loss = 0.50164824\n",
      "Iteration 241, loss = 0.50109653\n",
      "Iteration 242, loss = 0.50055414\n",
      "Iteration 243, loss = 0.50001628\n",
      "Iteration 244, loss = 0.49948334\n",
      "Iteration 245, loss = 0.49894274\n",
      "Iteration 246, loss = 0.49841515\n",
      "Iteration 247, loss = 0.49789166\n",
      "Iteration 248, loss = 0.49736986\n",
      "Iteration 249, loss = 0.49685501\n",
      "Iteration 250, loss = 0.49633833\n",
      "Iteration 251, loss = 0.49582620\n",
      "Iteration 252, loss = 0.49531756\n",
      "Iteration 253, loss = 0.49481571\n",
      "Iteration 254, loss = 0.49431381\n",
      "Iteration 255, loss = 0.49381668\n",
      "Iteration 256, loss = 0.49332333\n",
      "Iteration 257, loss = 0.49283522\n",
      "Iteration 258, loss = 0.49234993\n",
      "Iteration 259, loss = 0.49186669\n",
      "Iteration 260, loss = 0.49138636\n",
      "Iteration 261, loss = 0.49090545\n",
      "Iteration 262, loss = 0.49042887\n",
      "Iteration 263, loss = 0.48996100\n",
      "Iteration 264, loss = 0.48949985\n",
      "Iteration 265, loss = 0.48902500\n",
      "Iteration 266, loss = 0.48856346\n",
      "Iteration 267, loss = 0.48810758\n",
      "Iteration 268, loss = 0.48765581\n",
      "Iteration 269, loss = 0.48720090\n",
      "Iteration 270, loss = 0.48675223\n",
      "Iteration 271, loss = 0.48631342\n",
      "Iteration 272, loss = 0.48587211\n",
      "Iteration 273, loss = 0.48543413\n",
      "Iteration 274, loss = 0.48499381\n",
      "Iteration 275, loss = 0.48456240\n",
      "Iteration 276, loss = 0.48413766\n",
      "Iteration 277, loss = 0.48370807\n",
      "Iteration 278, loss = 0.48328699\n",
      "Iteration 279, loss = 0.48286808\n",
      "Iteration 280, loss = 0.48244649\n",
      "Iteration 281, loss = 0.48203399\n",
      "Iteration 282, loss = 0.48162938\n",
      "Iteration 283, loss = 0.48121673\n",
      "Iteration 284, loss = 0.48081452\n",
      "Iteration 285, loss = 0.48041258\n",
      "Iteration 286, loss = 0.48001217\n",
      "Iteration 287, loss = 0.47961770\n",
      "Iteration 288, loss = 0.47922203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 289, loss = 0.47883554\n",
      "Iteration 290, loss = 0.47845133\n",
      "Iteration 291, loss = 0.47806007\n",
      "Iteration 292, loss = 0.47768318\n",
      "Iteration 293, loss = 0.47730087\n",
      "Iteration 294, loss = 0.47692546\n",
      "Iteration 295, loss = 0.47655391\n",
      "Iteration 296, loss = 0.47618703\n",
      "Iteration 297, loss = 0.47582677\n",
      "Iteration 298, loss = 0.47545574\n",
      "Iteration 299, loss = 0.47510236\n",
      "Iteration 300, loss = 0.47474542\n",
      "Iteration 301, loss = 0.47438943\n",
      "Iteration 302, loss = 0.47404006\n",
      "Iteration 303, loss = 0.47369176\n",
      "Iteration 304, loss = 0.47334547\n",
      "Iteration 305, loss = 0.47300944\n",
      "Iteration 306, loss = 0.47266553\n",
      "Iteration 307, loss = 0.47233076\n",
      "Iteration 308, loss = 0.47199634\n",
      "Iteration 309, loss = 0.47166440\n",
      "Iteration 310, loss = 0.47133381\n",
      "Iteration 311, loss = 0.47101355\n",
      "Iteration 312, loss = 0.47068985\n",
      "Iteration 313, loss = 0.47037712\n",
      "Iteration 314, loss = 0.47005634\n",
      "Iteration 315, loss = 0.46974395\n",
      "Iteration 316, loss = 0.46943426\n",
      "Iteration 317, loss = 0.46912700\n",
      "Iteration 318, loss = 0.46882255\n",
      "Iteration 319, loss = 0.46851591\n",
      "Iteration 320, loss = 0.46822154\n",
      "Iteration 321, loss = 0.46791980\n",
      "Iteration 322, loss = 0.46763444\n",
      "Iteration 323, loss = 0.46733232\n",
      "Iteration 324, loss = 0.46704303\n",
      "Iteration 325, loss = 0.46675562\n",
      "Iteration 326, loss = 0.46647068\n",
      "Iteration 327, loss = 0.46618722\n",
      "Iteration 328, loss = 0.46590376\n",
      "Iteration 329, loss = 0.46562543\n",
      "Iteration 330, loss = 0.46534839\n",
      "Iteration 331, loss = 0.46508447\n",
      "Iteration 332, loss = 0.46480791\n",
      "Iteration 333, loss = 0.46454187\n",
      "Iteration 334, loss = 0.46427416\n",
      "Iteration 335, loss = 0.46400944\n",
      "Iteration 336, loss = 0.46375045\n",
      "Iteration 337, loss = 0.46348840\n",
      "Iteration 338, loss = 0.46323893\n",
      "Iteration 339, loss = 0.46298419\n",
      "Iteration 340, loss = 0.46273624\n",
      "Iteration 341, loss = 0.46248802\n",
      "Iteration 342, loss = 0.46224135\n",
      "Iteration 343, loss = 0.46200043\n",
      "Iteration 344, loss = 0.46176178\n",
      "Iteration 345, loss = 0.46152404\n",
      "Iteration 346, loss = 0.46129508\n",
      "Iteration 347, loss = 0.46105544\n",
      "Iteration 348, loss = 0.46082409\n",
      "Iteration 349, loss = 0.46059832\n",
      "Iteration 350, loss = 0.46037652\n",
      "Iteration 351, loss = 0.46015042\n",
      "Iteration 352, loss = 0.45993402\n",
      "Iteration 353, loss = 0.45971001\n",
      "Iteration 354, loss = 0.45948844\n",
      "Iteration 355, loss = 0.45927484\n",
      "Iteration 356, loss = 0.45906468\n",
      "Iteration 357, loss = 0.45886023\n",
      "Iteration 358, loss = 0.45865176\n",
      "Iteration 359, loss = 0.45843726\n",
      "Iteration 360, loss = 0.45822957\n",
      "Iteration 361, loss = 0.45803303\n",
      "Iteration 362, loss = 0.45782465\n",
      "Iteration 363, loss = 0.45763197\n",
      "Iteration 364, loss = 0.45743607\n",
      "Iteration 365, loss = 0.45724087\n",
      "Iteration 366, loss = 0.45704450\n",
      "Iteration 367, loss = 0.45685235\n",
      "Iteration 368, loss = 0.45666321\n",
      "Iteration 369, loss = 0.45647736\n",
      "Iteration 370, loss = 0.45629284\n",
      "Iteration 371, loss = 0.45611193\n",
      "Iteration 372, loss = 0.45592270\n",
      "Iteration 373, loss = 0.45574747\n",
      "Iteration 374, loss = 0.45556068\n",
      "Iteration 375, loss = 0.45538732\n",
      "Iteration 376, loss = 0.45521247\n",
      "Iteration 377, loss = 0.45503439\n",
      "Iteration 378, loss = 0.45486498\n",
      "Iteration 379, loss = 0.45470187\n",
      "Iteration 380, loss = 0.45452671\n",
      "Iteration 381, loss = 0.45436487\n",
      "Iteration 382, loss = 0.45419968\n",
      "Iteration 383, loss = 0.45403678\n",
      "Iteration 384, loss = 0.45387246\n",
      "Iteration 385, loss = 0.45371905\n",
      "Iteration 386, loss = 0.45355958\n",
      "Iteration 387, loss = 0.45340389\n",
      "Iteration 388, loss = 0.45325655\n",
      "Iteration 389, loss = 0.45310922\n",
      "Iteration 390, loss = 0.45295019\n",
      "Iteration 391, loss = 0.45280996\n",
      "Iteration 392, loss = 0.45266450\n",
      "Iteration 393, loss = 0.45251517\n",
      "Iteration 394, loss = 0.45237825\n",
      "Iteration 395, loss = 0.45222733\n",
      "Iteration 396, loss = 0.45209320\n",
      "Iteration 397, loss = 0.45195198\n",
      "Iteration 398, loss = 0.45182565\n",
      "Iteration 399, loss = 0.45168094\n",
      "Iteration 400, loss = 0.45154852\n",
      "Iteration 401, loss = 0.45141382\n",
      "Iteration 402, loss = 0.45128647\n",
      "Iteration 403, loss = 0.45115983\n",
      "Iteration 404, loss = 0.45103601\n",
      "Iteration 405, loss = 0.45091453\n",
      "Iteration 406, loss = 0.45078462\n",
      "Iteration 407, loss = 0.45065791\n",
      "Iteration 408, loss = 0.45053957\n",
      "Iteration 409, loss = 0.45042279\n",
      "Iteration 410, loss = 0.45030138\n",
      "Iteration 411, loss = 0.45019058\n",
      "Iteration 412, loss = 0.45007240\n",
      "Iteration 413, loss = 0.44996213\n",
      "Iteration 414, loss = 0.44985626\n",
      "Iteration 415, loss = 0.44973413\n",
      "Iteration 416, loss = 0.44962759\n",
      "Iteration 417, loss = 0.44952221\n",
      "Iteration 418, loss = 0.44941821\n",
      "Iteration 419, loss = 0.44931861\n",
      "Iteration 420, loss = 0.44921341\n",
      "Iteration 421, loss = 0.44911617\n",
      "Iteration 422, loss = 0.44900719\n",
      "Iteration 423, loss = 0.44892229\n",
      "Iteration 424, loss = 0.44881860\n",
      "Iteration 425, loss = 0.44872387\n",
      "Iteration 426, loss = 0.44862508\n",
      "Iteration 427, loss = 0.44852376\n",
      "Iteration 428, loss = 0.44843683\n",
      "Iteration 429, loss = 0.44833964\n",
      "Iteration 430, loss = 0.44825147\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72572252\n",
      "Iteration 2, loss = 0.72176638\n",
      "Iteration 3, loss = 0.71797566\n",
      "Iteration 4, loss = 0.71436096\n",
      "Iteration 5, loss = 0.71093412\n",
      "Iteration 6, loss = 0.70769946\n",
      "Iteration 7, loss = 0.70462955\n",
      "Iteration 8, loss = 0.70174727\n",
      "Iteration 9, loss = 0.69900577\n",
      "Iteration 10, loss = 0.69643044\n",
      "Iteration 11, loss = 0.69399827\n",
      "Iteration 12, loss = 0.69172726\n",
      "Iteration 13, loss = 0.68959952\n",
      "Iteration 14, loss = 0.68759502\n",
      "Iteration 15, loss = 0.68571642\n",
      "Iteration 16, loss = 0.68395906\n",
      "Iteration 17, loss = 0.68229124\n",
      "Iteration 18, loss = 0.68074275\n",
      "Iteration 19, loss = 0.67929208\n",
      "Iteration 20, loss = 0.67792716\n",
      "Iteration 21, loss = 0.67665279\n",
      "Iteration 22, loss = 0.67544577\n",
      "Iteration 23, loss = 0.67430676\n",
      "Iteration 24, loss = 0.67323353\n",
      "Iteration 25, loss = 0.67222050\n",
      "Iteration 26, loss = 0.67124276\n",
      "Iteration 27, loss = 0.67031191\n",
      "Iteration 28, loss = 0.66943339\n",
      "Iteration 29, loss = 0.66858545\n",
      "Iteration 30, loss = 0.66776417\n",
      "Iteration 31, loss = 0.66696962\n",
      "Iteration 32, loss = 0.66619818\n",
      "Iteration 33, loss = 0.66544185\n",
      "Iteration 34, loss = 0.66469394\n",
      "Iteration 35, loss = 0.66396321\n",
      "Iteration 36, loss = 0.66324676\n",
      "Iteration 37, loss = 0.66253096\n",
      "Iteration 38, loss = 0.66182322\n",
      "Iteration 39, loss = 0.66111995\n",
      "Iteration 40, loss = 0.66042183\n",
      "Iteration 41, loss = 0.65971983\n",
      "Iteration 42, loss = 0.65901889\n",
      "Iteration 43, loss = 0.65831960\n",
      "Iteration 44, loss = 0.65761950\n",
      "Iteration 45, loss = 0.65691077\n",
      "Iteration 46, loss = 0.65620510\n",
      "Iteration 47, loss = 0.65549781\n",
      "Iteration 48, loss = 0.65478841\n",
      "Iteration 49, loss = 0.65407472\n",
      "Iteration 50, loss = 0.65336254\n",
      "Iteration 51, loss = 0.65264016\n",
      "Iteration 52, loss = 0.65191766\n",
      "Iteration 53, loss = 0.65118995\n",
      "Iteration 54, loss = 0.65046233\n",
      "Iteration 55, loss = 0.64973148\n",
      "Iteration 56, loss = 0.64899629\n",
      "Iteration 57, loss = 0.64825886\n",
      "Iteration 58, loss = 0.64752323\n",
      "Iteration 59, loss = 0.64677554\n",
      "Iteration 60, loss = 0.64603319\n",
      "Iteration 61, loss = 0.64528416\n",
      "Iteration 62, loss = 0.64453116\n",
      "Iteration 63, loss = 0.64377808\n",
      "Iteration 64, loss = 0.64302112\n",
      "Iteration 65, loss = 0.64226384\n",
      "Iteration 66, loss = 0.64150151\n",
      "Iteration 67, loss = 0.64073580\n",
      "Iteration 68, loss = 0.63996712\n",
      "Iteration 69, loss = 0.63919635\n",
      "Iteration 70, loss = 0.63842235\n",
      "Iteration 71, loss = 0.63764911\n",
      "Iteration 72, loss = 0.63687158\n",
      "Iteration 73, loss = 0.63608996\n",
      "Iteration 74, loss = 0.63531310\n",
      "Iteration 75, loss = 0.63452583\n",
      "Iteration 76, loss = 0.63374250\n",
      "Iteration 77, loss = 0.63295351\n",
      "Iteration 78, loss = 0.63216528\n",
      "Iteration 79, loss = 0.63136966\n",
      "Iteration 80, loss = 0.63057780\n",
      "Iteration 81, loss = 0.62978530\n",
      "Iteration 82, loss = 0.62898759\n",
      "Iteration 83, loss = 0.62819251\n",
      "Iteration 84, loss = 0.62740386\n",
      "Iteration 85, loss = 0.62660043\n",
      "Iteration 86, loss = 0.62580209\n",
      "Iteration 87, loss = 0.62500204\n",
      "Iteration 88, loss = 0.62420454\n",
      "Iteration 89, loss = 0.62340116\n",
      "Iteration 90, loss = 0.62259707\n",
      "Iteration 91, loss = 0.62179764\n",
      "Iteration 92, loss = 0.62099854\n",
      "Iteration 93, loss = 0.62019187\n",
      "Iteration 94, loss = 0.61938893\n",
      "Iteration 95, loss = 0.61858848\n",
      "Iteration 96, loss = 0.61778121\n",
      "Iteration 97, loss = 0.61697844\n",
      "Iteration 98, loss = 0.61617646\n",
      "Iteration 99, loss = 0.61537353\n",
      "Iteration 100, loss = 0.61457491\n",
      "Iteration 101, loss = 0.61377650\n",
      "Iteration 102, loss = 0.61297317\n",
      "Iteration 103, loss = 0.61216987\n",
      "Iteration 104, loss = 0.61136847\n",
      "Iteration 105, loss = 0.61057044\n",
      "Iteration 106, loss = 0.60976911\n",
      "Iteration 107, loss = 0.60896930\n",
      "Iteration 108, loss = 0.60816815\n",
      "Iteration 109, loss = 0.60736347\n",
      "Iteration 110, loss = 0.60656069\n",
      "Iteration 111, loss = 0.60575693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 112, loss = 0.60495736\n",
      "Iteration 113, loss = 0.60415090\n",
      "Iteration 114, loss = 0.60334634\n",
      "Iteration 115, loss = 0.60254152\n",
      "Iteration 116, loss = 0.60173643\n",
      "Iteration 117, loss = 0.60093668\n",
      "Iteration 118, loss = 0.60013124\n",
      "Iteration 119, loss = 0.59932614\n",
      "Iteration 120, loss = 0.59851603\n",
      "Iteration 121, loss = 0.59771515\n",
      "Iteration 122, loss = 0.59691342\n",
      "Iteration 123, loss = 0.59610619\n",
      "Iteration 124, loss = 0.59530000\n",
      "Iteration 125, loss = 0.59449205\n",
      "Iteration 126, loss = 0.59368725\n",
      "Iteration 127, loss = 0.59288183\n",
      "Iteration 128, loss = 0.59207669\n",
      "Iteration 129, loss = 0.59127071\n",
      "Iteration 130, loss = 0.59046587\n",
      "Iteration 131, loss = 0.58966407\n",
      "Iteration 132, loss = 0.58885599\n",
      "Iteration 133, loss = 0.58805511\n",
      "Iteration 134, loss = 0.58725507\n",
      "Iteration 135, loss = 0.58645475\n",
      "Iteration 136, loss = 0.58565826\n",
      "Iteration 137, loss = 0.58485896\n",
      "Iteration 138, loss = 0.58406330\n",
      "Iteration 139, loss = 0.58326467\n",
      "Iteration 140, loss = 0.58247081\n",
      "Iteration 141, loss = 0.58168372\n",
      "Iteration 142, loss = 0.58089139\n",
      "Iteration 143, loss = 0.58010443\n",
      "Iteration 144, loss = 0.57931429\n",
      "Iteration 145, loss = 0.57853241\n",
      "Iteration 146, loss = 0.57774208\n",
      "Iteration 147, loss = 0.57696576\n",
      "Iteration 148, loss = 0.57618375\n",
      "Iteration 149, loss = 0.57540241\n",
      "Iteration 150, loss = 0.57462613\n",
      "Iteration 151, loss = 0.57384780\n",
      "Iteration 152, loss = 0.57307390\n",
      "Iteration 153, loss = 0.57230957\n",
      "Iteration 154, loss = 0.57153426\n",
      "Iteration 155, loss = 0.57076754\n",
      "Iteration 156, loss = 0.57001152\n",
      "Iteration 157, loss = 0.56923399\n",
      "Iteration 158, loss = 0.56847199\n",
      "Iteration 159, loss = 0.56770775\n",
      "Iteration 160, loss = 0.56694866\n",
      "Iteration 161, loss = 0.56618938\n",
      "Iteration 162, loss = 0.56543237\n",
      "Iteration 163, loss = 0.56467831\n",
      "Iteration 164, loss = 0.56392708\n",
      "Iteration 165, loss = 0.56318087\n",
      "Iteration 166, loss = 0.56244126\n",
      "Iteration 167, loss = 0.56169895\n",
      "Iteration 168, loss = 0.56095258\n",
      "Iteration 169, loss = 0.56021171\n",
      "Iteration 170, loss = 0.55948149\n",
      "Iteration 171, loss = 0.55874762\n",
      "Iteration 172, loss = 0.55802222\n",
      "Iteration 173, loss = 0.55729006\n",
      "Iteration 174, loss = 0.55656672\n",
      "Iteration 175, loss = 0.55584547\n",
      "Iteration 176, loss = 0.55512934\n",
      "Iteration 177, loss = 0.55441030\n",
      "Iteration 178, loss = 0.55369846\n",
      "Iteration 179, loss = 0.55298644\n",
      "Iteration 180, loss = 0.55228238\n",
      "Iteration 181, loss = 0.55156778\n",
      "Iteration 182, loss = 0.55086409\n",
      "Iteration 183, loss = 0.55016801\n",
      "Iteration 184, loss = 0.54947748\n",
      "Iteration 185, loss = 0.54876998\n",
      "Iteration 186, loss = 0.54808496\n",
      "Iteration 187, loss = 0.54739189\n",
      "Iteration 188, loss = 0.54671146\n",
      "Iteration 189, loss = 0.54603220\n",
      "Iteration 190, loss = 0.54534964\n",
      "Iteration 191, loss = 0.54467570\n",
      "Iteration 192, loss = 0.54400902\n",
      "Iteration 193, loss = 0.54333562\n",
      "Iteration 194, loss = 0.54267321\n",
      "Iteration 195, loss = 0.54200733\n",
      "Iteration 196, loss = 0.54135468\n",
      "Iteration 197, loss = 0.54069383\n",
      "Iteration 198, loss = 0.54004334\n",
      "Iteration 199, loss = 0.53939815\n",
      "Iteration 200, loss = 0.53875095\n",
      "Iteration 201, loss = 0.53810925\n",
      "Iteration 202, loss = 0.53748022\n",
      "Iteration 203, loss = 0.53684136\n",
      "Iteration 204, loss = 0.53620772\n",
      "Iteration 205, loss = 0.53557677\n",
      "Iteration 206, loss = 0.53494858\n",
      "Iteration 207, loss = 0.53432715\n",
      "Iteration 208, loss = 0.53370763\n",
      "Iteration 209, loss = 0.53309075\n",
      "Iteration 210, loss = 0.53248226\n",
      "Iteration 211, loss = 0.53187210\n",
      "Iteration 212, loss = 0.53127184\n",
      "Iteration 213, loss = 0.53067896\n",
      "Iteration 214, loss = 0.53007647\n",
      "Iteration 215, loss = 0.52947741\n",
      "Iteration 216, loss = 0.52889192\n",
      "Iteration 217, loss = 0.52830261\n",
      "Iteration 218, loss = 0.52771965\n",
      "Iteration 219, loss = 0.52714233\n",
      "Iteration 220, loss = 0.52656118\n",
      "Iteration 221, loss = 0.52598718\n",
      "Iteration 222, loss = 0.52541898\n",
      "Iteration 223, loss = 0.52485236\n",
      "Iteration 224, loss = 0.52428686\n",
      "Iteration 225, loss = 0.52373250\n",
      "Iteration 226, loss = 0.52316789\n",
      "Iteration 227, loss = 0.52261880\n",
      "Iteration 228, loss = 0.52206521\n",
      "Iteration 229, loss = 0.52151954\n",
      "Iteration 230, loss = 0.52097649\n",
      "Iteration 231, loss = 0.52044421\n",
      "Iteration 232, loss = 0.51990040\n",
      "Iteration 233, loss = 0.51936658\n",
      "Iteration 234, loss = 0.51884105\n",
      "Iteration 235, loss = 0.51831018\n",
      "Iteration 236, loss = 0.51778406\n",
      "Iteration 237, loss = 0.51727426\n",
      "Iteration 238, loss = 0.51674886\n",
      "Iteration 239, loss = 0.51623334\n",
      "Iteration 240, loss = 0.51572816\n",
      "Iteration 241, loss = 0.51521618\n",
      "Iteration 242, loss = 0.51471338\n",
      "Iteration 243, loss = 0.51421506\n",
      "Iteration 244, loss = 0.51372371\n",
      "Iteration 245, loss = 0.51322317\n",
      "Iteration 246, loss = 0.51273649\n",
      "Iteration 247, loss = 0.51225362\n",
      "Iteration 248, loss = 0.51177198\n",
      "Iteration 249, loss = 0.51129614\n",
      "Iteration 250, loss = 0.51081899\n",
      "Iteration 251, loss = 0.51035027\n",
      "Iteration 252, loss = 0.50987946\n",
      "Iteration 253, loss = 0.50941715\n",
      "Iteration 254, loss = 0.50895581\n",
      "Iteration 255, loss = 0.50849860\n",
      "Iteration 256, loss = 0.50804364\n",
      "Iteration 257, loss = 0.50759542\n",
      "Iteration 258, loss = 0.50715236\n",
      "Iteration 259, loss = 0.50670310\n",
      "Iteration 260, loss = 0.50626712\n",
      "Iteration 261, loss = 0.50582895\n",
      "Iteration 262, loss = 0.50539431\n",
      "Iteration 263, loss = 0.50496404\n",
      "Iteration 264, loss = 0.50454583\n",
      "Iteration 265, loss = 0.50411245\n",
      "Iteration 266, loss = 0.50369442\n",
      "Iteration 267, loss = 0.50328038\n",
      "Iteration 268, loss = 0.50286657\n",
      "Iteration 269, loss = 0.50245806\n",
      "Iteration 270, loss = 0.50204909\n",
      "Iteration 271, loss = 0.50165008\n",
      "Iteration 272, loss = 0.50124932\n",
      "Iteration 273, loss = 0.50085485\n",
      "Iteration 274, loss = 0.50045653\n",
      "Iteration 275, loss = 0.50006337\n",
      "Iteration 276, loss = 0.49967844\n",
      "Iteration 277, loss = 0.49929205\n",
      "Iteration 278, loss = 0.49891008\n",
      "Iteration 279, loss = 0.49853571\n",
      "Iteration 280, loss = 0.49814822\n",
      "Iteration 281, loss = 0.49777665\n",
      "Iteration 282, loss = 0.49741479\n",
      "Iteration 283, loss = 0.49703971\n",
      "Iteration 284, loss = 0.49668029\n",
      "Iteration 285, loss = 0.49631401\n",
      "Iteration 286, loss = 0.49595647\n",
      "Iteration 287, loss = 0.49559772\n",
      "Iteration 288, loss = 0.49524106\n",
      "Iteration 289, loss = 0.49489540\n",
      "Iteration 290, loss = 0.49454696\n",
      "Iteration 291, loss = 0.49420068\n",
      "Iteration 292, loss = 0.49385940\n",
      "Iteration 293, loss = 0.49351688\n",
      "Iteration 294, loss = 0.49317994\n",
      "Iteration 295, loss = 0.49284643\n",
      "Iteration 296, loss = 0.49251819\n",
      "Iteration 297, loss = 0.49219157\n",
      "Iteration 298, loss = 0.49186280\n",
      "Iteration 299, loss = 0.49154500\n",
      "Iteration 300, loss = 0.49122693\n",
      "Iteration 301, loss = 0.49090626\n",
      "Iteration 302, loss = 0.49059442\n",
      "Iteration 303, loss = 0.49028515\n",
      "Iteration 304, loss = 0.48997558\n",
      "Iteration 305, loss = 0.48967311\n",
      "Iteration 306, loss = 0.48937009\n",
      "Iteration 307, loss = 0.48906975\n",
      "Iteration 308, loss = 0.48876964\n",
      "Iteration 309, loss = 0.48847797\n",
      "Iteration 310, loss = 0.48818320\n",
      "Iteration 311, loss = 0.48790027\n",
      "Iteration 312, loss = 0.48761388\n",
      "Iteration 313, loss = 0.48733363\n",
      "Iteration 314, loss = 0.48704557\n",
      "Iteration 315, loss = 0.48676996\n",
      "Iteration 316, loss = 0.48649472\n",
      "Iteration 317, loss = 0.48622341\n",
      "Iteration 318, loss = 0.48595166\n",
      "Iteration 319, loss = 0.48568078\n",
      "Iteration 320, loss = 0.48541791\n",
      "Iteration 321, loss = 0.48515473\n",
      "Iteration 322, loss = 0.48490632\n",
      "Iteration 323, loss = 0.48463906\n",
      "Iteration 324, loss = 0.48438313\n",
      "Iteration 325, loss = 0.48412942\n",
      "Iteration 326, loss = 0.48388113\n",
      "Iteration 327, loss = 0.48363553\n",
      "Iteration 328, loss = 0.48338783\n",
      "Iteration 329, loss = 0.48314527\n",
      "Iteration 330, loss = 0.48290051\n",
      "Iteration 331, loss = 0.48267162\n",
      "Iteration 332, loss = 0.48242956\n",
      "Iteration 333, loss = 0.48219976\n",
      "Iteration 334, loss = 0.48196499\n",
      "Iteration 335, loss = 0.48173795\n",
      "Iteration 336, loss = 0.48150864\n",
      "Iteration 337, loss = 0.48128582\n",
      "Iteration 338, loss = 0.48107195\n",
      "Iteration 339, loss = 0.48084925\n",
      "Iteration 340, loss = 0.48063688\n",
      "Iteration 341, loss = 0.48042738\n",
      "Iteration 342, loss = 0.48021186\n",
      "Iteration 343, loss = 0.48000883\n",
      "Iteration 344, loss = 0.47980117\n",
      "Iteration 345, loss = 0.47959568\n",
      "Iteration 346, loss = 0.47940120\n",
      "Iteration 347, loss = 0.47919556\n",
      "Iteration 348, loss = 0.47899854\n",
      "Iteration 349, loss = 0.47880210\n",
      "Iteration 350, loss = 0.47861178\n",
      "Iteration 351, loss = 0.47842304\n",
      "Iteration 352, loss = 0.47823566\n",
      "Iteration 353, loss = 0.47804722\n",
      "Iteration 354, loss = 0.47785833\n",
      "Iteration 355, loss = 0.47767962\n",
      "Iteration 356, loss = 0.47750756\n",
      "Iteration 357, loss = 0.47733007\n",
      "Iteration 358, loss = 0.47715558\n",
      "Iteration 359, loss = 0.47697607\n",
      "Iteration 360, loss = 0.47680161\n",
      "Iteration 361, loss = 0.47663774\n",
      "Iteration 362, loss = 0.47646698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 363, loss = 0.47629975\n",
      "Iteration 364, loss = 0.47613785\n",
      "Iteration 365, loss = 0.47597647\n",
      "Iteration 366, loss = 0.47581175\n",
      "Iteration 367, loss = 0.47565515\n",
      "Iteration 368, loss = 0.47550187\n",
      "Iteration 369, loss = 0.47534654\n",
      "Iteration 370, loss = 0.47519236\n",
      "Iteration 371, loss = 0.47504154\n",
      "Iteration 372, loss = 0.47489157\n",
      "Iteration 373, loss = 0.47474744\n",
      "Iteration 374, loss = 0.47459609\n",
      "Iteration 375, loss = 0.47445391\n",
      "Iteration 376, loss = 0.47430937\n",
      "Iteration 377, loss = 0.47416827\n",
      "Iteration 378, loss = 0.47403189\n",
      "Iteration 379, loss = 0.47389699\n",
      "Iteration 380, loss = 0.47375545\n",
      "Iteration 381, loss = 0.47363126\n",
      "Iteration 382, loss = 0.47349983\n",
      "Iteration 383, loss = 0.47336114\n",
      "Iteration 384, loss = 0.47323319\n",
      "Iteration 385, loss = 0.47311105\n",
      "Iteration 386, loss = 0.47298645\n",
      "Iteration 387, loss = 0.47286228\n",
      "Iteration 388, loss = 0.47274313\n",
      "Iteration 389, loss = 0.47262863\n",
      "Iteration 390, loss = 0.47250447\n",
      "Iteration 391, loss = 0.47238988\n",
      "Iteration 392, loss = 0.47228490\n",
      "Iteration 393, loss = 0.47216307\n",
      "Iteration 394, loss = 0.47205180\n",
      "Iteration 395, loss = 0.47193607\n",
      "Iteration 396, loss = 0.47182928\n",
      "Iteration 397, loss = 0.47172423\n",
      "Iteration 398, loss = 0.47163058\n",
      "Iteration 399, loss = 0.47151376\n",
      "Iteration 400, loss = 0.47140853\n",
      "Iteration 401, loss = 0.47130598\n",
      "Iteration 402, loss = 0.47120970\n",
      "Iteration 403, loss = 0.47111111\n",
      "Iteration 404, loss = 0.47101822\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71952087\n",
      "Iteration 2, loss = 0.71578818\n",
      "Iteration 3, loss = 0.71222304\n",
      "Iteration 4, loss = 0.70883857\n",
      "Iteration 5, loss = 0.70564041\n",
      "Iteration 6, loss = 0.70262860\n",
      "Iteration 7, loss = 0.69978202\n",
      "Iteration 8, loss = 0.69711305\n",
      "Iteration 9, loss = 0.69458244\n",
      "Iteration 10, loss = 0.69221347\n",
      "Iteration 11, loss = 0.68998037\n",
      "Iteration 12, loss = 0.68790340\n",
      "Iteration 13, loss = 0.68595854\n",
      "Iteration 14, loss = 0.68413179\n",
      "Iteration 15, loss = 0.68241892\n",
      "Iteration 16, loss = 0.68081866\n",
      "Iteration 17, loss = 0.67929924\n",
      "Iteration 18, loss = 0.67788804\n",
      "Iteration 19, loss = 0.67656210\n",
      "Iteration 20, loss = 0.67531277\n",
      "Iteration 21, loss = 0.67414330\n",
      "Iteration 22, loss = 0.67302940\n",
      "Iteration 23, loss = 0.67197447\n",
      "Iteration 24, loss = 0.67097370\n",
      "Iteration 25, loss = 0.67002468\n",
      "Iteration 26, loss = 0.66910215\n",
      "Iteration 27, loss = 0.66821809\n",
      "Iteration 28, loss = 0.66737683\n",
      "Iteration 29, loss = 0.66655744\n",
      "Iteration 30, loss = 0.66575824\n",
      "Iteration 31, loss = 0.66497987\n",
      "Iteration 32, loss = 0.66421716\n",
      "Iteration 33, loss = 0.66346712\n",
      "Iteration 34, loss = 0.66272035\n",
      "Iteration 35, loss = 0.66198746\n",
      "Iteration 36, loss = 0.66126552\n",
      "Iteration 37, loss = 0.66053954\n",
      "Iteration 38, loss = 0.65981901\n",
      "Iteration 39, loss = 0.65910193\n",
      "Iteration 40, loss = 0.65838801\n",
      "Iteration 41, loss = 0.65766830\n",
      "Iteration 42, loss = 0.65694925\n",
      "Iteration 43, loss = 0.65622999\n",
      "Iteration 44, loss = 0.65550950\n",
      "Iteration 45, loss = 0.65477957\n",
      "Iteration 46, loss = 0.65405408\n",
      "Iteration 47, loss = 0.65332527\n",
      "Iteration 48, loss = 0.65259532\n",
      "Iteration 49, loss = 0.65186152\n",
      "Iteration 50, loss = 0.65112836\n",
      "Iteration 51, loss = 0.65038622\n",
      "Iteration 52, loss = 0.64964489\n",
      "Iteration 53, loss = 0.64889577\n",
      "Iteration 54, loss = 0.64814885\n",
      "Iteration 55, loss = 0.64739897\n",
      "Iteration 56, loss = 0.64664683\n",
      "Iteration 57, loss = 0.64589127\n",
      "Iteration 58, loss = 0.64513846\n",
      "Iteration 59, loss = 0.64437520\n",
      "Iteration 60, loss = 0.64361800\n",
      "Iteration 61, loss = 0.64285334\n",
      "Iteration 62, loss = 0.64208473\n",
      "Iteration 63, loss = 0.64131848\n",
      "Iteration 64, loss = 0.64054781\n",
      "Iteration 65, loss = 0.63977788\n",
      "Iteration 66, loss = 0.63900379\n",
      "Iteration 67, loss = 0.63822715\n",
      "Iteration 68, loss = 0.63744775\n",
      "Iteration 69, loss = 0.63666725\n",
      "Iteration 70, loss = 0.63588502\n",
      "Iteration 71, loss = 0.63510219\n",
      "Iteration 72, loss = 0.63431657\n",
      "Iteration 73, loss = 0.63352797\n",
      "Iteration 74, loss = 0.63274452\n",
      "Iteration 75, loss = 0.63195180\n",
      "Iteration 76, loss = 0.63116285\n",
      "Iteration 77, loss = 0.63036860\n",
      "Iteration 78, loss = 0.62957665\n",
      "Iteration 79, loss = 0.62877604\n",
      "Iteration 80, loss = 0.62798207\n",
      "Iteration 81, loss = 0.62718721\n",
      "Iteration 82, loss = 0.62638595\n",
      "Iteration 83, loss = 0.62558996\n",
      "Iteration 84, loss = 0.62479914\n",
      "Iteration 85, loss = 0.62399581\n",
      "Iteration 86, loss = 0.62319769\n",
      "Iteration 87, loss = 0.62239766\n",
      "Iteration 88, loss = 0.62160008\n",
      "Iteration 89, loss = 0.62079831\n",
      "Iteration 90, loss = 0.61999605\n",
      "Iteration 91, loss = 0.61919895\n",
      "Iteration 92, loss = 0.61840124\n",
      "Iteration 93, loss = 0.61759991\n",
      "Iteration 94, loss = 0.61680125\n",
      "Iteration 95, loss = 0.61600493\n",
      "Iteration 96, loss = 0.61520228\n",
      "Iteration 97, loss = 0.61440599\n",
      "Iteration 98, loss = 0.61360986\n",
      "Iteration 99, loss = 0.61281345\n",
      "Iteration 100, loss = 0.61202253\n",
      "Iteration 101, loss = 0.61123268\n",
      "Iteration 102, loss = 0.61043667\n",
      "Iteration 103, loss = 0.60964202\n",
      "Iteration 104, loss = 0.60885075\n",
      "Iteration 105, loss = 0.60806354\n",
      "Iteration 106, loss = 0.60727313\n",
      "Iteration 107, loss = 0.60648472\n",
      "Iteration 108, loss = 0.60569555\n",
      "Iteration 109, loss = 0.60490658\n",
      "Iteration 110, loss = 0.60411863\n",
      "Iteration 111, loss = 0.60333061\n",
      "Iteration 112, loss = 0.60254684\n",
      "Iteration 113, loss = 0.60175867\n",
      "Iteration 114, loss = 0.60097190\n",
      "Iteration 115, loss = 0.60018731\n",
      "Iteration 116, loss = 0.59940011\n",
      "Iteration 117, loss = 0.59862107\n",
      "Iteration 118, loss = 0.59783564\n",
      "Iteration 119, loss = 0.59705484\n",
      "Iteration 120, loss = 0.59626750\n",
      "Iteration 121, loss = 0.59549201\n",
      "Iteration 122, loss = 0.59471540\n",
      "Iteration 123, loss = 0.59393417\n",
      "Iteration 124, loss = 0.59315191\n",
      "Iteration 125, loss = 0.59237257\n",
      "Iteration 126, loss = 0.59159659\n",
      "Iteration 127, loss = 0.59082006\n",
      "Iteration 128, loss = 0.59004484\n",
      "Iteration 129, loss = 0.58927062\n",
      "Iteration 130, loss = 0.58849723\n",
      "Iteration 131, loss = 0.58772966\n",
      "Iteration 132, loss = 0.58695389\n",
      "Iteration 133, loss = 0.58618779\n",
      "Iteration 134, loss = 0.58542365\n",
      "Iteration 135, loss = 0.58465910\n",
      "Iteration 136, loss = 0.58389981\n",
      "Iteration 137, loss = 0.58313803\n",
      "Iteration 138, loss = 0.58238314\n",
      "Iteration 139, loss = 0.58162229\n",
      "Iteration 140, loss = 0.58087067\n",
      "Iteration 141, loss = 0.58012442\n",
      "Iteration 142, loss = 0.57937481\n",
      "Iteration 143, loss = 0.57863108\n",
      "Iteration 144, loss = 0.57788380\n",
      "Iteration 145, loss = 0.57714579\n",
      "Iteration 146, loss = 0.57640087\n",
      "Iteration 147, loss = 0.57567010\n",
      "Iteration 148, loss = 0.57493448\n",
      "Iteration 149, loss = 0.57420268\n",
      "Iteration 150, loss = 0.57347667\n",
      "Iteration 151, loss = 0.57274962\n",
      "Iteration 152, loss = 0.57203025\n",
      "Iteration 153, loss = 0.57131912\n",
      "Iteration 154, loss = 0.57060014\n",
      "Iteration 155, loss = 0.56988922\n",
      "Iteration 156, loss = 0.56918755\n",
      "Iteration 157, loss = 0.56846999\n",
      "Iteration 158, loss = 0.56776520\n",
      "Iteration 159, loss = 0.56705952\n",
      "Iteration 160, loss = 0.56635937\n",
      "Iteration 161, loss = 0.56566138\n",
      "Iteration 162, loss = 0.56496533\n",
      "Iteration 163, loss = 0.56427116\n",
      "Iteration 164, loss = 0.56358166\n",
      "Iteration 165, loss = 0.56289821\n",
      "Iteration 166, loss = 0.56222130\n",
      "Iteration 167, loss = 0.56154245\n",
      "Iteration 168, loss = 0.56086185\n",
      "Iteration 169, loss = 0.56018567\n",
      "Iteration 170, loss = 0.55952026\n",
      "Iteration 171, loss = 0.55885405\n",
      "Iteration 172, loss = 0.55819565\n",
      "Iteration 173, loss = 0.55753111\n",
      "Iteration 174, loss = 0.55687652\n",
      "Iteration 175, loss = 0.55622307\n",
      "Iteration 176, loss = 0.55557610\n",
      "Iteration 177, loss = 0.55492875\n",
      "Iteration 178, loss = 0.55428638\n",
      "Iteration 179, loss = 0.55364310\n",
      "Iteration 180, loss = 0.55301126\n",
      "Iteration 181, loss = 0.55236871\n",
      "Iteration 182, loss = 0.55173958\n",
      "Iteration 183, loss = 0.55111591\n",
      "Iteration 184, loss = 0.55050092\n",
      "Iteration 185, loss = 0.54986563\n",
      "Iteration 186, loss = 0.54925232\n",
      "Iteration 187, loss = 0.54863478\n",
      "Iteration 188, loss = 0.54803081\n",
      "Iteration 189, loss = 0.54742634\n",
      "Iteration 190, loss = 0.54681994\n",
      "Iteration 191, loss = 0.54622236\n",
      "Iteration 192, loss = 0.54563593\n",
      "Iteration 193, loss = 0.54503921\n",
      "Iteration 194, loss = 0.54445473\n",
      "Iteration 195, loss = 0.54386772\n",
      "Iteration 196, loss = 0.54329437\n",
      "Iteration 197, loss = 0.54271394\n",
      "Iteration 198, loss = 0.54214379\n",
      "Iteration 199, loss = 0.54157820\n",
      "Iteration 200, loss = 0.54101223\n",
      "Iteration 201, loss = 0.54045166\n",
      "Iteration 202, loss = 0.53990240\n",
      "Iteration 203, loss = 0.53934475\n",
      "Iteration 204, loss = 0.53879363\n",
      "Iteration 205, loss = 0.53824618\n",
      "Iteration 206, loss = 0.53770009\n",
      "Iteration 207, loss = 0.53716111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 208, loss = 0.53662621\n",
      "Iteration 209, loss = 0.53608957\n",
      "Iteration 210, loss = 0.53556371\n",
      "Iteration 211, loss = 0.53503801\n",
      "Iteration 212, loss = 0.53452085\n",
      "Iteration 213, loss = 0.53401267\n",
      "Iteration 214, loss = 0.53349281\n",
      "Iteration 215, loss = 0.53297752\n",
      "Iteration 216, loss = 0.53247734\n",
      "Iteration 217, loss = 0.53197365\n",
      "Iteration 218, loss = 0.53147592\n",
      "Iteration 219, loss = 0.53098384\n",
      "Iteration 220, loss = 0.53048656\n",
      "Iteration 221, loss = 0.52999854\n",
      "Iteration 222, loss = 0.52951471\n",
      "Iteration 223, loss = 0.52903258\n",
      "Iteration 224, loss = 0.52855448\n",
      "Iteration 225, loss = 0.52808708\n",
      "Iteration 226, loss = 0.52760980\n",
      "Iteration 227, loss = 0.52714443\n",
      "Iteration 228, loss = 0.52667995\n",
      "Iteration 229, loss = 0.52622450\n",
      "Iteration 230, loss = 0.52576664\n",
      "Iteration 231, loss = 0.52532057\n",
      "Iteration 232, loss = 0.52486635\n",
      "Iteration 233, loss = 0.52442063\n",
      "Iteration 234, loss = 0.52398098\n",
      "Iteration 235, loss = 0.52353706\n",
      "Iteration 236, loss = 0.52310012\n",
      "Iteration 237, loss = 0.52267633\n",
      "Iteration 238, loss = 0.52223743\n",
      "Iteration 239, loss = 0.52181398\n",
      "Iteration 240, loss = 0.52139777\n",
      "Iteration 241, loss = 0.52096974\n",
      "Iteration 242, loss = 0.52055622\n",
      "Iteration 243, loss = 0.52014346\n",
      "Iteration 244, loss = 0.51974370\n",
      "Iteration 245, loss = 0.51932703\n",
      "Iteration 246, loss = 0.51892764\n",
      "Iteration 247, loss = 0.51853041\n",
      "Iteration 248, loss = 0.51813594\n",
      "Iteration 249, loss = 0.51774484\n",
      "Iteration 250, loss = 0.51735444\n",
      "Iteration 251, loss = 0.51697211\n",
      "Iteration 252, loss = 0.51658788\n",
      "Iteration 253, loss = 0.51621119\n",
      "Iteration 254, loss = 0.51583582\n",
      "Iteration 255, loss = 0.51546489\n",
      "Iteration 256, loss = 0.51509215\n",
      "Iteration 257, loss = 0.51473078\n",
      "Iteration 258, loss = 0.51436835\n",
      "Iteration 259, loss = 0.51400466\n",
      "Iteration 260, loss = 0.51365066\n",
      "Iteration 261, loss = 0.51329810\n",
      "Iteration 262, loss = 0.51294547\n",
      "Iteration 263, loss = 0.51259789\n",
      "Iteration 264, loss = 0.51226251\n",
      "Iteration 265, loss = 0.51191381\n",
      "Iteration 266, loss = 0.51157533\n",
      "Iteration 267, loss = 0.51124327\n",
      "Iteration 268, loss = 0.51091051\n",
      "Iteration 269, loss = 0.51059011\n",
      "Iteration 270, loss = 0.51025921\n",
      "Iteration 271, loss = 0.50993733\n",
      "Iteration 272, loss = 0.50962051\n",
      "Iteration 273, loss = 0.50930477\n",
      "Iteration 274, loss = 0.50898426\n",
      "Iteration 275, loss = 0.50867321\n",
      "Iteration 276, loss = 0.50836692\n",
      "Iteration 277, loss = 0.50806175\n",
      "Iteration 278, loss = 0.50775857\n",
      "Iteration 279, loss = 0.50746125\n",
      "Iteration 280, loss = 0.50715861\n",
      "Iteration 281, loss = 0.50686429\n",
      "Iteration 282, loss = 0.50657953\n",
      "Iteration 283, loss = 0.50628467\n",
      "Iteration 284, loss = 0.50600656\n",
      "Iteration 285, loss = 0.50571507\n",
      "Iteration 286, loss = 0.50544007\n",
      "Iteration 287, loss = 0.50515358\n",
      "Iteration 288, loss = 0.50487964\n",
      "Iteration 289, loss = 0.50460617\n",
      "Iteration 290, loss = 0.50433471\n",
      "Iteration 291, loss = 0.50406452\n",
      "Iteration 292, loss = 0.50379798\n",
      "Iteration 293, loss = 0.50353122\n",
      "Iteration 294, loss = 0.50327059\n",
      "Iteration 295, loss = 0.50301311\n",
      "Iteration 296, loss = 0.50275594\n",
      "Iteration 297, loss = 0.50250572\n",
      "Iteration 298, loss = 0.50225063\n",
      "Iteration 299, loss = 0.50200671\n",
      "Iteration 300, loss = 0.50176066\n",
      "Iteration 301, loss = 0.50151464\n",
      "Iteration 302, loss = 0.50127431\n",
      "Iteration 303, loss = 0.50103971\n",
      "Iteration 304, loss = 0.50080113\n",
      "Iteration 305, loss = 0.50056756\n",
      "Iteration 306, loss = 0.50033701\n",
      "Iteration 307, loss = 0.50010684\n",
      "Iteration 308, loss = 0.49987978\n",
      "Iteration 309, loss = 0.49965698\n",
      "Iteration 310, loss = 0.49943190\n",
      "Iteration 311, loss = 0.49921769\n",
      "Iteration 312, loss = 0.49900336\n",
      "Iteration 313, loss = 0.49879203\n",
      "Iteration 314, loss = 0.49857542\n",
      "Iteration 315, loss = 0.49836514\n",
      "Iteration 316, loss = 0.49815574\n",
      "Iteration 317, loss = 0.49795839\n",
      "Iteration 318, loss = 0.49775080\n",
      "Iteration 319, loss = 0.49754746\n",
      "Iteration 320, loss = 0.49735441\n",
      "Iteration 321, loss = 0.49715356\n",
      "Iteration 322, loss = 0.49696880\n",
      "Iteration 323, loss = 0.49677116\n",
      "Iteration 324, loss = 0.49658051\n",
      "Iteration 325, loss = 0.49639211\n",
      "Iteration 326, loss = 0.49620863\n",
      "Iteration 327, loss = 0.49602504\n",
      "Iteration 328, loss = 0.49584609\n",
      "Iteration 329, loss = 0.49566579\n",
      "Iteration 330, loss = 0.49548662\n",
      "Iteration 331, loss = 0.49532183\n",
      "Iteration 332, loss = 0.49514221\n",
      "Iteration 333, loss = 0.49497460\n",
      "Iteration 334, loss = 0.49479829\n",
      "Iteration 335, loss = 0.49463598\n",
      "Iteration 336, loss = 0.49446745\n",
      "Iteration 337, loss = 0.49430621\n",
      "Iteration 338, loss = 0.49415247\n",
      "Iteration 339, loss = 0.49399178\n",
      "Iteration 340, loss = 0.49383604\n",
      "Iteration 341, loss = 0.49368576\n",
      "Iteration 342, loss = 0.49352824\n",
      "Iteration 343, loss = 0.49338321\n",
      "Iteration 344, loss = 0.49323374\n",
      "Iteration 345, loss = 0.49308868\n",
      "Iteration 346, loss = 0.49294876\n",
      "Iteration 347, loss = 0.49280099\n",
      "Iteration 348, loss = 0.49266245\n",
      "Iteration 349, loss = 0.49252129\n",
      "Iteration 350, loss = 0.49238456\n",
      "Iteration 351, loss = 0.49225197\n",
      "Iteration 352, loss = 0.49211914\n",
      "Iteration 353, loss = 0.49198631\n",
      "Iteration 354, loss = 0.49185279\n",
      "Iteration 355, loss = 0.49172715\n",
      "Iteration 356, loss = 0.49160528\n",
      "Iteration 357, loss = 0.49148494\n",
      "Iteration 358, loss = 0.49136356\n",
      "Iteration 359, loss = 0.49123526\n",
      "Iteration 360, loss = 0.49111315\n",
      "Iteration 361, loss = 0.49100011\n",
      "Iteration 362, loss = 0.49088327\n",
      "Iteration 363, loss = 0.49076872\n",
      "Iteration 364, loss = 0.49065710\n",
      "Iteration 365, loss = 0.49054319\n",
      "Iteration 366, loss = 0.49042954\n",
      "Iteration 367, loss = 0.49032510\n",
      "Iteration 368, loss = 0.49021781\n",
      "Iteration 369, loss = 0.49011348\n",
      "Iteration 370, loss = 0.49000747\n",
      "Iteration 371, loss = 0.48990355\n",
      "Iteration 372, loss = 0.48979962\n",
      "Iteration 373, loss = 0.48970415\n",
      "Iteration 374, loss = 0.48960360\n",
      "Iteration 375, loss = 0.48950633\n",
      "Iteration 376, loss = 0.48940837\n",
      "Iteration 377, loss = 0.48931524\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "y = y.ravel()\n",
    "cv_results = model_selection.cross_val_score(clf, x, y, cv=kfold, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como foi dito logo acima, o resultado da execução desse algoritmo será usado na seção [3 Resultados e Conclusões](https://github.com/GSansigolo/CAP-240-394/blob/master/src/conclusao/conclusao.ipynb) para uma comparação gráfica dos algoritmos de aprendizado de máquina. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74473516 0.90044671 0.70644544 0.70835992 0.72941927 0.76579451\n",
      " 0.76643267 0.56987875 0.62986599 0.70580728]\n"
     ]
    }
   ],
   "source": [
    "print(cv_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
