{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP 394 - Introduction to Data Science\n",
    "### Trabalho Final: Análise de Índices Espectrais e Métodos de Aprendizado de Máquina para Detecção de Áreas Queimadas\n",
    "### Alunos: Fabiana Zioti, Gabriel Sansigolo, Rafael Mariano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos os algoritmos SVM, MLP e a Árvore de Decisão. Para realizar o treinamento destes métodos, utilizamos os atributos NDVI, Diferença do NVDI, NBRL, Diferença do NBRL, e as medianas das bandas [2,5] dos dados vetoriais. Os dados vetoriais possui também uma coluna verifica, esta coluna indica se a área em questão é uma cicatriz de queimadas ou não. \n",
    "\n",
    "Foram realizados dois teste, o primeiro utilizando os índices espectrais e suas diferenças. No segundo teste, foi adicionado aos dados de entrada as medianas das bandas [2,5].  Este jupyter notebook apresenta o método de aprendizado de máquina Redes Neurais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Redes Neurais Artificiais simulam a maneira de transmissão e aprendizagem de informação dos neurônios presentes no cérebro humano. RNA’s são definidas como uma técnica de otimização onde se busca por uma hipótese de uma função recorrendo a otimização da mesma, ou seja, minimizar (ou otimizar) uma função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redes Neurais Artificias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diversos modelos de ANN: Perceptron, Auto Encoder, Convolution, Kohonen são alguns exemplos. Neste trabalho utilizamos a Rede Neural Perceptron Multicamadas (MLP). A rede MLP é um modelo composto por múltiplas camadas de neurônios artificiais totalmente conectadas, denominadas: Camada de Entrada, Camada Intermediária e Camada de Saída. Um exemplo de sua arquitetura pode ser visualizada na figura a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQAGlDDqdEvLbBHwJw9qpYzXHMJFhO7wB1EU0rL5Bn-bqUVIl5cyw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que a MLP consiga prever com exatidão a hipótese é necessário realizar seu treinamento. Por se tratar de um algoritmo de aprendizado supervisionado, devemos apresentar o conjunto $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})$. A seguir são apresentados os resultados dos algoritmos de aprendizagem supervisionado. Foram realizados dois teste. No primeiro os algoritmos receberam como entrada NDVI, Diferença do NVDI, NBRL, Diferença do NBRL. O conjunto de entrada neste caso ficou definido como $x^{(i)} \\in \\mathbb{R}^{4} y^{(i)} \\in {0,1} $. No segundo teste os algoritmos receberam como entrada os atributos dos índices espectrais NDVI, Diferença do NVDI, NBRL, Diferença do NBRL, e as medianas das bandas $[2,5]$. O conjunto de entrada neste caso ficou definido como $x^{(i)} \\in \\mathbb{R}^{8} y^{(i)} \\in {0,1} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código MLP usando o Sckit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza o import das bibliotecas e os dados vetoriais para realizar o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import gdal, gdalconst\n",
    "import geopandas as gpd\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"DS/221_067_2017.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cod_sat</th>\n",
       "      <th>cena_id</th>\n",
       "      <th>nome_arq</th>\n",
       "      <th>orb_pto</th>\n",
       "      <th>area_ha</th>\n",
       "      <th>perim</th>\n",
       "      <th>n_arq_ant</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>nbrl</th>\n",
       "      <th>...</th>\n",
       "      <th>medianb4</th>\n",
       "      <th>medianb5</th>\n",
       "      <th>medianb6</th>\n",
       "      <th>medianb7</th>\n",
       "      <th>verifica</th>\n",
       "      <th>proc_id</th>\n",
       "      <th>focos</th>\n",
       "      <th>data_atual</th>\n",
       "      <th>data_anter</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23946449</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>12.968645</td>\n",
       "      <td>2639</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.211523</td>\n",
       "      <td>-0.044128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144337</td>\n",
       "      <td>0.227026</td>\n",
       "      <td>0.304834</td>\n",
       "      <td>0.245830</td>\n",
       "      <td>1</td>\n",
       "      <td>5886</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.9715795250224 -10.3363584942161,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23946450</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>1.263320</td>\n",
       "      <td>600</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.042114</td>\n",
       "      <td>-0.027508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>0.044027</td>\n",
       "      <td>0.049672</td>\n",
       "      <td>0.046862</td>\n",
       "      <td>1</td>\n",
       "      <td>5886</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.358025045795 -10.3373571795547, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23946451</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>7.568930</td>\n",
       "      <td>2340</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.306289</td>\n",
       "      <td>-0.003284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083032</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.205350</td>\n",
       "      <td>0.169358</td>\n",
       "      <td>1</td>\n",
       "      <td>5886</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.9860921817999 -10.3359982294198,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23946452</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>1.804195</td>\n",
       "      <td>660</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.256839</td>\n",
       "      <td>-0.017937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143473</td>\n",
       "      <td>0.242181</td>\n",
       "      <td>0.360290</td>\n",
       "      <td>0.251297</td>\n",
       "      <td>0</td>\n",
       "      <td>5886</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.0128759745859 -10.3521908768478,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23946453</td>\n",
       "      <td>8</td>\n",
       "      <td>LC82210672017125LGN00</td>\n",
       "      <td>LC82210672017125LGN00.tar.gz</td>\n",
       "      <td>221_067</td>\n",
       "      <td>2.886433</td>\n",
       "      <td>1140</td>\n",
       "      <td>LC82210672017109LGN00.tar.bz</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116126</td>\n",
       "      <td>0.195281</td>\n",
       "      <td>0.244584</td>\n",
       "      <td>0.189877</td>\n",
       "      <td>1</td>\n",
       "      <td>5886</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>2017-04-19 00:00:00</td>\n",
       "      <td>POLYGON ((-46.3574643464459 -10.3343756813685,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  cod_sat                cena_id                      nome_arq  \\\n",
       "0  23946449        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "1  23946450        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "2  23946451        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "3  23946452        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "4  23946453        8  LC82210672017125LGN00  LC82210672017125LGN00.tar.gz   \n",
       "\n",
       "   orb_pto    area_ha  perim                     n_arq_ant      ndvi  \\\n",
       "0  221_067  12.968645   2639  LC82210672017109LGN00.tar.bz  0.211523   \n",
       "1  221_067   1.263320    600  LC82210672017109LGN00.tar.bz  0.042114   \n",
       "2  221_067   7.568930   2340  LC82210672017109LGN00.tar.bz  0.306289   \n",
       "3  221_067   1.804195    660  LC82210672017109LGN00.tar.bz  0.256839   \n",
       "4  221_067   2.886433   1140  LC82210672017109LGN00.tar.bz  0.240240   \n",
       "\n",
       "       nbrl                        ...                          medianb4  \\\n",
       "0 -0.044128                        ...                          0.144337   \n",
       "1 -0.027508                        ...                          0.040925   \n",
       "2 -0.003284                        ...                          0.083032   \n",
       "3 -0.017937                        ...                          0.143473   \n",
       "4  0.000871                        ...                          0.116126   \n",
       "\n",
       "   medianb5  medianb6  medianb7  verifica  proc_id  focos  data_atual  \\\n",
       "0  0.227026  0.304834  0.245830         1     5886      2  2017-05-05   \n",
       "1  0.044027  0.049672  0.046862         1     5886      0  2017-05-05   \n",
       "2  0.151609  0.205350  0.169358         1     5886      1  2017-05-05   \n",
       "3  0.242181  0.360290  0.251297         0     5886      0  2017-05-05   \n",
       "4  0.195281  0.244584  0.189877         1     5886      2  2017-05-05   \n",
       "\n",
       "            data_anter                                           geometry  \n",
       "0  2017-04-19 00:00:00  POLYGON ((-46.9715795250224 -10.3363584942161,...  \n",
       "1  2017-04-19 00:00:00  POLYGON ((-46.358025045795 -10.3373571795547, ...  \n",
       "2  2017-04-19 00:00:00  POLYGON ((-46.9860921817999 -10.3359982294198,...  \n",
       "3  2017-04-19 00:00:00  POLYGON ((-46.0128759745859 -10.3521908768478,...  \n",
       "4  2017-04-19 00:00:00  POLYGON ((-46.3574643464459 -10.3343756813685,...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepara os dados de entrada NDVI, Diferença do NVDI, NBRL, Diferença do NBRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df[['ndvi','nbrl','dif_ndvi','dif_dnbrl']])\n",
    "y = np.array(df[['verifica']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define o modelo da Rede Neural Artificial Multilayer. Os parâmetros foram definidos de forma empirica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(activation='identity', solver='adam', verbose=True, learning_rate_init=0.00001,\n",
    "                    alpha=1e-5,hidden_layer_sizes=(80,2),  momentum=0.7, random_state=1, max_iter=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69645092\n",
      "Iteration 2, loss = 0.69513492\n",
      "Iteration 3, loss = 0.69399732\n",
      "Iteration 4, loss = 0.69298255\n",
      "Iteration 5, loss = 0.69205988\n",
      "Iteration 6, loss = 0.69119180\n",
      "Iteration 7, loss = 0.69036597\n",
      "Iteration 8, loss = 0.68956344\n",
      "Iteration 9, loss = 0.68877914\n",
      "Iteration 10, loss = 0.68801262\n",
      "Iteration 11, loss = 0.68725170\n",
      "Iteration 12, loss = 0.68649485\n",
      "Iteration 13, loss = 0.68574292\n",
      "Iteration 14, loss = 0.68500047\n",
      "Iteration 15, loss = 0.68424602\n",
      "Iteration 16, loss = 0.68351533\n",
      "Iteration 17, loss = 0.68276942\n",
      "Iteration 18, loss = 0.68204237\n",
      "Iteration 19, loss = 0.68129723\n",
      "Iteration 20, loss = 0.68056282\n",
      "Iteration 21, loss = 0.67983284\n",
      "Iteration 22, loss = 0.67910008\n",
      "Iteration 23, loss = 0.67837511\n",
      "Iteration 24, loss = 0.67764927\n",
      "Iteration 25, loss = 0.67692413\n",
      "Iteration 26, loss = 0.67620012\n",
      "Iteration 27, loss = 0.67547915\n",
      "Iteration 28, loss = 0.67475420\n",
      "Iteration 29, loss = 0.67403890\n",
      "Iteration 30, loss = 0.67331989\n",
      "Iteration 31, loss = 0.67260286\n",
      "Iteration 32, loss = 0.67189113\n",
      "Iteration 33, loss = 0.67117408\n",
      "Iteration 34, loss = 0.67045917\n",
      "Iteration 35, loss = 0.66975433\n",
      "Iteration 36, loss = 0.66903488\n",
      "Iteration 37, loss = 0.66832972\n",
      "Iteration 38, loss = 0.66761906\n",
      "Iteration 39, loss = 0.66691839\n",
      "Iteration 40, loss = 0.66620427\n",
      "Iteration 41, loss = 0.66550949\n",
      "Iteration 42, loss = 0.66479501\n",
      "Iteration 43, loss = 0.66409759\n",
      "Iteration 44, loss = 0.66339504\n",
      "Iteration 45, loss = 0.66269092\n",
      "Iteration 46, loss = 0.66199787\n",
      "Iteration 47, loss = 0.66130021\n",
      "Iteration 48, loss = 0.66059445\n",
      "Iteration 49, loss = 0.65990636\n",
      "Iteration 50, loss = 0.65921754\n",
      "Iteration 51, loss = 0.65852902\n",
      "Iteration 52, loss = 0.65783842\n",
      "Iteration 53, loss = 0.65714741\n",
      "Iteration 54, loss = 0.65646516\n",
      "Iteration 55, loss = 0.65578034\n",
      "Iteration 56, loss = 0.65511068\n",
      "Iteration 57, loss = 0.65442275\n",
      "Iteration 58, loss = 0.65374884\n",
      "Iteration 59, loss = 0.65306481\n",
      "Iteration 60, loss = 0.65239159\n",
      "Iteration 61, loss = 0.65171868\n",
      "Iteration 62, loss = 0.65104749\n",
      "Iteration 63, loss = 0.65036068\n",
      "Iteration 64, loss = 0.64969776\n",
      "Iteration 65, loss = 0.64901997\n",
      "Iteration 66, loss = 0.64834812\n",
      "Iteration 67, loss = 0.64767881\n",
      "Iteration 68, loss = 0.64700679\n",
      "Iteration 69, loss = 0.64633868\n",
      "Iteration 70, loss = 0.64567242\n",
      "Iteration 71, loss = 0.64500266\n",
      "Iteration 72, loss = 0.64433497\n",
      "Iteration 73, loss = 0.64367649\n",
      "Iteration 74, loss = 0.64300875\n",
      "Iteration 75, loss = 0.64234141\n",
      "Iteration 76, loss = 0.64168413\n",
      "Iteration 77, loss = 0.64101862\n",
      "Iteration 78, loss = 0.64035971\n",
      "Iteration 79, loss = 0.63970385\n",
      "Iteration 80, loss = 0.63904178\n",
      "Iteration 81, loss = 0.63837939\n",
      "Iteration 82, loss = 0.63771848\n",
      "Iteration 83, loss = 0.63705983\n",
      "Iteration 84, loss = 0.63640351\n",
      "Iteration 85, loss = 0.63575095\n",
      "Iteration 86, loss = 0.63510134\n",
      "Iteration 87, loss = 0.63445121\n",
      "Iteration 88, loss = 0.63380087\n",
      "Iteration 89, loss = 0.63315662\n",
      "Iteration 90, loss = 0.63250089\n",
      "Iteration 91, loss = 0.63185634\n",
      "Iteration 92, loss = 0.63121380\n",
      "Iteration 93, loss = 0.63057208\n",
      "Iteration 94, loss = 0.62993148\n",
      "Iteration 95, loss = 0.62929513\n",
      "Iteration 96, loss = 0.62865629\n",
      "Iteration 97, loss = 0.62803283\n",
      "Iteration 98, loss = 0.62739191\n",
      "Iteration 99, loss = 0.62676076\n",
      "Iteration 100, loss = 0.62612736\n",
      "Iteration 101, loss = 0.62550136\n",
      "Iteration 102, loss = 0.62487035\n",
      "Iteration 103, loss = 0.62424289\n",
      "Iteration 104, loss = 0.62361950\n",
      "Iteration 105, loss = 0.62300040\n",
      "Iteration 106, loss = 0.62237036\n",
      "Iteration 107, loss = 0.62175239\n",
      "Iteration 108, loss = 0.62113215\n",
      "Iteration 109, loss = 0.62051347\n",
      "Iteration 110, loss = 0.61990441\n",
      "Iteration 111, loss = 0.61929394\n",
      "Iteration 112, loss = 0.61867584\n",
      "Iteration 113, loss = 0.61806543\n",
      "Iteration 114, loss = 0.61745752\n",
      "Iteration 115, loss = 0.61685055\n",
      "Iteration 116, loss = 0.61624292\n",
      "Iteration 117, loss = 0.61564045\n",
      "Iteration 118, loss = 0.61504232\n",
      "Iteration 119, loss = 0.61443195\n",
      "Iteration 120, loss = 0.61383226\n",
      "Iteration 121, loss = 0.61324432\n",
      "Iteration 122, loss = 0.61264389\n",
      "Iteration 123, loss = 0.61204230\n",
      "Iteration 124, loss = 0.61144793\n",
      "Iteration 125, loss = 0.61086228\n",
      "Iteration 126, loss = 0.61026795\n",
      "Iteration 127, loss = 0.60967767\n",
      "Iteration 128, loss = 0.60908141\n",
      "Iteration 129, loss = 0.60849812\n",
      "Iteration 130, loss = 0.60790753\n",
      "Iteration 131, loss = 0.60732064\n",
      "Iteration 132, loss = 0.60673621\n",
      "Iteration 133, loss = 0.60616055\n",
      "Iteration 134, loss = 0.60557098\n",
      "Iteration 135, loss = 0.60498604\n",
      "Iteration 136, loss = 0.60441256\n",
      "Iteration 137, loss = 0.60383731\n",
      "Iteration 138, loss = 0.60325805\n",
      "Iteration 139, loss = 0.60268499\n",
      "Iteration 140, loss = 0.60211458\n",
      "Iteration 141, loss = 0.60154808\n",
      "Iteration 142, loss = 0.60097793\n",
      "Iteration 143, loss = 0.60042206\n",
      "Iteration 144, loss = 0.59984476\n",
      "Iteration 145, loss = 0.59928567\n",
      "Iteration 146, loss = 0.59872607\n",
      "Iteration 147, loss = 0.59818925\n",
      "Iteration 148, loss = 0.59761339\n",
      "Iteration 149, loss = 0.59705765\n",
      "Iteration 150, loss = 0.59650019\n",
      "Iteration 151, loss = 0.59595478\n",
      "Iteration 152, loss = 0.59541037\n",
      "Iteration 153, loss = 0.59486052\n",
      "Iteration 154, loss = 0.59432032\n",
      "Iteration 155, loss = 0.59377993\n",
      "Iteration 156, loss = 0.59324098\n",
      "Iteration 157, loss = 0.59270196\n",
      "Iteration 158, loss = 0.59216943\n",
      "Iteration 159, loss = 0.59163724\n",
      "Iteration 160, loss = 0.59110657\n",
      "Iteration 161, loss = 0.59058116\n",
      "Iteration 162, loss = 0.59005511\n",
      "Iteration 163, loss = 0.58954351\n",
      "Iteration 164, loss = 0.58901688\n",
      "Iteration 165, loss = 0.58850289\n",
      "Iteration 166, loss = 0.58799211\n",
      "Iteration 167, loss = 0.58747847\n",
      "Iteration 168, loss = 0.58697657\n",
      "Iteration 169, loss = 0.58646650\n",
      "Iteration 170, loss = 0.58596180\n",
      "Iteration 171, loss = 0.58545871\n",
      "Iteration 172, loss = 0.58496116\n",
      "Iteration 173, loss = 0.58446454\n",
      "Iteration 174, loss = 0.58396571\n",
      "Iteration 175, loss = 0.58347435\n",
      "Iteration 176, loss = 0.58298758\n",
      "Iteration 177, loss = 0.58249600\n",
      "Iteration 178, loss = 0.58200836\n",
      "Iteration 179, loss = 0.58153107\n",
      "Iteration 180, loss = 0.58104770\n",
      "Iteration 181, loss = 0.58057716\n",
      "Iteration 182, loss = 0.58009951\n",
      "Iteration 183, loss = 0.57962738\n",
      "Iteration 184, loss = 0.57915685\n",
      "Iteration 185, loss = 0.57869001\n",
      "Iteration 186, loss = 0.57822389\n",
      "Iteration 187, loss = 0.57776151\n",
      "Iteration 188, loss = 0.57730112\n",
      "Iteration 189, loss = 0.57683951\n",
      "Iteration 190, loss = 0.57638259\n",
      "Iteration 191, loss = 0.57592723\n",
      "Iteration 192, loss = 0.57548116\n",
      "Iteration 193, loss = 0.57503374\n",
      "Iteration 194, loss = 0.57459003\n",
      "Iteration 195, loss = 0.57414822\n",
      "Iteration 196, loss = 0.57370609\n",
      "Iteration 197, loss = 0.57327092\n",
      "Iteration 198, loss = 0.57283843\n",
      "Iteration 199, loss = 0.57240650\n",
      "Iteration 200, loss = 0.57198286\n",
      "Iteration 201, loss = 0.57155937\n",
      "Iteration 202, loss = 0.57113022\n",
      "Iteration 203, loss = 0.57070788\n",
      "Iteration 204, loss = 0.57029076\n",
      "Iteration 205, loss = 0.56987866\n",
      "Iteration 206, loss = 0.56947055\n",
      "Iteration 207, loss = 0.56908198\n",
      "Iteration 208, loss = 0.56865993\n",
      "Iteration 209, loss = 0.56824851\n",
      "Iteration 210, loss = 0.56785031\n",
      "Iteration 211, loss = 0.56745609\n",
      "Iteration 212, loss = 0.56707001\n",
      "Iteration 213, loss = 0.56667149\n",
      "Iteration 214, loss = 0.56628400\n",
      "Iteration 215, loss = 0.56590129\n",
      "Iteration 216, loss = 0.56551737\n",
      "Iteration 217, loss = 0.56513755\n",
      "Iteration 218, loss = 0.56475485\n",
      "Iteration 219, loss = 0.56438389\n",
      "Iteration 220, loss = 0.56401188\n",
      "Iteration 221, loss = 0.56364952\n",
      "Iteration 222, loss = 0.56327716\n",
      "Iteration 223, loss = 0.56291319\n",
      "Iteration 224, loss = 0.56255662\n",
      "Iteration 225, loss = 0.56219692\n",
      "Iteration 226, loss = 0.56184146\n",
      "Iteration 227, loss = 0.56149444\n",
      "Iteration 228, loss = 0.56114833\n",
      "Iteration 229, loss = 0.56079976\n",
      "Iteration 230, loss = 0.56045727\n",
      "Iteration 231, loss = 0.56012236\n",
      "Iteration 232, loss = 0.55978711\n",
      "Iteration 233, loss = 0.55945725\n",
      "Iteration 234, loss = 0.55912983\n",
      "Iteration 235, loss = 0.55879065\n",
      "Iteration 236, loss = 0.55846867\n",
      "Iteration 237, loss = 0.55814587\n",
      "Iteration 238, loss = 0.55782385\n",
      "Iteration 239, loss = 0.55750750\n",
      "Iteration 240, loss = 0.55720508\n",
      "Iteration 241, loss = 0.55688662\n",
      "Iteration 242, loss = 0.55656603\n",
      "Iteration 243, loss = 0.55625744\n",
      "Iteration 244, loss = 0.55595826\n",
      "Iteration 245, loss = 0.55566145\n",
      "Iteration 246, loss = 0.55534921\n",
      "Iteration 247, loss = 0.55505352\n",
      "Iteration 248, loss = 0.55475696\n",
      "Iteration 249, loss = 0.55446798\n",
      "Iteration 250, loss = 0.55417864\n",
      "Iteration 251, loss = 0.55389340\n",
      "Iteration 252, loss = 0.55360550\n",
      "Iteration 253, loss = 0.55332736\n",
      "Iteration 254, loss = 0.55304750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.55277701\n",
      "Iteration 256, loss = 0.55249137\n",
      "Iteration 257, loss = 0.55222001\n",
      "Iteration 258, loss = 0.55195377\n",
      "Iteration 259, loss = 0.55168288\n",
      "Iteration 260, loss = 0.55142935\n",
      "Iteration 261, loss = 0.55116481\n",
      "Iteration 262, loss = 0.55090601\n",
      "Iteration 263, loss = 0.55065411\n",
      "Iteration 264, loss = 0.55038880\n",
      "Iteration 265, loss = 0.55014417\n",
      "Iteration 266, loss = 0.54989013\n",
      "Iteration 267, loss = 0.54965234\n",
      "Iteration 268, loss = 0.54940299\n",
      "Iteration 269, loss = 0.54916304\n",
      "Iteration 270, loss = 0.54891964\n",
      "Iteration 271, loss = 0.54869328\n",
      "Iteration 272, loss = 0.54846126\n",
      "Iteration 273, loss = 0.54822414\n",
      "Iteration 274, loss = 0.54799794\n",
      "Iteration 275, loss = 0.54777874\n",
      "Iteration 276, loss = 0.54755807\n",
      "Iteration 277, loss = 0.54732885\n",
      "Iteration 278, loss = 0.54711151\n",
      "Iteration 279, loss = 0.54689820\n",
      "Iteration 280, loss = 0.54668946\n",
      "Iteration 281, loss = 0.54646202\n",
      "Iteration 282, loss = 0.54626609\n",
      "Iteration 283, loss = 0.54604920\n",
      "Iteration 284, loss = 0.54586444\n",
      "Iteration 285, loss = 0.54565066\n",
      "Iteration 286, loss = 0.54544601\n",
      "Iteration 287, loss = 0.54524691\n",
      "Iteration 288, loss = 0.54506225\n",
      "Iteration 289, loss = 0.54486144\n",
      "Iteration 290, loss = 0.54467843\n",
      "Iteration 291, loss = 0.54448407\n",
      "Iteration 292, loss = 0.54429571\n",
      "Iteration 293, loss = 0.54410662\n",
      "Iteration 294, loss = 0.54392543\n",
      "Iteration 295, loss = 0.54374489\n",
      "Iteration 296, loss = 0.54356213\n",
      "Iteration 297, loss = 0.54339145\n",
      "Iteration 298, loss = 0.54321090\n",
      "Iteration 299, loss = 0.54304364\n",
      "Iteration 300, loss = 0.54286706\n",
      "Iteration 301, loss = 0.54269115\n",
      "Iteration 302, loss = 0.54252848\n",
      "Iteration 303, loss = 0.54235821\n",
      "Iteration 304, loss = 0.54221507\n",
      "Iteration 305, loss = 0.54203756\n",
      "Iteration 306, loss = 0.54187346\n",
      "Iteration 307, loss = 0.54171763\n",
      "Iteration 308, loss = 0.54156345\n",
      "Iteration 309, loss = 0.54142245\n",
      "Iteration 310, loss = 0.54124690\n",
      "Iteration 311, loss = 0.54109619\n",
      "Iteration 312, loss = 0.54095024\n",
      "Iteration 313, loss = 0.54080400\n",
      "Iteration 314, loss = 0.54065088\n",
      "Iteration 315, loss = 0.54050277\n",
      "Iteration 316, loss = 0.54036133\n",
      "Iteration 317, loss = 0.54022045\n",
      "Iteration 318, loss = 0.54007927\n",
      "Iteration 319, loss = 0.53995256\n",
      "Iteration 320, loss = 0.53981890\n",
      "Iteration 321, loss = 0.53967957\n",
      "Iteration 322, loss = 0.53954874\n",
      "Iteration 323, loss = 0.53941378\n",
      "Iteration 324, loss = 0.53928408\n",
      "Iteration 325, loss = 0.53915552\n",
      "Iteration 326, loss = 0.53903252\n",
      "Iteration 327, loss = 0.53890329\n",
      "Iteration 328, loss = 0.53878508\n",
      "Iteration 329, loss = 0.53865650\n",
      "Iteration 330, loss = 0.53853938\n",
      "Iteration 331, loss = 0.53842050\n",
      "Iteration 332, loss = 0.53829661\n",
      "Iteration 333, loss = 0.53818032\n",
      "Iteration 334, loss = 0.53806413\n",
      "Iteration 335, loss = 0.53795995\n",
      "Iteration 336, loss = 0.53783603\n",
      "Iteration 337, loss = 0.53772663\n",
      "Iteration 338, loss = 0.53761635\n",
      "Iteration 339, loss = 0.53750420\n",
      "Iteration 340, loss = 0.53739791\n",
      "Iteration 341, loss = 0.53729526\n",
      "Iteration 342, loss = 0.53719030\n",
      "Iteration 343, loss = 0.53709233\n",
      "Iteration 344, loss = 0.53698403\n",
      "Iteration 345, loss = 0.53688822\n",
      "Iteration 346, loss = 0.53678818\n",
      "Iteration 347, loss = 0.53669329\n",
      "Iteration 348, loss = 0.53658937\n",
      "Iteration 349, loss = 0.53649470\n",
      "Iteration 350, loss = 0.53640301\n",
      "Iteration 351, loss = 0.53630513\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69549004\n",
      "Iteration 2, loss = 0.69459908\n",
      "Iteration 3, loss = 0.69378921\n",
      "Iteration 4, loss = 0.69302938\n",
      "Iteration 5, loss = 0.69230356\n",
      "Iteration 6, loss = 0.69160091\n",
      "Iteration 7, loss = 0.69091051\n",
      "Iteration 8, loss = 0.69023131\n",
      "Iteration 9, loss = 0.68955654\n",
      "Iteration 10, loss = 0.68889308\n",
      "Iteration 11, loss = 0.68822979\n",
      "Iteration 12, loss = 0.68757025\n",
      "Iteration 13, loss = 0.68691156\n",
      "Iteration 14, loss = 0.68626054\n",
      "Iteration 15, loss = 0.68560004\n",
      "Iteration 16, loss = 0.68496453\n",
      "Iteration 17, loss = 0.68430821\n",
      "Iteration 18, loss = 0.68367072\n",
      "Iteration 19, loss = 0.68302113\n",
      "Iteration 20, loss = 0.68237789\n",
      "Iteration 21, loss = 0.68174380\n",
      "Iteration 22, loss = 0.68110411\n",
      "Iteration 23, loss = 0.68046798\n",
      "Iteration 24, loss = 0.67983634\n",
      "Iteration 25, loss = 0.67920518\n",
      "Iteration 26, loss = 0.67857435\n",
      "Iteration 27, loss = 0.67794446\n",
      "Iteration 28, loss = 0.67731015\n",
      "Iteration 29, loss = 0.67669137\n",
      "Iteration 30, loss = 0.67606521\n",
      "Iteration 31, loss = 0.67544314\n",
      "Iteration 32, loss = 0.67482601\n",
      "Iteration 33, loss = 0.67419893\n",
      "Iteration 34, loss = 0.67357757\n",
      "Iteration 35, loss = 0.67296869\n",
      "Iteration 36, loss = 0.67233895\n",
      "Iteration 37, loss = 0.67172990\n",
      "Iteration 38, loss = 0.67111444\n",
      "Iteration 39, loss = 0.67051294\n",
      "Iteration 40, loss = 0.66989141\n",
      "Iteration 41, loss = 0.66928816\n",
      "Iteration 42, loss = 0.66867474\n",
      "Iteration 43, loss = 0.66807262\n",
      "Iteration 44, loss = 0.66746501\n",
      "Iteration 45, loss = 0.66685537\n",
      "Iteration 46, loss = 0.66625690\n",
      "Iteration 47, loss = 0.66565221\n",
      "Iteration 48, loss = 0.66504405\n",
      "Iteration 49, loss = 0.66444815\n",
      "Iteration 50, loss = 0.66385620\n",
      "Iteration 51, loss = 0.66325933\n",
      "Iteration 52, loss = 0.66266736\n",
      "Iteration 53, loss = 0.66206783\n",
      "Iteration 54, loss = 0.66148180\n",
      "Iteration 55, loss = 0.66088761\n",
      "Iteration 56, loss = 0.66031391\n",
      "Iteration 57, loss = 0.65971870\n",
      "Iteration 58, loss = 0.65913551\n",
      "Iteration 59, loss = 0.65855199\n",
      "Iteration 60, loss = 0.65796975\n",
      "Iteration 61, loss = 0.65739072\n",
      "Iteration 62, loss = 0.65681657\n",
      "Iteration 63, loss = 0.65622481\n",
      "Iteration 64, loss = 0.65565642\n",
      "Iteration 65, loss = 0.65507072\n",
      "Iteration 66, loss = 0.65449572\n",
      "Iteration 67, loss = 0.65392157\n",
      "Iteration 68, loss = 0.65334436\n",
      "Iteration 69, loss = 0.65277091\n",
      "Iteration 70, loss = 0.65219857\n",
      "Iteration 71, loss = 0.65162649\n",
      "Iteration 72, loss = 0.65105364\n",
      "Iteration 73, loss = 0.65048906\n",
      "Iteration 74, loss = 0.64992144\n",
      "Iteration 75, loss = 0.64934724\n",
      "Iteration 76, loss = 0.64878378\n",
      "Iteration 77, loss = 0.64821549\n",
      "Iteration 78, loss = 0.64765249\n",
      "Iteration 79, loss = 0.64709219\n",
      "Iteration 80, loss = 0.64652407\n",
      "Iteration 81, loss = 0.64595819\n",
      "Iteration 82, loss = 0.64539596\n",
      "Iteration 83, loss = 0.64483277\n",
      "Iteration 84, loss = 0.64427369\n",
      "Iteration 85, loss = 0.64371970\n",
      "Iteration 86, loss = 0.64316569\n",
      "Iteration 87, loss = 0.64261328\n",
      "Iteration 88, loss = 0.64205667\n",
      "Iteration 89, loss = 0.64150848\n",
      "Iteration 90, loss = 0.64094920\n",
      "Iteration 91, loss = 0.64040206\n",
      "Iteration 92, loss = 0.63985335\n",
      "Iteration 93, loss = 0.63930975\n",
      "Iteration 94, loss = 0.63876085\n",
      "Iteration 95, loss = 0.63822087\n",
      "Iteration 96, loss = 0.63767633\n",
      "Iteration 97, loss = 0.63714452\n",
      "Iteration 98, loss = 0.63660236\n",
      "Iteration 99, loss = 0.63607055\n",
      "Iteration 100, loss = 0.63553414\n",
      "Iteration 101, loss = 0.63500104\n",
      "Iteration 102, loss = 0.63446990\n",
      "Iteration 103, loss = 0.63393790\n",
      "Iteration 104, loss = 0.63341027\n",
      "Iteration 105, loss = 0.63288763\n",
      "Iteration 106, loss = 0.63235367\n",
      "Iteration 107, loss = 0.63183179\n",
      "Iteration 108, loss = 0.63130662\n",
      "Iteration 109, loss = 0.63078439\n",
      "Iteration 110, loss = 0.63026821\n",
      "Iteration 111, loss = 0.62975440\n",
      "Iteration 112, loss = 0.62923353\n",
      "Iteration 113, loss = 0.62871569\n",
      "Iteration 114, loss = 0.62820072\n",
      "Iteration 115, loss = 0.62769014\n",
      "Iteration 116, loss = 0.62717810\n",
      "Iteration 117, loss = 0.62666654\n",
      "Iteration 118, loss = 0.62616474\n",
      "Iteration 119, loss = 0.62564705\n",
      "Iteration 120, loss = 0.62514111\n",
      "Iteration 121, loss = 0.62464357\n",
      "Iteration 122, loss = 0.62414608\n",
      "Iteration 123, loss = 0.62363528\n",
      "Iteration 124, loss = 0.62313479\n",
      "Iteration 125, loss = 0.62264397\n",
      "Iteration 126, loss = 0.62214191\n",
      "Iteration 127, loss = 0.62164675\n",
      "Iteration 128, loss = 0.62114695\n",
      "Iteration 129, loss = 0.62065847\n",
      "Iteration 130, loss = 0.62016290\n",
      "Iteration 131, loss = 0.61967103\n",
      "Iteration 132, loss = 0.61918200\n",
      "Iteration 133, loss = 0.61870157\n",
      "Iteration 134, loss = 0.61820656\n",
      "Iteration 135, loss = 0.61771839\n",
      "Iteration 136, loss = 0.61724569\n",
      "Iteration 137, loss = 0.61676485\n",
      "Iteration 138, loss = 0.61628208\n",
      "Iteration 139, loss = 0.61580553\n",
      "Iteration 140, loss = 0.61533163\n",
      "Iteration 141, loss = 0.61486009\n",
      "Iteration 142, loss = 0.61438901\n",
      "Iteration 143, loss = 0.61392454\n",
      "Iteration 144, loss = 0.61344602\n",
      "Iteration 145, loss = 0.61298121\n",
      "Iteration 146, loss = 0.61251723\n",
      "Iteration 147, loss = 0.61207516\n",
      "Iteration 148, loss = 0.61159660\n",
      "Iteration 149, loss = 0.61113435\n",
      "Iteration 150, loss = 0.61067581\n",
      "Iteration 151, loss = 0.61022268\n",
      "Iteration 152, loss = 0.60977158\n",
      "Iteration 153, loss = 0.60931577\n",
      "Iteration 154, loss = 0.60887086\n",
      "Iteration 155, loss = 0.60842282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 156, loss = 0.60797582\n",
      "Iteration 157, loss = 0.60753582\n",
      "Iteration 158, loss = 0.60709243\n",
      "Iteration 159, loss = 0.60665189\n",
      "Iteration 160, loss = 0.60621478\n",
      "Iteration 161, loss = 0.60578190\n",
      "Iteration 162, loss = 0.60534608\n",
      "Iteration 163, loss = 0.60492320\n",
      "Iteration 164, loss = 0.60448800\n",
      "Iteration 165, loss = 0.60406325\n",
      "Iteration 166, loss = 0.60364287\n",
      "Iteration 167, loss = 0.60321686\n",
      "Iteration 168, loss = 0.60280656\n",
      "Iteration 169, loss = 0.60238379\n",
      "Iteration 170, loss = 0.60196688\n",
      "Iteration 171, loss = 0.60155343\n",
      "Iteration 172, loss = 0.60114384\n",
      "Iteration 173, loss = 0.60073401\n",
      "Iteration 174, loss = 0.60032827\n",
      "Iteration 175, loss = 0.59991921\n",
      "Iteration 176, loss = 0.59952008\n",
      "Iteration 177, loss = 0.59911595\n",
      "Iteration 178, loss = 0.59871224\n",
      "Iteration 179, loss = 0.59831999\n",
      "Iteration 180, loss = 0.59792342\n",
      "Iteration 181, loss = 0.59753233\n",
      "Iteration 182, loss = 0.59714534\n",
      "Iteration 183, loss = 0.59675460\n",
      "Iteration 184, loss = 0.59636893\n",
      "Iteration 185, loss = 0.59598657\n",
      "Iteration 186, loss = 0.59560208\n",
      "Iteration 187, loss = 0.59522117\n",
      "Iteration 188, loss = 0.59484409\n",
      "Iteration 189, loss = 0.59446704\n",
      "Iteration 190, loss = 0.59409246\n",
      "Iteration 191, loss = 0.59371902\n",
      "Iteration 192, loss = 0.59335270\n",
      "Iteration 193, loss = 0.59299123\n",
      "Iteration 194, loss = 0.59262715\n",
      "Iteration 195, loss = 0.59226736\n",
      "Iteration 196, loss = 0.59190449\n",
      "Iteration 197, loss = 0.59154682\n",
      "Iteration 198, loss = 0.59119714\n",
      "Iteration 199, loss = 0.59084345\n",
      "Iteration 200, loss = 0.59049827\n",
      "Iteration 201, loss = 0.59015141\n",
      "Iteration 202, loss = 0.58980425\n",
      "Iteration 203, loss = 0.58945466\n",
      "Iteration 204, loss = 0.58911484\n",
      "Iteration 205, loss = 0.58878146\n",
      "Iteration 206, loss = 0.58844881\n",
      "Iteration 207, loss = 0.58813653\n",
      "Iteration 208, loss = 0.58778916\n",
      "Iteration 209, loss = 0.58745203\n",
      "Iteration 210, loss = 0.58712997\n",
      "Iteration 211, loss = 0.58680819\n",
      "Iteration 212, loss = 0.58648906\n",
      "Iteration 213, loss = 0.58616451\n",
      "Iteration 214, loss = 0.58585158\n",
      "Iteration 215, loss = 0.58553928\n",
      "Iteration 216, loss = 0.58522568\n",
      "Iteration 217, loss = 0.58491369\n",
      "Iteration 218, loss = 0.58460753\n",
      "Iteration 219, loss = 0.58430227\n",
      "Iteration 220, loss = 0.58399656\n",
      "Iteration 221, loss = 0.58370045\n",
      "Iteration 222, loss = 0.58339696\n",
      "Iteration 223, loss = 0.58310190\n",
      "Iteration 224, loss = 0.58281441\n",
      "Iteration 225, loss = 0.58251840\n",
      "Iteration 226, loss = 0.58222952\n",
      "Iteration 227, loss = 0.58194528\n",
      "Iteration 228, loss = 0.58166351\n",
      "Iteration 229, loss = 0.58138022\n",
      "Iteration 230, loss = 0.58110138\n",
      "Iteration 231, loss = 0.58082811\n",
      "Iteration 232, loss = 0.58055338\n",
      "Iteration 233, loss = 0.58028752\n",
      "Iteration 234, loss = 0.58001905\n",
      "Iteration 235, loss = 0.57974374\n",
      "Iteration 236, loss = 0.57948241\n",
      "Iteration 237, loss = 0.57921913\n",
      "Iteration 238, loss = 0.57895602\n",
      "Iteration 239, loss = 0.57869821\n",
      "Iteration 240, loss = 0.57845335\n",
      "Iteration 241, loss = 0.57819236\n",
      "Iteration 242, loss = 0.57793172\n",
      "Iteration 243, loss = 0.57768048\n",
      "Iteration 244, loss = 0.57743519\n",
      "Iteration 245, loss = 0.57719735\n",
      "Iteration 246, loss = 0.57694159\n",
      "Iteration 247, loss = 0.57669913\n",
      "Iteration 248, loss = 0.57645957\n",
      "Iteration 249, loss = 0.57622768\n",
      "Iteration 250, loss = 0.57598995\n",
      "Iteration 251, loss = 0.57575526\n",
      "Iteration 252, loss = 0.57552311\n",
      "Iteration 253, loss = 0.57529739\n",
      "Iteration 254, loss = 0.57507395\n",
      "Iteration 255, loss = 0.57485731\n",
      "Iteration 256, loss = 0.57462221\n",
      "Iteration 257, loss = 0.57439792\n",
      "Iteration 258, loss = 0.57418037\n",
      "Iteration 259, loss = 0.57396200\n",
      "Iteration 260, loss = 0.57375187\n",
      "Iteration 261, loss = 0.57353968\n",
      "Iteration 262, loss = 0.57333056\n",
      "Iteration 263, loss = 0.57311857\n",
      "Iteration 264, loss = 0.57290739\n",
      "Iteration 265, loss = 0.57270913\n",
      "Iteration 266, loss = 0.57250219\n",
      "Iteration 267, loss = 0.57231358\n",
      "Iteration 268, loss = 0.57210640\n",
      "Iteration 269, loss = 0.57190753\n",
      "Iteration 270, loss = 0.57170813\n",
      "Iteration 271, loss = 0.57152377\n",
      "Iteration 272, loss = 0.57133580\n",
      "Iteration 273, loss = 0.57113859\n",
      "Iteration 274, loss = 0.57095304\n",
      "Iteration 275, loss = 0.57077175\n",
      "Iteration 276, loss = 0.57059882\n",
      "Iteration 277, loss = 0.57040572\n",
      "Iteration 278, loss = 0.57022526\n",
      "Iteration 279, loss = 0.57005194\n",
      "Iteration 280, loss = 0.56987934\n",
      "Iteration 281, loss = 0.56969326\n",
      "Iteration 282, loss = 0.56953079\n",
      "Iteration 283, loss = 0.56935021\n",
      "Iteration 284, loss = 0.56920030\n",
      "Iteration 285, loss = 0.56901924\n",
      "Iteration 286, loss = 0.56885713\n",
      "Iteration 287, loss = 0.56868901\n",
      "Iteration 288, loss = 0.56853784\n",
      "Iteration 289, loss = 0.56837018\n",
      "Iteration 290, loss = 0.56821938\n",
      "Iteration 291, loss = 0.56806189\n",
      "Iteration 292, loss = 0.56790484\n",
      "Iteration 293, loss = 0.56774672\n",
      "Iteration 294, loss = 0.56759498\n",
      "Iteration 295, loss = 0.56745120\n",
      "Iteration 296, loss = 0.56729297\n",
      "Iteration 297, loss = 0.56715937\n",
      "Iteration 298, loss = 0.56700301\n",
      "Iteration 299, loss = 0.56686687\n",
      "Iteration 300, loss = 0.56671831\n",
      "Iteration 301, loss = 0.56657194\n",
      "Iteration 302, loss = 0.56643768\n",
      "Iteration 303, loss = 0.56629628\n",
      "Iteration 304, loss = 0.56617862\n",
      "Iteration 305, loss = 0.56602994\n",
      "Iteration 306, loss = 0.56589819\n",
      "Iteration 307, loss = 0.56576301\n",
      "Iteration 308, loss = 0.56563938\n",
      "Iteration 309, loss = 0.56551825\n",
      "Iteration 310, loss = 0.56537743\n",
      "Iteration 311, loss = 0.56524838\n",
      "Iteration 312, loss = 0.56512863\n",
      "Iteration 313, loss = 0.56500548\n",
      "Iteration 314, loss = 0.56487474\n",
      "Iteration 315, loss = 0.56475399\n",
      "Iteration 316, loss = 0.56463590\n",
      "Iteration 317, loss = 0.56451795\n",
      "Iteration 318, loss = 0.56439749\n",
      "Iteration 319, loss = 0.56429405\n",
      "Iteration 320, loss = 0.56417702\n",
      "Iteration 321, loss = 0.56406744\n",
      "Iteration 322, loss = 0.56395158\n",
      "Iteration 323, loss = 0.56384265\n",
      "Iteration 324, loss = 0.56373148\n",
      "Iteration 325, loss = 0.56362736\n",
      "Iteration 326, loss = 0.56351758\n",
      "Iteration 327, loss = 0.56340937\n",
      "Iteration 328, loss = 0.56330523\n",
      "Iteration 329, loss = 0.56319983\n",
      "Iteration 330, loss = 0.56309950\n",
      "Iteration 331, loss = 0.56299716\n",
      "Iteration 332, loss = 0.56289579\n",
      "Iteration 333, loss = 0.56279537\n",
      "Iteration 334, loss = 0.56269380\n",
      "Iteration 335, loss = 0.56261653\n",
      "Iteration 336, loss = 0.56250337\n",
      "Iteration 337, loss = 0.56241345\n",
      "Iteration 338, loss = 0.56231700\n",
      "Iteration 339, loss = 0.56222146\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69743136\n",
      "Iteration 2, loss = 0.69542845\n",
      "Iteration 3, loss = 0.69372711\n",
      "Iteration 4, loss = 0.69226727\n",
      "Iteration 5, loss = 0.69100577\n",
      "Iteration 6, loss = 0.68987500\n",
      "Iteration 7, loss = 0.68885480\n",
      "Iteration 8, loss = 0.68791323\n",
      "Iteration 9, loss = 0.68702112\n",
      "Iteration 10, loss = 0.68617943\n",
      "Iteration 11, loss = 0.68536159\n",
      "Iteration 12, loss = 0.68456410\n",
      "Iteration 13, loss = 0.68377134\n",
      "Iteration 14, loss = 0.68299728\n",
      "Iteration 15, loss = 0.68222700\n",
      "Iteration 16, loss = 0.68147381\n",
      "Iteration 17, loss = 0.68070572\n",
      "Iteration 18, loss = 0.67994942\n",
      "Iteration 19, loss = 0.67919027\n",
      "Iteration 20, loss = 0.67843854\n",
      "Iteration 21, loss = 0.67768870\n",
      "Iteration 22, loss = 0.67694141\n",
      "Iteration 23, loss = 0.67619100\n",
      "Iteration 24, loss = 0.67544198\n",
      "Iteration 25, loss = 0.67469802\n",
      "Iteration 26, loss = 0.67395492\n",
      "Iteration 27, loss = 0.67320814\n",
      "Iteration 28, loss = 0.67246274\n",
      "Iteration 29, loss = 0.67172562\n",
      "Iteration 30, loss = 0.67098418\n",
      "Iteration 31, loss = 0.67024848\n",
      "Iteration 32, loss = 0.66951603\n",
      "Iteration 33, loss = 0.66877196\n",
      "Iteration 34, loss = 0.66803705\n",
      "Iteration 35, loss = 0.66731189\n",
      "Iteration 36, loss = 0.66656810\n",
      "Iteration 37, loss = 0.66584560\n",
      "Iteration 38, loss = 0.66511323\n",
      "Iteration 39, loss = 0.66440351\n",
      "Iteration 40, loss = 0.66366589\n",
      "Iteration 41, loss = 0.66294704\n",
      "Iteration 42, loss = 0.66222404\n",
      "Iteration 43, loss = 0.66150677\n",
      "Iteration 44, loss = 0.66078724\n",
      "Iteration 45, loss = 0.66006588\n",
      "Iteration 46, loss = 0.65935395\n",
      "Iteration 47, loss = 0.65863458\n",
      "Iteration 48, loss = 0.65791286\n",
      "Iteration 49, loss = 0.65720630\n",
      "Iteration 50, loss = 0.65650304\n",
      "Iteration 51, loss = 0.65579161\n",
      "Iteration 52, loss = 0.65508586\n",
      "Iteration 53, loss = 0.65437692\n",
      "Iteration 54, loss = 0.65367478\n",
      "Iteration 55, loss = 0.65296949\n",
      "Iteration 56, loss = 0.65227558\n",
      "Iteration 57, loss = 0.65156893\n",
      "Iteration 58, loss = 0.65087028\n",
      "Iteration 59, loss = 0.65017208\n",
      "Iteration 60, loss = 0.64947604\n",
      "Iteration 61, loss = 0.64878087\n",
      "Iteration 62, loss = 0.64809353\n",
      "Iteration 63, loss = 0.64738676\n",
      "Iteration 64, loss = 0.64669832\n",
      "Iteration 65, loss = 0.64600268\n",
      "Iteration 66, loss = 0.64530939\n",
      "Iteration 67, loss = 0.64461961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 0.64392832\n",
      "Iteration 69, loss = 0.64323833\n",
      "Iteration 70, loss = 0.64255001\n",
      "Iteration 71, loss = 0.64186353\n",
      "Iteration 72, loss = 0.64117785\n",
      "Iteration 73, loss = 0.64049873\n",
      "Iteration 74, loss = 0.63981524\n",
      "Iteration 75, loss = 0.63913114\n",
      "Iteration 76, loss = 0.63845357\n",
      "Iteration 77, loss = 0.63777191\n",
      "Iteration 78, loss = 0.63709636\n",
      "Iteration 79, loss = 0.63642248\n",
      "Iteration 80, loss = 0.63574265\n",
      "Iteration 81, loss = 0.63506815\n",
      "Iteration 82, loss = 0.63439393\n",
      "Iteration 83, loss = 0.63372160\n",
      "Iteration 84, loss = 0.63305342\n",
      "Iteration 85, loss = 0.63238792\n",
      "Iteration 86, loss = 0.63172953\n",
      "Iteration 87, loss = 0.63106473\n",
      "Iteration 88, loss = 0.63039714\n",
      "Iteration 89, loss = 0.62974722\n",
      "Iteration 90, loss = 0.62907778\n",
      "Iteration 91, loss = 0.62842265\n",
      "Iteration 92, loss = 0.62776507\n",
      "Iteration 93, loss = 0.62710914\n",
      "Iteration 94, loss = 0.62645406\n",
      "Iteration 95, loss = 0.62580816\n",
      "Iteration 96, loss = 0.62514756\n",
      "Iteration 97, loss = 0.62450415\n",
      "Iteration 98, loss = 0.62385799\n",
      "Iteration 99, loss = 0.62321646\n",
      "Iteration 100, loss = 0.62257443\n",
      "Iteration 101, loss = 0.62193186\n",
      "Iteration 102, loss = 0.62128993\n",
      "Iteration 103, loss = 0.62064942\n",
      "Iteration 104, loss = 0.62001770\n",
      "Iteration 105, loss = 0.61939244\n",
      "Iteration 106, loss = 0.61875088\n",
      "Iteration 107, loss = 0.61812213\n",
      "Iteration 108, loss = 0.61749488\n",
      "Iteration 109, loss = 0.61686573\n",
      "Iteration 110, loss = 0.61624503\n",
      "Iteration 111, loss = 0.61561956\n",
      "Iteration 112, loss = 0.61500265\n",
      "Iteration 113, loss = 0.61437586\n",
      "Iteration 114, loss = 0.61375741\n",
      "Iteration 115, loss = 0.61314035\n",
      "Iteration 116, loss = 0.61253077\n",
      "Iteration 117, loss = 0.61191141\n",
      "Iteration 118, loss = 0.61130423\n",
      "Iteration 119, loss = 0.61068423\n",
      "Iteration 120, loss = 0.61007405\n",
      "Iteration 121, loss = 0.60947302\n",
      "Iteration 122, loss = 0.60887247\n",
      "Iteration 123, loss = 0.60825654\n",
      "Iteration 124, loss = 0.60765368\n",
      "Iteration 125, loss = 0.60705367\n",
      "Iteration 126, loss = 0.60645203\n",
      "Iteration 127, loss = 0.60585642\n",
      "Iteration 128, loss = 0.60525003\n",
      "Iteration 129, loss = 0.60465527\n",
      "Iteration 130, loss = 0.60406075\n",
      "Iteration 131, loss = 0.60346306\n",
      "Iteration 132, loss = 0.60286967\n",
      "Iteration 133, loss = 0.60228925\n",
      "Iteration 134, loss = 0.60168910\n",
      "Iteration 135, loss = 0.60110247\n",
      "Iteration 136, loss = 0.60052210\n",
      "Iteration 137, loss = 0.59994218\n",
      "Iteration 138, loss = 0.59935815\n",
      "Iteration 139, loss = 0.59878047\n",
      "Iteration 140, loss = 0.59820512\n",
      "Iteration 141, loss = 0.59763738\n",
      "Iteration 142, loss = 0.59706578\n",
      "Iteration 143, loss = 0.59649983\n",
      "Iteration 144, loss = 0.59592501\n",
      "Iteration 145, loss = 0.59535975\n",
      "Iteration 146, loss = 0.59479775\n",
      "Iteration 147, loss = 0.59425677\n",
      "Iteration 148, loss = 0.59368040\n",
      "Iteration 149, loss = 0.59312246\n",
      "Iteration 150, loss = 0.59256664\n",
      "Iteration 151, loss = 0.59201774\n",
      "Iteration 152, loss = 0.59146933\n",
      "Iteration 153, loss = 0.59091872\n",
      "Iteration 154, loss = 0.59037601\n",
      "Iteration 155, loss = 0.58983700\n",
      "Iteration 156, loss = 0.58929555\n",
      "Iteration 157, loss = 0.58876527\n",
      "Iteration 158, loss = 0.58822748\n",
      "Iteration 159, loss = 0.58769380\n",
      "Iteration 160, loss = 0.58716358\n",
      "Iteration 161, loss = 0.58663790\n",
      "Iteration 162, loss = 0.58611482\n",
      "Iteration 163, loss = 0.58559604\n",
      "Iteration 164, loss = 0.58507425\n",
      "Iteration 165, loss = 0.58455769\n",
      "Iteration 166, loss = 0.58405171\n",
      "Iteration 167, loss = 0.58353492\n",
      "Iteration 168, loss = 0.58303649\n",
      "Iteration 169, loss = 0.58252510\n",
      "Iteration 170, loss = 0.58202385\n",
      "Iteration 171, loss = 0.58152307\n",
      "Iteration 172, loss = 0.58102097\n",
      "Iteration 173, loss = 0.58052922\n",
      "Iteration 174, loss = 0.58003821\n",
      "Iteration 175, loss = 0.57954529\n",
      "Iteration 176, loss = 0.57906216\n",
      "Iteration 177, loss = 0.57857397\n",
      "Iteration 178, loss = 0.57809105\n",
      "Iteration 179, loss = 0.57761322\n",
      "Iteration 180, loss = 0.57713795\n",
      "Iteration 181, loss = 0.57666351\n",
      "Iteration 182, loss = 0.57619493\n",
      "Iteration 183, loss = 0.57572480\n",
      "Iteration 184, loss = 0.57526274\n",
      "Iteration 185, loss = 0.57480101\n",
      "Iteration 186, loss = 0.57433916\n",
      "Iteration 187, loss = 0.57388592\n",
      "Iteration 188, loss = 0.57343082\n",
      "Iteration 189, loss = 0.57298454\n",
      "Iteration 190, loss = 0.57253627\n",
      "Iteration 191, loss = 0.57208608\n",
      "Iteration 192, loss = 0.57165019\n",
      "Iteration 193, loss = 0.57121455\n",
      "Iteration 194, loss = 0.57078659\n",
      "Iteration 195, loss = 0.57035306\n",
      "Iteration 196, loss = 0.56992584\n",
      "Iteration 197, loss = 0.56949512\n",
      "Iteration 198, loss = 0.56907702\n",
      "Iteration 199, loss = 0.56865917\n",
      "Iteration 200, loss = 0.56824871\n",
      "Iteration 201, loss = 0.56783344\n",
      "Iteration 202, loss = 0.56741856\n",
      "Iteration 203, loss = 0.56700890\n",
      "Iteration 204, loss = 0.56660572\n",
      "Iteration 205, loss = 0.56621156\n",
      "Iteration 206, loss = 0.56581299\n",
      "Iteration 207, loss = 0.56543549\n",
      "Iteration 208, loss = 0.56503222\n",
      "Iteration 209, loss = 0.56463733\n",
      "Iteration 210, loss = 0.56425489\n",
      "Iteration 211, loss = 0.56387763\n",
      "Iteration 212, loss = 0.56349741\n",
      "Iteration 213, loss = 0.56311928\n",
      "Iteration 214, loss = 0.56275037\n",
      "Iteration 215, loss = 0.56238048\n",
      "Iteration 216, loss = 0.56201303\n",
      "Iteration 217, loss = 0.56164746\n",
      "Iteration 218, loss = 0.56128649\n",
      "Iteration 219, loss = 0.56092742\n",
      "Iteration 220, loss = 0.56057178\n",
      "Iteration 221, loss = 0.56022475\n",
      "Iteration 222, loss = 0.55987250\n",
      "Iteration 223, loss = 0.55952817\n",
      "Iteration 224, loss = 0.55919060\n",
      "Iteration 225, loss = 0.55884991\n",
      "Iteration 226, loss = 0.55851233\n",
      "Iteration 227, loss = 0.55818225\n",
      "Iteration 228, loss = 0.55785644\n",
      "Iteration 229, loss = 0.55752599\n",
      "Iteration 230, loss = 0.55720225\n",
      "Iteration 231, loss = 0.55688620\n",
      "Iteration 232, loss = 0.55656644\n",
      "Iteration 233, loss = 0.55626076\n",
      "Iteration 234, loss = 0.55594977\n",
      "Iteration 235, loss = 0.55562871\n",
      "Iteration 236, loss = 0.55532508\n",
      "Iteration 237, loss = 0.55502791\n",
      "Iteration 238, loss = 0.55471910\n",
      "Iteration 239, loss = 0.55442258\n",
      "Iteration 240, loss = 0.55414109\n",
      "Iteration 241, loss = 0.55383735\n",
      "Iteration 242, loss = 0.55353982\n",
      "Iteration 243, loss = 0.55325157\n",
      "Iteration 244, loss = 0.55296850\n",
      "Iteration 245, loss = 0.55269428\n",
      "Iteration 246, loss = 0.55240404\n",
      "Iteration 247, loss = 0.55212659\n",
      "Iteration 248, loss = 0.55185355\n",
      "Iteration 249, loss = 0.55158618\n",
      "Iteration 250, loss = 0.55131344\n",
      "Iteration 251, loss = 0.55104688\n",
      "Iteration 252, loss = 0.55077995\n",
      "Iteration 253, loss = 0.55052307\n",
      "Iteration 254, loss = 0.55026435\n",
      "Iteration 255, loss = 0.55002057\n",
      "Iteration 256, loss = 0.54975600\n",
      "Iteration 257, loss = 0.54950140\n",
      "Iteration 258, loss = 0.54925054\n",
      "Iteration 259, loss = 0.54900516\n",
      "Iteration 260, loss = 0.54875876\n",
      "Iteration 261, loss = 0.54852397\n",
      "Iteration 262, loss = 0.54828600\n",
      "Iteration 263, loss = 0.54804378\n",
      "Iteration 264, loss = 0.54780487\n",
      "Iteration 265, loss = 0.54758103\n",
      "Iteration 266, loss = 0.54734501\n",
      "Iteration 267, loss = 0.54713154\n",
      "Iteration 268, loss = 0.54689588\n",
      "Iteration 269, loss = 0.54667142\n",
      "Iteration 270, loss = 0.54645010\n",
      "Iteration 271, loss = 0.54623430\n",
      "Iteration 272, loss = 0.54602389\n",
      "Iteration 273, loss = 0.54580284\n",
      "Iteration 274, loss = 0.54559765\n",
      "Iteration 275, loss = 0.54538918\n",
      "Iteration 276, loss = 0.54519010\n",
      "Iteration 277, loss = 0.54497638\n",
      "Iteration 278, loss = 0.54477216\n",
      "Iteration 279, loss = 0.54457455\n",
      "Iteration 280, loss = 0.54438037\n",
      "Iteration 281, loss = 0.54417803\n",
      "Iteration 282, loss = 0.54399176\n",
      "Iteration 283, loss = 0.54379115\n",
      "Iteration 284, loss = 0.54362212\n",
      "Iteration 285, loss = 0.54341429\n",
      "Iteration 286, loss = 0.54323598\n",
      "Iteration 287, loss = 0.54304564\n",
      "Iteration 288, loss = 0.54287308\n",
      "Iteration 289, loss = 0.54268623\n",
      "Iteration 290, loss = 0.54251325\n",
      "Iteration 291, loss = 0.54233344\n",
      "Iteration 292, loss = 0.54216182\n",
      "Iteration 293, loss = 0.54198341\n",
      "Iteration 294, loss = 0.54181510\n",
      "Iteration 295, loss = 0.54165233\n",
      "Iteration 296, loss = 0.54147676\n",
      "Iteration 297, loss = 0.54132389\n",
      "Iteration 298, loss = 0.54115207\n",
      "Iteration 299, loss = 0.54100142\n",
      "Iteration 300, loss = 0.54083782\n",
      "Iteration 301, loss = 0.54067237\n",
      "Iteration 302, loss = 0.54051736\n",
      "Iteration 303, loss = 0.54036107\n",
      "Iteration 304, loss = 0.54022216\n",
      "Iteration 305, loss = 0.54006689\n",
      "Iteration 306, loss = 0.53991841\n",
      "Iteration 307, loss = 0.53976650\n",
      "Iteration 308, loss = 0.53962616\n",
      "Iteration 309, loss = 0.53948239\n",
      "Iteration 310, loss = 0.53933527\n",
      "Iteration 311, loss = 0.53919635\n",
      "Iteration 312, loss = 0.53905094\n",
      "Iteration 313, loss = 0.53892107\n",
      "Iteration 314, loss = 0.53877371\n",
      "Iteration 315, loss = 0.53863610\n",
      "Iteration 316, loss = 0.53850599\n",
      "Iteration 317, loss = 0.53837335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 318, loss = 0.53824078\n",
      "Iteration 319, loss = 0.53812062\n",
      "Iteration 320, loss = 0.53798657\n",
      "Iteration 321, loss = 0.53786597\n",
      "Iteration 322, loss = 0.53773724\n",
      "Iteration 323, loss = 0.53762300\n",
      "Iteration 324, loss = 0.53749331\n",
      "Iteration 325, loss = 0.53737504\n",
      "Iteration 326, loss = 0.53725455\n",
      "Iteration 327, loss = 0.53713704\n",
      "Iteration 328, loss = 0.53702045\n",
      "Iteration 329, loss = 0.53690804\n",
      "Iteration 330, loss = 0.53678823\n",
      "Iteration 331, loss = 0.53667836\n",
      "Iteration 332, loss = 0.53656874\n",
      "Iteration 333, loss = 0.53645419\n",
      "Iteration 334, loss = 0.53634120\n",
      "Iteration 335, loss = 0.53625056\n",
      "Iteration 336, loss = 0.53613317\n",
      "Iteration 337, loss = 0.53603141\n",
      "Iteration 338, loss = 0.53592573\n",
      "Iteration 339, loss = 0.53581822\n",
      "Iteration 340, loss = 0.53571073\n",
      "Iteration 341, loss = 0.53561162\n",
      "Iteration 342, loss = 0.53551554\n",
      "Iteration 343, loss = 0.53541264\n",
      "Iteration 344, loss = 0.53531043\n",
      "Iteration 345, loss = 0.53522132\n",
      "Iteration 346, loss = 0.53512533\n",
      "Iteration 347, loss = 0.53502314\n",
      "Iteration 348, loss = 0.53492350\n",
      "Iteration 349, loss = 0.53483102\n",
      "Iteration 350, loss = 0.53475922\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69981807\n",
      "Iteration 2, loss = 0.69666219\n",
      "Iteration 3, loss = 0.69389309\n",
      "Iteration 4, loss = 0.69146900\n",
      "Iteration 5, loss = 0.68936492\n",
      "Iteration 6, loss = 0.68749977\n",
      "Iteration 7, loss = 0.68586747\n",
      "Iteration 8, loss = 0.68441547\n",
      "Iteration 9, loss = 0.68310670\n",
      "Iteration 10, loss = 0.68193024\n",
      "Iteration 11, loss = 0.68085650\n",
      "Iteration 12, loss = 0.67986267\n",
      "Iteration 13, loss = 0.67891134\n",
      "Iteration 14, loss = 0.67801602\n",
      "Iteration 15, loss = 0.67715928\n",
      "Iteration 16, loss = 0.67633163\n",
      "Iteration 17, loss = 0.67550775\n",
      "Iteration 18, loss = 0.67471131\n",
      "Iteration 19, loss = 0.67391687\n",
      "Iteration 20, loss = 0.67313265\n",
      "Iteration 21, loss = 0.67235589\n",
      "Iteration 22, loss = 0.67158748\n",
      "Iteration 23, loss = 0.67081543\n",
      "Iteration 24, loss = 0.67004372\n",
      "Iteration 25, loss = 0.66927985\n",
      "Iteration 26, loss = 0.66851578\n",
      "Iteration 27, loss = 0.66774456\n",
      "Iteration 28, loss = 0.66698143\n",
      "Iteration 29, loss = 0.66621723\n",
      "Iteration 30, loss = 0.66545379\n",
      "Iteration 31, loss = 0.66468951\n",
      "Iteration 32, loss = 0.66393230\n",
      "Iteration 33, loss = 0.66316442\n",
      "Iteration 34, loss = 0.66240243\n",
      "Iteration 35, loss = 0.66165246\n",
      "Iteration 36, loss = 0.66088013\n",
      "Iteration 37, loss = 0.66012554\n",
      "Iteration 38, loss = 0.65936339\n",
      "Iteration 39, loss = 0.65861489\n",
      "Iteration 40, loss = 0.65785421\n",
      "Iteration 41, loss = 0.65709952\n",
      "Iteration 42, loss = 0.65634526\n",
      "Iteration 43, loss = 0.65559358\n",
      "Iteration 44, loss = 0.65483923\n",
      "Iteration 45, loss = 0.65408829\n",
      "Iteration 46, loss = 0.65334133\n",
      "Iteration 47, loss = 0.65258587\n",
      "Iteration 48, loss = 0.65183098\n",
      "Iteration 49, loss = 0.65108754\n",
      "Iteration 50, loss = 0.65035203\n",
      "Iteration 51, loss = 0.64959783\n",
      "Iteration 52, loss = 0.64885525\n",
      "Iteration 53, loss = 0.64811129\n",
      "Iteration 54, loss = 0.64737117\n",
      "Iteration 55, loss = 0.64662861\n",
      "Iteration 56, loss = 0.64589766\n",
      "Iteration 57, loss = 0.64515388\n",
      "Iteration 58, loss = 0.64441643\n",
      "Iteration 59, loss = 0.64367936\n",
      "Iteration 60, loss = 0.64294746\n",
      "Iteration 61, loss = 0.64221394\n",
      "Iteration 62, loss = 0.64148643\n",
      "Iteration 63, loss = 0.64074094\n",
      "Iteration 64, loss = 0.64001162\n",
      "Iteration 65, loss = 0.63927750\n",
      "Iteration 66, loss = 0.63854192\n",
      "Iteration 67, loss = 0.63781040\n",
      "Iteration 68, loss = 0.63707901\n",
      "Iteration 69, loss = 0.63634796\n",
      "Iteration 70, loss = 0.63561858\n",
      "Iteration 71, loss = 0.63489257\n",
      "Iteration 72, loss = 0.63416597\n",
      "Iteration 73, loss = 0.63345135\n",
      "Iteration 74, loss = 0.63272330\n",
      "Iteration 75, loss = 0.63200336\n",
      "Iteration 76, loss = 0.63128355\n",
      "Iteration 77, loss = 0.63056075\n",
      "Iteration 78, loss = 0.62984060\n",
      "Iteration 79, loss = 0.62912716\n",
      "Iteration 80, loss = 0.62840666\n",
      "Iteration 81, loss = 0.62769519\n",
      "Iteration 82, loss = 0.62698322\n",
      "Iteration 83, loss = 0.62627483\n",
      "Iteration 84, loss = 0.62556152\n",
      "Iteration 85, loss = 0.62485403\n",
      "Iteration 86, loss = 0.62415639\n",
      "Iteration 87, loss = 0.62345004\n",
      "Iteration 88, loss = 0.62274169\n",
      "Iteration 89, loss = 0.62205127\n",
      "Iteration 90, loss = 0.62134780\n",
      "Iteration 91, loss = 0.62064624\n",
      "Iteration 92, loss = 0.61994944\n",
      "Iteration 93, loss = 0.61925770\n",
      "Iteration 94, loss = 0.61856231\n",
      "Iteration 95, loss = 0.61787029\n",
      "Iteration 96, loss = 0.61717571\n",
      "Iteration 97, loss = 0.61649210\n",
      "Iteration 98, loss = 0.61580427\n",
      "Iteration 99, loss = 0.61512150\n",
      "Iteration 100, loss = 0.61443650\n",
      "Iteration 101, loss = 0.61375218\n",
      "Iteration 102, loss = 0.61307219\n",
      "Iteration 103, loss = 0.61239070\n",
      "Iteration 104, loss = 0.61171785\n",
      "Iteration 105, loss = 0.61104426\n",
      "Iteration 106, loss = 0.61036977\n",
      "Iteration 107, loss = 0.60969717\n",
      "Iteration 108, loss = 0.60902646\n",
      "Iteration 109, loss = 0.60835501\n",
      "Iteration 110, loss = 0.60769411\n",
      "Iteration 111, loss = 0.60702485\n",
      "Iteration 112, loss = 0.60636387\n",
      "Iteration 113, loss = 0.60569638\n",
      "Iteration 114, loss = 0.60504013\n",
      "Iteration 115, loss = 0.60437577\n",
      "Iteration 116, loss = 0.60372443\n",
      "Iteration 117, loss = 0.60306562\n",
      "Iteration 118, loss = 0.60241004\n",
      "Iteration 119, loss = 0.60175662\n",
      "Iteration 120, loss = 0.60110918\n",
      "Iteration 121, loss = 0.60046222\n",
      "Iteration 122, loss = 0.59981704\n",
      "Iteration 123, loss = 0.59916563\n",
      "Iteration 124, loss = 0.59852251\n",
      "Iteration 125, loss = 0.59788162\n",
      "Iteration 126, loss = 0.59724152\n",
      "Iteration 127, loss = 0.59660094\n",
      "Iteration 128, loss = 0.59595824\n",
      "Iteration 129, loss = 0.59532295\n",
      "Iteration 130, loss = 0.59469461\n",
      "Iteration 131, loss = 0.59405726\n",
      "Iteration 132, loss = 0.59342355\n",
      "Iteration 133, loss = 0.59280753\n",
      "Iteration 134, loss = 0.59217277\n",
      "Iteration 135, loss = 0.59154617\n",
      "Iteration 136, loss = 0.59093200\n",
      "Iteration 137, loss = 0.59031560\n",
      "Iteration 138, loss = 0.58969353\n",
      "Iteration 139, loss = 0.58908303\n",
      "Iteration 140, loss = 0.58846930\n",
      "Iteration 141, loss = 0.58786675\n",
      "Iteration 142, loss = 0.58725836\n",
      "Iteration 143, loss = 0.58665365\n",
      "Iteration 144, loss = 0.58604678\n",
      "Iteration 145, loss = 0.58544687\n",
      "Iteration 146, loss = 0.58484497\n",
      "Iteration 147, loss = 0.58426904\n",
      "Iteration 148, loss = 0.58365203\n",
      "Iteration 149, loss = 0.58306259\n",
      "Iteration 150, loss = 0.58247813\n",
      "Iteration 151, loss = 0.58188430\n",
      "Iteration 152, loss = 0.58130103\n",
      "Iteration 153, loss = 0.58071685\n",
      "Iteration 154, loss = 0.58014079\n",
      "Iteration 155, loss = 0.57956339\n",
      "Iteration 156, loss = 0.57898788\n",
      "Iteration 157, loss = 0.57841888\n",
      "Iteration 158, loss = 0.57784995\n",
      "Iteration 159, loss = 0.57728158\n",
      "Iteration 160, loss = 0.57672287\n",
      "Iteration 161, loss = 0.57615531\n",
      "Iteration 162, loss = 0.57559816\n",
      "Iteration 163, loss = 0.57504463\n",
      "Iteration 164, loss = 0.57449423\n",
      "Iteration 165, loss = 0.57394137\n",
      "Iteration 166, loss = 0.57340565\n",
      "Iteration 167, loss = 0.57285528\n",
      "Iteration 168, loss = 0.57232762\n",
      "Iteration 169, loss = 0.57177940\n",
      "Iteration 170, loss = 0.57124825\n",
      "Iteration 171, loss = 0.57071331\n",
      "Iteration 172, loss = 0.57018529\n",
      "Iteration 173, loss = 0.56966324\n",
      "Iteration 174, loss = 0.56914131\n",
      "Iteration 175, loss = 0.56862082\n",
      "Iteration 176, loss = 0.56810452\n",
      "Iteration 177, loss = 0.56758964\n",
      "Iteration 178, loss = 0.56707522\n",
      "Iteration 179, loss = 0.56656946\n",
      "Iteration 180, loss = 0.56607046\n",
      "Iteration 181, loss = 0.56556470\n",
      "Iteration 182, loss = 0.56506762\n",
      "Iteration 183, loss = 0.56457055\n",
      "Iteration 184, loss = 0.56408336\n",
      "Iteration 185, loss = 0.56359229\n",
      "Iteration 186, loss = 0.56310522\n",
      "Iteration 187, loss = 0.56262639\n",
      "Iteration 188, loss = 0.56214502\n",
      "Iteration 189, loss = 0.56167196\n",
      "Iteration 190, loss = 0.56120082\n",
      "Iteration 191, loss = 0.56072660\n",
      "Iteration 192, loss = 0.56027106\n",
      "Iteration 193, loss = 0.55980637\n",
      "Iteration 194, loss = 0.55935036\n",
      "Iteration 195, loss = 0.55889415\n",
      "Iteration 196, loss = 0.55844527\n",
      "Iteration 197, loss = 0.55799037\n",
      "Iteration 198, loss = 0.55755202\n",
      "Iteration 199, loss = 0.55711276\n",
      "Iteration 200, loss = 0.55667715\n",
      "Iteration 201, loss = 0.55623913\n",
      "Iteration 202, loss = 0.55580374\n",
      "Iteration 203, loss = 0.55537711\n",
      "Iteration 204, loss = 0.55495315\n",
      "Iteration 205, loss = 0.55453372\n",
      "Iteration 206, loss = 0.55412220\n",
      "Iteration 207, loss = 0.55372333\n",
      "Iteration 208, loss = 0.55330091\n",
      "Iteration 209, loss = 0.55288875\n",
      "Iteration 210, loss = 0.55248915\n",
      "Iteration 211, loss = 0.55208778\n",
      "Iteration 212, loss = 0.55169128\n",
      "Iteration 213, loss = 0.55129523\n",
      "Iteration 214, loss = 0.55090506\n",
      "Iteration 215, loss = 0.55051578\n",
      "Iteration 216, loss = 0.55013553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 217, loss = 0.54975079\n",
      "Iteration 218, loss = 0.54937124\n",
      "Iteration 219, loss = 0.54899651\n",
      "Iteration 220, loss = 0.54862674\n",
      "Iteration 221, loss = 0.54826004\n",
      "Iteration 222, loss = 0.54789188\n",
      "Iteration 223, loss = 0.54753073\n",
      "Iteration 224, loss = 0.54718265\n",
      "Iteration 225, loss = 0.54682382\n",
      "Iteration 226, loss = 0.54646882\n",
      "Iteration 227, loss = 0.54612587\n",
      "Iteration 228, loss = 0.54578324\n",
      "Iteration 229, loss = 0.54543979\n",
      "Iteration 230, loss = 0.54510352\n",
      "Iteration 231, loss = 0.54477766\n",
      "Iteration 232, loss = 0.54443932\n",
      "Iteration 233, loss = 0.54411762\n",
      "Iteration 234, loss = 0.54379204\n",
      "Iteration 235, loss = 0.54346510\n",
      "Iteration 236, loss = 0.54314578\n",
      "Iteration 237, loss = 0.54284645\n",
      "Iteration 238, loss = 0.54252358\n",
      "Iteration 239, loss = 0.54221525\n",
      "Iteration 240, loss = 0.54191561\n",
      "Iteration 241, loss = 0.54160450\n",
      "Iteration 242, loss = 0.54130441\n",
      "Iteration 243, loss = 0.54100451\n",
      "Iteration 244, loss = 0.54071020\n",
      "Iteration 245, loss = 0.54042450\n",
      "Iteration 246, loss = 0.54013119\n",
      "Iteration 247, loss = 0.53984469\n",
      "Iteration 248, loss = 0.53956211\n",
      "Iteration 249, loss = 0.53928589\n",
      "Iteration 250, loss = 0.53900343\n",
      "Iteration 251, loss = 0.53873163\n",
      "Iteration 252, loss = 0.53845904\n",
      "Iteration 253, loss = 0.53819602\n",
      "Iteration 254, loss = 0.53792497\n",
      "Iteration 255, loss = 0.53767583\n",
      "Iteration 256, loss = 0.53740604\n",
      "Iteration 257, loss = 0.53714091\n",
      "Iteration 258, loss = 0.53689012\n",
      "Iteration 259, loss = 0.53663782\n",
      "Iteration 260, loss = 0.53638912\n",
      "Iteration 261, loss = 0.53614574\n",
      "Iteration 262, loss = 0.53590311\n",
      "Iteration 263, loss = 0.53566175\n",
      "Iteration 264, loss = 0.53542168\n",
      "Iteration 265, loss = 0.53519289\n",
      "Iteration 266, loss = 0.53495705\n",
      "Iteration 267, loss = 0.53474337\n",
      "Iteration 268, loss = 0.53450598\n",
      "Iteration 269, loss = 0.53427812\n",
      "Iteration 270, loss = 0.53405689\n",
      "Iteration 271, loss = 0.53383746\n",
      "Iteration 272, loss = 0.53363109\n",
      "Iteration 273, loss = 0.53340711\n",
      "Iteration 274, loss = 0.53320063\n",
      "Iteration 275, loss = 0.53299253\n",
      "Iteration 276, loss = 0.53279754\n",
      "Iteration 277, loss = 0.53258289\n",
      "Iteration 278, loss = 0.53238183\n",
      "Iteration 279, loss = 0.53218237\n",
      "Iteration 280, loss = 0.53198512\n",
      "Iteration 281, loss = 0.53178665\n",
      "Iteration 282, loss = 0.53160240\n",
      "Iteration 283, loss = 0.53140927\n",
      "Iteration 284, loss = 0.53123046\n",
      "Iteration 285, loss = 0.53103881\n",
      "Iteration 286, loss = 0.53086043\n",
      "Iteration 287, loss = 0.53067579\n",
      "Iteration 288, loss = 0.53050468\n",
      "Iteration 289, loss = 0.53032194\n",
      "Iteration 290, loss = 0.53015475\n",
      "Iteration 291, loss = 0.52997701\n",
      "Iteration 292, loss = 0.52981361\n",
      "Iteration 293, loss = 0.52964160\n",
      "Iteration 294, loss = 0.52947924\n",
      "Iteration 295, loss = 0.52932058\n",
      "Iteration 296, loss = 0.52915252\n",
      "Iteration 297, loss = 0.52900174\n",
      "Iteration 298, loss = 0.52883786\n",
      "Iteration 299, loss = 0.52868639\n",
      "Iteration 300, loss = 0.52854098\n",
      "Iteration 301, loss = 0.52837845\n",
      "Iteration 302, loss = 0.52823406\n",
      "Iteration 303, loss = 0.52808288\n",
      "Iteration 304, loss = 0.52794765\n",
      "Iteration 305, loss = 0.52780789\n",
      "Iteration 306, loss = 0.52766713\n",
      "Iteration 307, loss = 0.52752166\n",
      "Iteration 308, loss = 0.52738442\n",
      "Iteration 309, loss = 0.52725149\n",
      "Iteration 310, loss = 0.52711818\n",
      "Iteration 311, loss = 0.52699118\n",
      "Iteration 312, loss = 0.52685082\n",
      "Iteration 313, loss = 0.52672503\n",
      "Iteration 314, loss = 0.52659073\n",
      "Iteration 315, loss = 0.52646646\n",
      "Iteration 316, loss = 0.52634374\n",
      "Iteration 317, loss = 0.52622083\n",
      "Iteration 318, loss = 0.52609850\n",
      "Iteration 319, loss = 0.52598273\n",
      "Iteration 320, loss = 0.52586053\n",
      "Iteration 321, loss = 0.52575199\n",
      "Iteration 322, loss = 0.52563672\n",
      "Iteration 323, loss = 0.52552516\n",
      "Iteration 324, loss = 0.52540898\n",
      "Iteration 325, loss = 0.52529941\n",
      "Iteration 326, loss = 0.52519126\n",
      "Iteration 327, loss = 0.52508400\n",
      "Iteration 328, loss = 0.52498076\n",
      "Iteration 329, loss = 0.52487142\n",
      "Iteration 330, loss = 0.52476735\n",
      "Iteration 331, loss = 0.52467019\n",
      "Iteration 332, loss = 0.52456713\n",
      "Iteration 333, loss = 0.52446610\n",
      "Iteration 334, loss = 0.52436924\n",
      "Iteration 335, loss = 0.52428812\n",
      "Iteration 336, loss = 0.52417699\n",
      "Iteration 337, loss = 0.52408642\n",
      "Iteration 338, loss = 0.52399925\n",
      "Iteration 339, loss = 0.52389628\n",
      "Iteration 340, loss = 0.52380249\n",
      "Iteration 341, loss = 0.52371568\n",
      "Iteration 342, loss = 0.52363070\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70040747\n",
      "Iteration 2, loss = 0.69683556\n",
      "Iteration 3, loss = 0.69366961\n",
      "Iteration 4, loss = 0.69086430\n",
      "Iteration 5, loss = 0.68841734\n",
      "Iteration 6, loss = 0.68625570\n",
      "Iteration 7, loss = 0.68435545\n",
      "Iteration 8, loss = 0.68267156\n",
      "Iteration 9, loss = 0.68117277\n",
      "Iteration 10, loss = 0.67983796\n",
      "Iteration 11, loss = 0.67864088\n",
      "Iteration 12, loss = 0.67754862\n",
      "Iteration 13, loss = 0.67652540\n",
      "Iteration 14, loss = 0.67557832\n",
      "Iteration 15, loss = 0.67469045\n",
      "Iteration 16, loss = 0.67384217\n",
      "Iteration 17, loss = 0.67301906\n",
      "Iteration 18, loss = 0.67222884\n",
      "Iteration 19, loss = 0.67144465\n",
      "Iteration 20, loss = 0.67068061\n",
      "Iteration 21, loss = 0.66992522\n",
      "Iteration 22, loss = 0.66918143\n",
      "Iteration 23, loss = 0.66843831\n",
      "Iteration 24, loss = 0.66769720\n",
      "Iteration 25, loss = 0.66696225\n",
      "Iteration 26, loss = 0.66622739\n",
      "Iteration 27, loss = 0.66548744\n",
      "Iteration 28, loss = 0.66475339\n",
      "Iteration 29, loss = 0.66402113\n",
      "Iteration 30, loss = 0.66328713\n",
      "Iteration 31, loss = 0.66255448\n",
      "Iteration 32, loss = 0.66182874\n",
      "Iteration 33, loss = 0.66109211\n",
      "Iteration 34, loss = 0.66036147\n",
      "Iteration 35, loss = 0.65964060\n",
      "Iteration 36, loss = 0.65890176\n",
      "Iteration 37, loss = 0.65817834\n",
      "Iteration 38, loss = 0.65744647\n",
      "Iteration 39, loss = 0.65672206\n",
      "Iteration 40, loss = 0.65599260\n",
      "Iteration 41, loss = 0.65526628\n",
      "Iteration 42, loss = 0.65453881\n",
      "Iteration 43, loss = 0.65381260\n",
      "Iteration 44, loss = 0.65308528\n",
      "Iteration 45, loss = 0.65236355\n",
      "Iteration 46, loss = 0.65163908\n",
      "Iteration 47, loss = 0.65091085\n",
      "Iteration 48, loss = 0.65018071\n",
      "Iteration 49, loss = 0.64946123\n",
      "Iteration 50, loss = 0.64874692\n",
      "Iteration 51, loss = 0.64802558\n",
      "Iteration 52, loss = 0.64730689\n",
      "Iteration 53, loss = 0.64658508\n",
      "Iteration 54, loss = 0.64587402\n",
      "Iteration 55, loss = 0.64515380\n",
      "Iteration 56, loss = 0.64443815\n",
      "Iteration 57, loss = 0.64372357\n",
      "Iteration 58, loss = 0.64301084\n",
      "Iteration 59, loss = 0.64229715\n",
      "Iteration 60, loss = 0.64158934\n",
      "Iteration 61, loss = 0.64088001\n",
      "Iteration 62, loss = 0.64017420\n",
      "Iteration 63, loss = 0.63945471\n",
      "Iteration 64, loss = 0.63874748\n",
      "Iteration 65, loss = 0.63803893\n",
      "Iteration 66, loss = 0.63732860\n",
      "Iteration 67, loss = 0.63662198\n",
      "Iteration 68, loss = 0.63591508\n",
      "Iteration 69, loss = 0.63520705\n",
      "Iteration 70, loss = 0.63450283\n",
      "Iteration 71, loss = 0.63380173\n",
      "Iteration 72, loss = 0.63310108\n",
      "Iteration 73, loss = 0.63241404\n",
      "Iteration 74, loss = 0.63170491\n",
      "Iteration 75, loss = 0.63100887\n",
      "Iteration 76, loss = 0.63031599\n",
      "Iteration 77, loss = 0.62961593\n",
      "Iteration 78, loss = 0.62891795\n",
      "Iteration 79, loss = 0.62822748\n",
      "Iteration 80, loss = 0.62753300\n",
      "Iteration 81, loss = 0.62684329\n",
      "Iteration 82, loss = 0.62615490\n",
      "Iteration 83, loss = 0.62547185\n",
      "Iteration 84, loss = 0.62477909\n",
      "Iteration 85, loss = 0.62409669\n",
      "Iteration 86, loss = 0.62342241\n",
      "Iteration 87, loss = 0.62274120\n",
      "Iteration 88, loss = 0.62205267\n",
      "Iteration 89, loss = 0.62137910\n",
      "Iteration 90, loss = 0.62070428\n",
      "Iteration 91, loss = 0.62002881\n",
      "Iteration 92, loss = 0.61935537\n",
      "Iteration 93, loss = 0.61868362\n",
      "Iteration 94, loss = 0.61801565\n",
      "Iteration 95, loss = 0.61735082\n",
      "Iteration 96, loss = 0.61668024\n",
      "Iteration 97, loss = 0.61601843\n",
      "Iteration 98, loss = 0.61535379\n",
      "Iteration 99, loss = 0.61469509\n",
      "Iteration 100, loss = 0.61403469\n",
      "Iteration 101, loss = 0.61337448\n",
      "Iteration 102, loss = 0.61271502\n",
      "Iteration 103, loss = 0.61205803\n",
      "Iteration 104, loss = 0.61140878\n",
      "Iteration 105, loss = 0.61075736\n",
      "Iteration 106, loss = 0.61010514\n",
      "Iteration 107, loss = 0.60945654\n",
      "Iteration 108, loss = 0.60880785\n",
      "Iteration 109, loss = 0.60816372\n",
      "Iteration 110, loss = 0.60752359\n",
      "Iteration 111, loss = 0.60687944\n",
      "Iteration 112, loss = 0.60623970\n",
      "Iteration 113, loss = 0.60559746\n",
      "Iteration 114, loss = 0.60495969\n",
      "Iteration 115, loss = 0.60432670\n",
      "Iteration 116, loss = 0.60369125\n",
      "Iteration 117, loss = 0.60305626\n",
      "Iteration 118, loss = 0.60242668\n",
      "Iteration 119, loss = 0.60179649\n",
      "Iteration 120, loss = 0.60116683\n",
      "Iteration 121, loss = 0.60053918\n",
      "Iteration 122, loss = 0.59991753\n",
      "Iteration 123, loss = 0.59928865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 124, loss = 0.59866769\n",
      "Iteration 125, loss = 0.59804382\n",
      "Iteration 126, loss = 0.59742510\n",
      "Iteration 127, loss = 0.59680470\n",
      "Iteration 128, loss = 0.59618458\n",
      "Iteration 129, loss = 0.59556757\n",
      "Iteration 130, loss = 0.59496318\n",
      "Iteration 131, loss = 0.59434271\n",
      "Iteration 132, loss = 0.59372955\n",
      "Iteration 133, loss = 0.59312870\n",
      "Iteration 134, loss = 0.59251532\n",
      "Iteration 135, loss = 0.59191037\n",
      "Iteration 136, loss = 0.59131058\n",
      "Iteration 137, loss = 0.59072167\n",
      "Iteration 138, loss = 0.59011592\n",
      "Iteration 139, loss = 0.58952245\n",
      "Iteration 140, loss = 0.58892973\n",
      "Iteration 141, loss = 0.58834594\n",
      "Iteration 142, loss = 0.58775759\n",
      "Iteration 143, loss = 0.58717339\n",
      "Iteration 144, loss = 0.58658672\n",
      "Iteration 145, loss = 0.58600790\n",
      "Iteration 146, loss = 0.58542515\n",
      "Iteration 147, loss = 0.58487354\n",
      "Iteration 148, loss = 0.58427954\n",
      "Iteration 149, loss = 0.58370734\n",
      "Iteration 150, loss = 0.58313702\n",
      "Iteration 151, loss = 0.58256718\n",
      "Iteration 152, loss = 0.58200469\n",
      "Iteration 153, loss = 0.58143781\n",
      "Iteration 154, loss = 0.58088085\n",
      "Iteration 155, loss = 0.58031850\n",
      "Iteration 156, loss = 0.57976150\n",
      "Iteration 157, loss = 0.57921058\n",
      "Iteration 158, loss = 0.57865903\n",
      "Iteration 159, loss = 0.57810833\n",
      "Iteration 160, loss = 0.57756307\n",
      "Iteration 161, loss = 0.57701445\n",
      "Iteration 162, loss = 0.57647436\n",
      "Iteration 163, loss = 0.57593521\n",
      "Iteration 164, loss = 0.57540562\n",
      "Iteration 165, loss = 0.57486154\n",
      "Iteration 166, loss = 0.57434039\n",
      "Iteration 167, loss = 0.57380650\n",
      "Iteration 168, loss = 0.57328993\n",
      "Iteration 169, loss = 0.57275825\n",
      "Iteration 170, loss = 0.57224548\n",
      "Iteration 171, loss = 0.57172117\n",
      "Iteration 172, loss = 0.57120634\n",
      "Iteration 173, loss = 0.57069957\n",
      "Iteration 174, loss = 0.57019314\n",
      "Iteration 175, loss = 0.56968506\n",
      "Iteration 176, loss = 0.56918079\n",
      "Iteration 177, loss = 0.56868208\n",
      "Iteration 178, loss = 0.56818633\n",
      "Iteration 179, loss = 0.56769087\n",
      "Iteration 180, loss = 0.56720792\n",
      "Iteration 181, loss = 0.56671792\n",
      "Iteration 182, loss = 0.56623173\n",
      "Iteration 183, loss = 0.56574959\n",
      "Iteration 184, loss = 0.56527796\n",
      "Iteration 185, loss = 0.56480111\n",
      "Iteration 186, loss = 0.56432703\n",
      "Iteration 187, loss = 0.56386147\n",
      "Iteration 188, loss = 0.56339003\n",
      "Iteration 189, loss = 0.56292834\n",
      "Iteration 190, loss = 0.56247079\n",
      "Iteration 191, loss = 0.56200877\n",
      "Iteration 192, loss = 0.56156312\n",
      "Iteration 193, loss = 0.56111106\n",
      "Iteration 194, loss = 0.56066704\n",
      "Iteration 195, loss = 0.56022147\n",
      "Iteration 196, loss = 0.55978456\n",
      "Iteration 197, loss = 0.55933800\n",
      "Iteration 198, loss = 0.55891053\n",
      "Iteration 199, loss = 0.55848092\n",
      "Iteration 200, loss = 0.55805670\n",
      "Iteration 201, loss = 0.55762704\n",
      "Iteration 202, loss = 0.55720193\n",
      "Iteration 203, loss = 0.55678479\n",
      "Iteration 204, loss = 0.55636539\n",
      "Iteration 205, loss = 0.55595877\n",
      "Iteration 206, loss = 0.55555458\n",
      "Iteration 207, loss = 0.55516336\n",
      "Iteration 208, loss = 0.55475221\n",
      "Iteration 209, loss = 0.55434573\n",
      "Iteration 210, loss = 0.55395874\n",
      "Iteration 211, loss = 0.55356051\n",
      "Iteration 212, loss = 0.55317060\n",
      "Iteration 213, loss = 0.55278177\n",
      "Iteration 214, loss = 0.55239975\n",
      "Iteration 215, loss = 0.55201958\n",
      "Iteration 216, loss = 0.55164674\n",
      "Iteration 217, loss = 0.55126772\n",
      "Iteration 218, loss = 0.55089364\n",
      "Iteration 219, loss = 0.55052736\n",
      "Iteration 220, loss = 0.55016141\n",
      "Iteration 221, loss = 0.54980264\n",
      "Iteration 222, loss = 0.54944205\n",
      "Iteration 223, loss = 0.54908722\n",
      "Iteration 224, loss = 0.54873986\n",
      "Iteration 225, loss = 0.54839213\n",
      "Iteration 226, loss = 0.54804077\n",
      "Iteration 227, loss = 0.54770101\n",
      "Iteration 228, loss = 0.54736719\n",
      "Iteration 229, loss = 0.54702413\n",
      "Iteration 230, loss = 0.54669059\n",
      "Iteration 231, loss = 0.54637129\n",
      "Iteration 232, loss = 0.54603844\n",
      "Iteration 233, loss = 0.54571881\n",
      "Iteration 234, loss = 0.54539140\n",
      "Iteration 235, loss = 0.54506987\n",
      "Iteration 236, loss = 0.54475781\n",
      "Iteration 237, loss = 0.54445534\n",
      "Iteration 238, loss = 0.54414094\n",
      "Iteration 239, loss = 0.54383679\n",
      "Iteration 240, loss = 0.54353625\n",
      "Iteration 241, loss = 0.54322915\n",
      "Iteration 242, loss = 0.54292766\n",
      "Iteration 243, loss = 0.54263872\n",
      "Iteration 244, loss = 0.54233925\n",
      "Iteration 245, loss = 0.54205527\n",
      "Iteration 246, loss = 0.54176212\n",
      "Iteration 247, loss = 0.54147660\n",
      "Iteration 248, loss = 0.54119368\n",
      "Iteration 249, loss = 0.54092180\n",
      "Iteration 250, loss = 0.54063853\n",
      "Iteration 251, loss = 0.54037017\n",
      "Iteration 252, loss = 0.54009389\n",
      "Iteration 253, loss = 0.53982980\n",
      "Iteration 254, loss = 0.53955992\n",
      "Iteration 255, loss = 0.53930918\n",
      "Iteration 256, loss = 0.53904021\n",
      "Iteration 257, loss = 0.53877675\n",
      "Iteration 258, loss = 0.53852267\n",
      "Iteration 259, loss = 0.53827499\n",
      "Iteration 260, loss = 0.53802251\n",
      "Iteration 261, loss = 0.53777682\n",
      "Iteration 262, loss = 0.53753161\n",
      "Iteration 263, loss = 0.53729395\n",
      "Iteration 264, loss = 0.53705014\n",
      "Iteration 265, loss = 0.53681829\n",
      "Iteration 266, loss = 0.53658160\n",
      "Iteration 267, loss = 0.53636283\n",
      "Iteration 268, loss = 0.53612604\n",
      "Iteration 269, loss = 0.53589797\n",
      "Iteration 270, loss = 0.53568026\n",
      "Iteration 271, loss = 0.53545355\n",
      "Iteration 272, loss = 0.53524338\n",
      "Iteration 273, loss = 0.53501853\n",
      "Iteration 274, loss = 0.53480763\n",
      "Iteration 275, loss = 0.53459876\n",
      "Iteration 276, loss = 0.53439772\n",
      "Iteration 277, loss = 0.53418241\n",
      "Iteration 278, loss = 0.53397542\n",
      "Iteration 279, loss = 0.53377852\n",
      "Iteration 280, loss = 0.53357981\n",
      "Iteration 281, loss = 0.53337529\n",
      "Iteration 282, loss = 0.53318493\n",
      "Iteration 283, loss = 0.53299628\n",
      "Iteration 284, loss = 0.53280929\n",
      "Iteration 285, loss = 0.53261547\n",
      "Iteration 286, loss = 0.53243365\n",
      "Iteration 287, loss = 0.53224571\n",
      "Iteration 288, loss = 0.53207027\n",
      "Iteration 289, loss = 0.53188802\n",
      "Iteration 290, loss = 0.53171589\n",
      "Iteration 291, loss = 0.53153604\n",
      "Iteration 292, loss = 0.53136483\n",
      "Iteration 293, loss = 0.53119535\n",
      "Iteration 294, loss = 0.53102805\n",
      "Iteration 295, loss = 0.53086639\n",
      "Iteration 296, loss = 0.53069455\n",
      "Iteration 297, loss = 0.53054229\n",
      "Iteration 298, loss = 0.53037044\n",
      "Iteration 299, loss = 0.53021318\n",
      "Iteration 300, loss = 0.53007306\n",
      "Iteration 301, loss = 0.52990355\n",
      "Iteration 302, loss = 0.52975776\n",
      "Iteration 303, loss = 0.52960275\n",
      "Iteration 304, loss = 0.52946325\n",
      "Iteration 305, loss = 0.52931359\n",
      "Iteration 306, loss = 0.52917333\n",
      "Iteration 307, loss = 0.52902772\n",
      "Iteration 308, loss = 0.52888271\n",
      "Iteration 309, loss = 0.52874909\n",
      "Iteration 310, loss = 0.52861058\n",
      "Iteration 311, loss = 0.52848539\n",
      "Iteration 312, loss = 0.52833850\n",
      "Iteration 313, loss = 0.52820469\n",
      "Iteration 314, loss = 0.52807298\n",
      "Iteration 315, loss = 0.52794455\n",
      "Iteration 316, loss = 0.52781871\n",
      "Iteration 317, loss = 0.52769530\n",
      "Iteration 318, loss = 0.52757015\n",
      "Iteration 319, loss = 0.52744908\n",
      "Iteration 320, loss = 0.52732122\n",
      "Iteration 321, loss = 0.52720570\n",
      "Iteration 322, loss = 0.52708979\n",
      "Iteration 323, loss = 0.52697647\n",
      "Iteration 324, loss = 0.52685525\n",
      "Iteration 325, loss = 0.52674020\n",
      "Iteration 326, loss = 0.52662864\n",
      "Iteration 327, loss = 0.52651772\n",
      "Iteration 328, loss = 0.52640991\n",
      "Iteration 329, loss = 0.52629927\n",
      "Iteration 330, loss = 0.52619285\n",
      "Iteration 331, loss = 0.52608417\n",
      "Iteration 332, loss = 0.52598297\n",
      "Iteration 333, loss = 0.52587499\n",
      "Iteration 334, loss = 0.52577312\n",
      "Iteration 335, loss = 0.52568640\n",
      "Iteration 336, loss = 0.52557759\n",
      "Iteration 337, loss = 0.52547935\n",
      "Iteration 338, loss = 0.52538552\n",
      "Iteration 339, loss = 0.52528180\n",
      "Iteration 340, loss = 0.52518576\n",
      "Iteration 341, loss = 0.52509599\n",
      "Iteration 342, loss = 0.52500870\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70114198\n",
      "Iteration 2, loss = 0.69715881\n",
      "Iteration 3, loss = 0.69356868\n",
      "Iteration 4, loss = 0.69035444\n",
      "Iteration 5, loss = 0.68752324\n",
      "Iteration 6, loss = 0.68500501\n",
      "Iteration 7, loss = 0.68278471\n",
      "Iteration 8, loss = 0.68082301\n",
      "Iteration 9, loss = 0.67907611\n",
      "Iteration 10, loss = 0.67753049\n",
      "Iteration 11, loss = 0.67615747\n",
      "Iteration 12, loss = 0.67491496\n",
      "Iteration 13, loss = 0.67378198\n",
      "Iteration 14, loss = 0.67274785\n",
      "Iteration 15, loss = 0.67179185\n",
      "Iteration 16, loss = 0.67089878\n",
      "Iteration 17, loss = 0.67005281\n",
      "Iteration 18, loss = 0.66925247\n",
      "Iteration 19, loss = 0.66846453\n",
      "Iteration 20, loss = 0.66770718\n",
      "Iteration 21, loss = 0.66696849\n",
      "Iteration 22, loss = 0.66624568\n",
      "Iteration 23, loss = 0.66552869\n",
      "Iteration 24, loss = 0.66481631\n",
      "Iteration 25, loss = 0.66411466\n",
      "Iteration 26, loss = 0.66341193\n",
      "Iteration 27, loss = 0.66270855\n",
      "Iteration 28, loss = 0.66201001\n",
      "Iteration 29, loss = 0.66131384\n",
      "Iteration 30, loss = 0.66061674\n",
      "Iteration 31, loss = 0.65992090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.65923450\n",
      "Iteration 33, loss = 0.65853207\n",
      "Iteration 34, loss = 0.65783774\n",
      "Iteration 35, loss = 0.65714724\n",
      "Iteration 36, loss = 0.65645009\n",
      "Iteration 37, loss = 0.65575874\n",
      "Iteration 38, loss = 0.65505967\n",
      "Iteration 39, loss = 0.65436974\n",
      "Iteration 40, loss = 0.65367052\n",
      "Iteration 41, loss = 0.65297548\n",
      "Iteration 42, loss = 0.65228100\n",
      "Iteration 43, loss = 0.65158449\n",
      "Iteration 44, loss = 0.65088924\n",
      "Iteration 45, loss = 0.65020328\n",
      "Iteration 46, loss = 0.64950684\n",
      "Iteration 47, loss = 0.64881072\n",
      "Iteration 48, loss = 0.64811202\n",
      "Iteration 49, loss = 0.64742151\n",
      "Iteration 50, loss = 0.64673737\n",
      "Iteration 51, loss = 0.64604590\n",
      "Iteration 52, loss = 0.64535808\n",
      "Iteration 53, loss = 0.64466476\n",
      "Iteration 54, loss = 0.64398268\n",
      "Iteration 55, loss = 0.64329342\n",
      "Iteration 56, loss = 0.64260659\n",
      "Iteration 57, loss = 0.64192075\n",
      "Iteration 58, loss = 0.64124089\n",
      "Iteration 59, loss = 0.64055361\n",
      "Iteration 60, loss = 0.63987776\n",
      "Iteration 61, loss = 0.63919191\n",
      "Iteration 62, loss = 0.63851310\n",
      "Iteration 63, loss = 0.63782359\n",
      "Iteration 64, loss = 0.63714339\n",
      "Iteration 65, loss = 0.63646348\n",
      "Iteration 66, loss = 0.63577833\n",
      "Iteration 67, loss = 0.63509919\n",
      "Iteration 68, loss = 0.63441895\n",
      "Iteration 69, loss = 0.63374044\n",
      "Iteration 70, loss = 0.63306258\n",
      "Iteration 71, loss = 0.63238756\n",
      "Iteration 72, loss = 0.63171290\n",
      "Iteration 73, loss = 0.63104935\n",
      "Iteration 74, loss = 0.63037594\n",
      "Iteration 75, loss = 0.62970004\n",
      "Iteration 76, loss = 0.62903759\n",
      "Iteration 77, loss = 0.62836161\n",
      "Iteration 78, loss = 0.62769253\n",
      "Iteration 79, loss = 0.62702893\n",
      "Iteration 80, loss = 0.62636035\n",
      "Iteration 81, loss = 0.62569704\n",
      "Iteration 82, loss = 0.62503763\n",
      "Iteration 83, loss = 0.62437738\n",
      "Iteration 84, loss = 0.62371479\n",
      "Iteration 85, loss = 0.62305894\n",
      "Iteration 86, loss = 0.62240984\n",
      "Iteration 87, loss = 0.62175624\n",
      "Iteration 88, loss = 0.62109813\n",
      "Iteration 89, loss = 0.62045102\n",
      "Iteration 90, loss = 0.61980690\n",
      "Iteration 91, loss = 0.61915729\n",
      "Iteration 92, loss = 0.61850963\n",
      "Iteration 93, loss = 0.61786537\n",
      "Iteration 94, loss = 0.61722524\n",
      "Iteration 95, loss = 0.61658765\n",
      "Iteration 96, loss = 0.61594284\n",
      "Iteration 97, loss = 0.61530638\n",
      "Iteration 98, loss = 0.61466747\n",
      "Iteration 99, loss = 0.61403978\n",
      "Iteration 100, loss = 0.61340028\n",
      "Iteration 101, loss = 0.61276549\n",
      "Iteration 102, loss = 0.61213468\n",
      "Iteration 103, loss = 0.61150252\n",
      "Iteration 104, loss = 0.61087381\n",
      "Iteration 105, loss = 0.61025176\n",
      "Iteration 106, loss = 0.60961813\n",
      "Iteration 107, loss = 0.60899456\n",
      "Iteration 108, loss = 0.60836983\n",
      "Iteration 109, loss = 0.60774901\n",
      "Iteration 110, loss = 0.60713334\n",
      "Iteration 111, loss = 0.60650969\n",
      "Iteration 112, loss = 0.60589286\n",
      "Iteration 113, loss = 0.60527402\n",
      "Iteration 114, loss = 0.60466059\n",
      "Iteration 115, loss = 0.60405108\n",
      "Iteration 116, loss = 0.60343733\n",
      "Iteration 117, loss = 0.60282738\n",
      "Iteration 118, loss = 0.60222189\n",
      "Iteration 119, loss = 0.60161690\n",
      "Iteration 120, loss = 0.60101261\n",
      "Iteration 121, loss = 0.60040659\n",
      "Iteration 122, loss = 0.59980744\n",
      "Iteration 123, loss = 0.59921042\n",
      "Iteration 124, loss = 0.59861346\n",
      "Iteration 125, loss = 0.59801717\n",
      "Iteration 126, loss = 0.59741999\n",
      "Iteration 127, loss = 0.59682844\n",
      "Iteration 128, loss = 0.59623340\n",
      "Iteration 129, loss = 0.59564268\n",
      "Iteration 130, loss = 0.59506368\n",
      "Iteration 131, loss = 0.59447281\n",
      "Iteration 132, loss = 0.59388418\n",
      "Iteration 133, loss = 0.59330902\n",
      "Iteration 134, loss = 0.59272349\n",
      "Iteration 135, loss = 0.59214321\n",
      "Iteration 136, loss = 0.59156763\n",
      "Iteration 137, loss = 0.59100019\n",
      "Iteration 138, loss = 0.59042573\n",
      "Iteration 139, loss = 0.58985512\n",
      "Iteration 140, loss = 0.58929100\n",
      "Iteration 141, loss = 0.58872866\n",
      "Iteration 142, loss = 0.58816556\n",
      "Iteration 143, loss = 0.58760417\n",
      "Iteration 144, loss = 0.58704402\n",
      "Iteration 145, loss = 0.58648427\n",
      "Iteration 146, loss = 0.58593003\n",
      "Iteration 147, loss = 0.58539947\n",
      "Iteration 148, loss = 0.58483153\n",
      "Iteration 149, loss = 0.58428053\n",
      "Iteration 150, loss = 0.58374170\n",
      "Iteration 151, loss = 0.58318835\n",
      "Iteration 152, loss = 0.58264874\n",
      "Iteration 153, loss = 0.58210709\n",
      "Iteration 154, loss = 0.58157520\n",
      "Iteration 155, loss = 0.58103413\n",
      "Iteration 156, loss = 0.58050142\n",
      "Iteration 157, loss = 0.57997268\n",
      "Iteration 158, loss = 0.57944699\n",
      "Iteration 159, loss = 0.57891179\n",
      "Iteration 160, loss = 0.57839078\n",
      "Iteration 161, loss = 0.57786438\n",
      "Iteration 162, loss = 0.57734679\n",
      "Iteration 163, loss = 0.57682849\n",
      "Iteration 164, loss = 0.57631856\n",
      "Iteration 165, loss = 0.57579902\n",
      "Iteration 166, loss = 0.57529647\n",
      "Iteration 167, loss = 0.57478219\n",
      "Iteration 168, loss = 0.57428636\n",
      "Iteration 169, loss = 0.57377464\n",
      "Iteration 170, loss = 0.57328083\n",
      "Iteration 171, loss = 0.57277657\n",
      "Iteration 172, loss = 0.57227786\n",
      "Iteration 173, loss = 0.57178829\n",
      "Iteration 174, loss = 0.57129720\n",
      "Iteration 175, loss = 0.57080963\n",
      "Iteration 176, loss = 0.57032355\n",
      "Iteration 177, loss = 0.56984071\n",
      "Iteration 178, loss = 0.56936543\n",
      "Iteration 179, loss = 0.56888715\n",
      "Iteration 180, loss = 0.56842161\n",
      "Iteration 181, loss = 0.56794826\n",
      "Iteration 182, loss = 0.56747692\n",
      "Iteration 183, loss = 0.56701073\n",
      "Iteration 184, loss = 0.56655971\n",
      "Iteration 185, loss = 0.56609411\n",
      "Iteration 186, loss = 0.56563883\n",
      "Iteration 187, loss = 0.56518971\n",
      "Iteration 188, loss = 0.56473569\n",
      "Iteration 189, loss = 0.56429238\n",
      "Iteration 190, loss = 0.56385121\n",
      "Iteration 191, loss = 0.56340336\n",
      "Iteration 192, loss = 0.56297533\n",
      "Iteration 193, loss = 0.56253726\n",
      "Iteration 194, loss = 0.56210383\n",
      "Iteration 195, loss = 0.56167365\n",
      "Iteration 196, loss = 0.56124726\n",
      "Iteration 197, loss = 0.56082249\n",
      "Iteration 198, loss = 0.56040498\n",
      "Iteration 199, loss = 0.55998983\n",
      "Iteration 200, loss = 0.55957842\n",
      "Iteration 201, loss = 0.55916320\n",
      "Iteration 202, loss = 0.55875621\n",
      "Iteration 203, loss = 0.55834782\n",
      "Iteration 204, loss = 0.55794152\n",
      "Iteration 205, loss = 0.55754567\n",
      "Iteration 206, loss = 0.55715028\n",
      "Iteration 207, loss = 0.55677185\n",
      "Iteration 208, loss = 0.55637195\n",
      "Iteration 209, loss = 0.55597754\n",
      "Iteration 210, loss = 0.55559529\n",
      "Iteration 211, loss = 0.55521296\n",
      "Iteration 212, loss = 0.55483618\n",
      "Iteration 213, loss = 0.55445544\n",
      "Iteration 214, loss = 0.55408119\n",
      "Iteration 215, loss = 0.55371150\n",
      "Iteration 216, loss = 0.55335076\n",
      "Iteration 217, loss = 0.55298180\n",
      "Iteration 218, loss = 0.55261744\n",
      "Iteration 219, loss = 0.55226246\n",
      "Iteration 220, loss = 0.55190637\n",
      "Iteration 221, loss = 0.55155803\n",
      "Iteration 222, loss = 0.55120655\n",
      "Iteration 223, loss = 0.55086218\n",
      "Iteration 224, loss = 0.55052674\n",
      "Iteration 225, loss = 0.55018737\n",
      "Iteration 226, loss = 0.54984383\n",
      "Iteration 227, loss = 0.54951429\n",
      "Iteration 228, loss = 0.54918725\n",
      "Iteration 229, loss = 0.54885371\n",
      "Iteration 230, loss = 0.54853018\n",
      "Iteration 231, loss = 0.54821703\n",
      "Iteration 232, loss = 0.54789074\n",
      "Iteration 233, loss = 0.54757985\n",
      "Iteration 234, loss = 0.54725960\n",
      "Iteration 235, loss = 0.54694966\n",
      "Iteration 236, loss = 0.54663998\n",
      "Iteration 237, loss = 0.54634179\n",
      "Iteration 238, loss = 0.54603877\n",
      "Iteration 239, loss = 0.54574236\n",
      "Iteration 240, loss = 0.54544552\n",
      "Iteration 241, loss = 0.54514873\n",
      "Iteration 242, loss = 0.54485247\n",
      "Iteration 243, loss = 0.54456985\n",
      "Iteration 244, loss = 0.54427728\n",
      "Iteration 245, loss = 0.54399676\n",
      "Iteration 246, loss = 0.54370921\n",
      "Iteration 247, loss = 0.54343010\n",
      "Iteration 248, loss = 0.54314926\n",
      "Iteration 249, loss = 0.54288196\n",
      "Iteration 250, loss = 0.54260154\n",
      "Iteration 251, loss = 0.54233583\n",
      "Iteration 252, loss = 0.54206525\n",
      "Iteration 253, loss = 0.54180643\n",
      "Iteration 254, loss = 0.54153888\n",
      "Iteration 255, loss = 0.54128536\n",
      "Iteration 256, loss = 0.54102903\n",
      "Iteration 257, loss = 0.54076675\n",
      "Iteration 258, loss = 0.54051580\n",
      "Iteration 259, loss = 0.54026915\n",
      "Iteration 260, loss = 0.54002408\n",
      "Iteration 261, loss = 0.53977978\n",
      "Iteration 262, loss = 0.53954145\n",
      "Iteration 263, loss = 0.53930186\n",
      "Iteration 264, loss = 0.53905908\n",
      "Iteration 265, loss = 0.53883102\n",
      "Iteration 266, loss = 0.53859707\n",
      "Iteration 267, loss = 0.53837579\n",
      "Iteration 268, loss = 0.53813892\n",
      "Iteration 269, loss = 0.53791680\n",
      "Iteration 270, loss = 0.53769796\n",
      "Iteration 271, loss = 0.53747469\n",
      "Iteration 272, loss = 0.53726326\n",
      "Iteration 273, loss = 0.53704051\n",
      "Iteration 274, loss = 0.53682900\n",
      "Iteration 275, loss = 0.53662269\n",
      "Iteration 276, loss = 0.53642414\n",
      "Iteration 277, loss = 0.53620657\n",
      "Iteration 278, loss = 0.53600271\n",
      "Iteration 279, loss = 0.53580434\n",
      "Iteration 280, loss = 0.53560864\n",
      "Iteration 281, loss = 0.53540443\n",
      "Iteration 282, loss = 0.53521465\n",
      "Iteration 283, loss = 0.53502605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 284, loss = 0.53484115\n",
      "Iteration 285, loss = 0.53464664\n",
      "Iteration 286, loss = 0.53446336\n",
      "Iteration 287, loss = 0.53428157\n",
      "Iteration 288, loss = 0.53410290\n",
      "Iteration 289, loss = 0.53392004\n",
      "Iteration 290, loss = 0.53375212\n",
      "Iteration 291, loss = 0.53356761\n",
      "Iteration 292, loss = 0.53339682\n",
      "Iteration 293, loss = 0.53323177\n",
      "Iteration 294, loss = 0.53305677\n",
      "Iteration 295, loss = 0.53289950\n",
      "Iteration 296, loss = 0.53272893\n",
      "Iteration 297, loss = 0.53257046\n",
      "Iteration 298, loss = 0.53240269\n",
      "Iteration 299, loss = 0.53224485\n",
      "Iteration 300, loss = 0.53210036\n",
      "Iteration 301, loss = 0.53193946\n",
      "Iteration 302, loss = 0.53178573\n",
      "Iteration 303, loss = 0.53163512\n",
      "Iteration 304, loss = 0.53149078\n",
      "Iteration 305, loss = 0.53133787\n",
      "Iteration 306, loss = 0.53119707\n",
      "Iteration 307, loss = 0.53105400\n",
      "Iteration 308, loss = 0.53091032\n",
      "Iteration 309, loss = 0.53077349\n",
      "Iteration 310, loss = 0.53063483\n",
      "Iteration 311, loss = 0.53050154\n",
      "Iteration 312, loss = 0.53036194\n",
      "Iteration 313, loss = 0.53022332\n",
      "Iteration 314, loss = 0.53009001\n",
      "Iteration 315, loss = 0.52995751\n",
      "Iteration 316, loss = 0.52983080\n",
      "Iteration 317, loss = 0.52970728\n",
      "Iteration 318, loss = 0.52958125\n",
      "Iteration 319, loss = 0.52945488\n",
      "Iteration 320, loss = 0.52932654\n",
      "Iteration 321, loss = 0.52920702\n",
      "Iteration 322, loss = 0.52908567\n",
      "Iteration 323, loss = 0.52897193\n",
      "Iteration 324, loss = 0.52884808\n",
      "Iteration 325, loss = 0.52873163\n",
      "Iteration 326, loss = 0.52861660\n",
      "Iteration 327, loss = 0.52850799\n",
      "Iteration 328, loss = 0.52839509\n",
      "Iteration 329, loss = 0.52828275\n",
      "Iteration 330, loss = 0.52817110\n",
      "Iteration 331, loss = 0.52806200\n",
      "Iteration 332, loss = 0.52796287\n",
      "Iteration 333, loss = 0.52785238\n",
      "Iteration 334, loss = 0.52774635\n",
      "Iteration 335, loss = 0.52765605\n",
      "Iteration 336, loss = 0.52754558\n",
      "Iteration 337, loss = 0.52744740\n",
      "Iteration 338, loss = 0.52734800\n",
      "Iteration 339, loss = 0.52724310\n",
      "Iteration 340, loss = 0.52714551\n",
      "Iteration 341, loss = 0.52705344\n",
      "Iteration 342, loss = 0.52696847\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70110800\n",
      "Iteration 2, loss = 0.69700137\n",
      "Iteration 3, loss = 0.69330123\n",
      "Iteration 4, loss = 0.68998503\n",
      "Iteration 5, loss = 0.68705935\n",
      "Iteration 6, loss = 0.68445995\n",
      "Iteration 7, loss = 0.68216629\n",
      "Iteration 8, loss = 0.68014591\n",
      "Iteration 9, loss = 0.67835231\n",
      "Iteration 10, loss = 0.67676923\n",
      "Iteration 11, loss = 0.67537314\n",
      "Iteration 12, loss = 0.67411622\n",
      "Iteration 13, loss = 0.67298022\n",
      "Iteration 14, loss = 0.67195296\n",
      "Iteration 15, loss = 0.67101076\n",
      "Iteration 16, loss = 0.67013712\n",
      "Iteration 17, loss = 0.66931880\n",
      "Iteration 18, loss = 0.66854869\n",
      "Iteration 19, loss = 0.66779631\n",
      "Iteration 20, loss = 0.66707729\n",
      "Iteration 21, loss = 0.66638069\n",
      "Iteration 22, loss = 0.66570012\n",
      "Iteration 23, loss = 0.66502997\n",
      "Iteration 24, loss = 0.66436247\n",
      "Iteration 25, loss = 0.66370830\n",
      "Iteration 26, loss = 0.66305259\n",
      "Iteration 27, loss = 0.66240025\n",
      "Iteration 28, loss = 0.66175117\n",
      "Iteration 29, loss = 0.66110420\n",
      "Iteration 30, loss = 0.66045693\n",
      "Iteration 31, loss = 0.65981138\n",
      "Iteration 32, loss = 0.65917528\n",
      "Iteration 33, loss = 0.65852124\n",
      "Iteration 34, loss = 0.65787741\n",
      "Iteration 35, loss = 0.65723330\n",
      "Iteration 36, loss = 0.65659319\n",
      "Iteration 37, loss = 0.65594943\n",
      "Iteration 38, loss = 0.65529886\n",
      "Iteration 39, loss = 0.65465771\n",
      "Iteration 40, loss = 0.65401385\n",
      "Iteration 41, loss = 0.65336842\n",
      "Iteration 42, loss = 0.65272585\n",
      "Iteration 43, loss = 0.65207957\n",
      "Iteration 44, loss = 0.65143597\n",
      "Iteration 45, loss = 0.65080111\n",
      "Iteration 46, loss = 0.65015399\n",
      "Iteration 47, loss = 0.64950811\n",
      "Iteration 48, loss = 0.64886306\n",
      "Iteration 49, loss = 0.64822199\n",
      "Iteration 50, loss = 0.64758905\n",
      "Iteration 51, loss = 0.64694560\n",
      "Iteration 52, loss = 0.64630938\n",
      "Iteration 53, loss = 0.64566739\n",
      "Iteration 54, loss = 0.64503516\n",
      "Iteration 55, loss = 0.64439596\n",
      "Iteration 56, loss = 0.64376025\n",
      "Iteration 57, loss = 0.64312378\n",
      "Iteration 58, loss = 0.64249657\n",
      "Iteration 59, loss = 0.64185904\n",
      "Iteration 60, loss = 0.64123463\n",
      "Iteration 61, loss = 0.64059713\n",
      "Iteration 62, loss = 0.63997253\n",
      "Iteration 63, loss = 0.63933082\n",
      "Iteration 64, loss = 0.63870542\n",
      "Iteration 65, loss = 0.63807323\n",
      "Iteration 66, loss = 0.63744070\n",
      "Iteration 67, loss = 0.63681170\n",
      "Iteration 68, loss = 0.63618059\n",
      "Iteration 69, loss = 0.63555418\n",
      "Iteration 70, loss = 0.63492603\n",
      "Iteration 71, loss = 0.63430253\n",
      "Iteration 72, loss = 0.63367929\n",
      "Iteration 73, loss = 0.63306485\n",
      "Iteration 74, loss = 0.63244268\n",
      "Iteration 75, loss = 0.63181803\n",
      "Iteration 76, loss = 0.63120739\n",
      "Iteration 77, loss = 0.63058230\n",
      "Iteration 78, loss = 0.62996440\n",
      "Iteration 79, loss = 0.62935004\n",
      "Iteration 80, loss = 0.62873518\n",
      "Iteration 81, loss = 0.62812191\n",
      "Iteration 82, loss = 0.62751222\n",
      "Iteration 83, loss = 0.62690270\n",
      "Iteration 84, loss = 0.62629294\n",
      "Iteration 85, loss = 0.62568919\n",
      "Iteration 86, loss = 0.62509057\n",
      "Iteration 87, loss = 0.62448640\n",
      "Iteration 88, loss = 0.62387943\n",
      "Iteration 89, loss = 0.62328124\n",
      "Iteration 90, loss = 0.62268909\n",
      "Iteration 91, loss = 0.62209121\n",
      "Iteration 92, loss = 0.62149358\n",
      "Iteration 93, loss = 0.62089937\n",
      "Iteration 94, loss = 0.62030917\n",
      "Iteration 95, loss = 0.61972543\n",
      "Iteration 96, loss = 0.61912893\n",
      "Iteration 97, loss = 0.61854449\n",
      "Iteration 98, loss = 0.61795830\n",
      "Iteration 99, loss = 0.61738019\n",
      "Iteration 100, loss = 0.61679422\n",
      "Iteration 101, loss = 0.61621041\n",
      "Iteration 102, loss = 0.61563290\n",
      "Iteration 103, loss = 0.61505245\n",
      "Iteration 104, loss = 0.61447570\n",
      "Iteration 105, loss = 0.61390514\n",
      "Iteration 106, loss = 0.61332221\n",
      "Iteration 107, loss = 0.61275004\n",
      "Iteration 108, loss = 0.61217748\n",
      "Iteration 109, loss = 0.61160652\n",
      "Iteration 110, loss = 0.61104507\n",
      "Iteration 111, loss = 0.61046957\n",
      "Iteration 112, loss = 0.60990499\n",
      "Iteration 113, loss = 0.60933866\n",
      "Iteration 114, loss = 0.60877554\n",
      "Iteration 115, loss = 0.60821691\n",
      "Iteration 116, loss = 0.60765532\n",
      "Iteration 117, loss = 0.60709651\n",
      "Iteration 118, loss = 0.60654313\n",
      "Iteration 119, loss = 0.60598868\n",
      "Iteration 120, loss = 0.60543732\n",
      "Iteration 121, loss = 0.60488238\n",
      "Iteration 122, loss = 0.60433772\n",
      "Iteration 123, loss = 0.60378932\n",
      "Iteration 124, loss = 0.60325001\n",
      "Iteration 125, loss = 0.60270541\n",
      "Iteration 126, loss = 0.60215905\n",
      "Iteration 127, loss = 0.60162186\n",
      "Iteration 128, loss = 0.60107987\n",
      "Iteration 129, loss = 0.60054296\n",
      "Iteration 130, loss = 0.60001706\n",
      "Iteration 131, loss = 0.59947902\n",
      "Iteration 132, loss = 0.59894499\n",
      "Iteration 133, loss = 0.59842700\n",
      "Iteration 134, loss = 0.59789302\n",
      "Iteration 135, loss = 0.59736742\n",
      "Iteration 136, loss = 0.59684296\n",
      "Iteration 137, loss = 0.59632579\n",
      "Iteration 138, loss = 0.59580780\n",
      "Iteration 139, loss = 0.59528887\n",
      "Iteration 140, loss = 0.59477656\n",
      "Iteration 141, loss = 0.59426531\n",
      "Iteration 142, loss = 0.59375791\n",
      "Iteration 143, loss = 0.59324642\n",
      "Iteration 144, loss = 0.59273801\n",
      "Iteration 145, loss = 0.59222976\n",
      "Iteration 146, loss = 0.59172637\n",
      "Iteration 147, loss = 0.59124283\n",
      "Iteration 148, loss = 0.59072947\n",
      "Iteration 149, loss = 0.59022540\n",
      "Iteration 150, loss = 0.58973589\n",
      "Iteration 151, loss = 0.58923770\n",
      "Iteration 152, loss = 0.58874654\n",
      "Iteration 153, loss = 0.58825413\n",
      "Iteration 154, loss = 0.58777311\n",
      "Iteration 155, loss = 0.58728035\n",
      "Iteration 156, loss = 0.58679577\n",
      "Iteration 157, loss = 0.58631760\n",
      "Iteration 158, loss = 0.58584036\n",
      "Iteration 159, loss = 0.58535260\n",
      "Iteration 160, loss = 0.58488140\n",
      "Iteration 161, loss = 0.58440197\n",
      "Iteration 162, loss = 0.58393657\n",
      "Iteration 163, loss = 0.58346296\n",
      "Iteration 164, loss = 0.58300045\n",
      "Iteration 165, loss = 0.58253091\n",
      "Iteration 166, loss = 0.58207419\n",
      "Iteration 167, loss = 0.58160745\n",
      "Iteration 168, loss = 0.58115962\n",
      "Iteration 169, loss = 0.58069961\n",
      "Iteration 170, loss = 0.58024605\n",
      "Iteration 171, loss = 0.57979356\n",
      "Iteration 172, loss = 0.57934165\n",
      "Iteration 173, loss = 0.57889754\n",
      "Iteration 174, loss = 0.57845439\n",
      "Iteration 175, loss = 0.57801081\n",
      "Iteration 176, loss = 0.57757459\n",
      "Iteration 177, loss = 0.57713379\n",
      "Iteration 178, loss = 0.57670794\n",
      "Iteration 179, loss = 0.57627288\n",
      "Iteration 180, loss = 0.57585214\n",
      "Iteration 181, loss = 0.57542519\n",
      "Iteration 182, loss = 0.57499895\n",
      "Iteration 183, loss = 0.57457679\n",
      "Iteration 184, loss = 0.57417010\n",
      "Iteration 185, loss = 0.57375003\n",
      "Iteration 186, loss = 0.57334079\n",
      "Iteration 187, loss = 0.57293191\n",
      "Iteration 188, loss = 0.57252081\n",
      "Iteration 189, loss = 0.57212028\n",
      "Iteration 190, loss = 0.57172127\n",
      "Iteration 191, loss = 0.57131575\n",
      "Iteration 192, loss = 0.57093097\n",
      "Iteration 193, loss = 0.57053364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 194, loss = 0.57014026\n",
      "Iteration 195, loss = 0.56975134\n",
      "Iteration 196, loss = 0.56936373\n",
      "Iteration 197, loss = 0.56898021\n",
      "Iteration 198, loss = 0.56860087\n",
      "Iteration 199, loss = 0.56822737\n",
      "Iteration 200, loss = 0.56785247\n",
      "Iteration 201, loss = 0.56747671\n",
      "Iteration 202, loss = 0.56710989\n",
      "Iteration 203, loss = 0.56673487\n",
      "Iteration 204, loss = 0.56636852\n",
      "Iteration 205, loss = 0.56600744\n",
      "Iteration 206, loss = 0.56565026\n",
      "Iteration 207, loss = 0.56530669\n",
      "Iteration 208, loss = 0.56494666\n",
      "Iteration 209, loss = 0.56458643\n",
      "Iteration 210, loss = 0.56424159\n",
      "Iteration 211, loss = 0.56389191\n",
      "Iteration 212, loss = 0.56355300\n",
      "Iteration 213, loss = 0.56320325\n",
      "Iteration 214, loss = 0.56286203\n",
      "Iteration 215, loss = 0.56252713\n",
      "Iteration 216, loss = 0.56219851\n",
      "Iteration 217, loss = 0.56186890\n",
      "Iteration 218, loss = 0.56153202\n",
      "Iteration 219, loss = 0.56120921\n",
      "Iteration 220, loss = 0.56088699\n",
      "Iteration 221, loss = 0.56056788\n",
      "Iteration 222, loss = 0.56024917\n",
      "Iteration 223, loss = 0.55993432\n",
      "Iteration 224, loss = 0.55963218\n",
      "Iteration 225, loss = 0.55932310\n",
      "Iteration 226, loss = 0.55900820\n",
      "Iteration 227, loss = 0.55871016\n",
      "Iteration 228, loss = 0.55840978\n",
      "Iteration 229, loss = 0.55810757\n",
      "Iteration 230, loss = 0.55781425\n",
      "Iteration 231, loss = 0.55752564\n",
      "Iteration 232, loss = 0.55723259\n",
      "Iteration 233, loss = 0.55694840\n",
      "Iteration 234, loss = 0.55665671\n",
      "Iteration 235, loss = 0.55637718\n",
      "Iteration 236, loss = 0.55609285\n",
      "Iteration 237, loss = 0.55582177\n",
      "Iteration 238, loss = 0.55554509\n",
      "Iteration 239, loss = 0.55527433\n",
      "Iteration 240, loss = 0.55500475\n",
      "Iteration 241, loss = 0.55473028\n",
      "Iteration 242, loss = 0.55446117\n",
      "Iteration 243, loss = 0.55420211\n",
      "Iteration 244, loss = 0.55393540\n",
      "Iteration 245, loss = 0.55368024\n",
      "Iteration 246, loss = 0.55341652\n",
      "Iteration 247, loss = 0.55316061\n",
      "Iteration 248, loss = 0.55290382\n",
      "Iteration 249, loss = 0.55265816\n",
      "Iteration 250, loss = 0.55240152\n",
      "Iteration 251, loss = 0.55216093\n",
      "Iteration 252, loss = 0.55191133\n",
      "Iteration 253, loss = 0.55167722\n",
      "Iteration 254, loss = 0.55142899\n",
      "Iteration 255, loss = 0.55119640\n",
      "Iteration 256, loss = 0.55096655\n",
      "Iteration 257, loss = 0.55072735\n",
      "Iteration 258, loss = 0.55049540\n",
      "Iteration 259, loss = 0.55026909\n",
      "Iteration 260, loss = 0.55004768\n",
      "Iteration 261, loss = 0.54982598\n",
      "Iteration 262, loss = 0.54960923\n",
      "Iteration 263, loss = 0.54938605\n",
      "Iteration 264, loss = 0.54916578\n",
      "Iteration 265, loss = 0.54895522\n",
      "Iteration 266, loss = 0.54874054\n",
      "Iteration 267, loss = 0.54853816\n",
      "Iteration 268, loss = 0.54832150\n",
      "Iteration 269, loss = 0.54811665\n",
      "Iteration 270, loss = 0.54791884\n",
      "Iteration 271, loss = 0.54771061\n",
      "Iteration 272, loss = 0.54752491\n",
      "Iteration 273, loss = 0.54731399\n",
      "Iteration 274, loss = 0.54711996\n",
      "Iteration 275, loss = 0.54692923\n",
      "Iteration 276, loss = 0.54674843\n",
      "Iteration 277, loss = 0.54654887\n",
      "Iteration 278, loss = 0.54636089\n",
      "Iteration 279, loss = 0.54617984\n",
      "Iteration 280, loss = 0.54600148\n",
      "Iteration 281, loss = 0.54580861\n",
      "Iteration 282, loss = 0.54563298\n",
      "Iteration 283, loss = 0.54546130\n",
      "Iteration 284, loss = 0.54528911\n",
      "Iteration 285, loss = 0.54510928\n",
      "Iteration 286, loss = 0.54494028\n",
      "Iteration 287, loss = 0.54476898\n",
      "Iteration 288, loss = 0.54460891\n",
      "Iteration 289, loss = 0.54443749\n",
      "Iteration 290, loss = 0.54428754\n",
      "Iteration 291, loss = 0.54411162\n",
      "Iteration 292, loss = 0.54395279\n",
      "Iteration 293, loss = 0.54380274\n",
      "Iteration 294, loss = 0.54363798\n",
      "Iteration 295, loss = 0.54348709\n",
      "Iteration 296, loss = 0.54333318\n",
      "Iteration 297, loss = 0.54318607\n",
      "Iteration 298, loss = 0.54303107\n",
      "Iteration 299, loss = 0.54288403\n",
      "Iteration 300, loss = 0.54274783\n",
      "Iteration 301, loss = 0.54260037\n",
      "Iteration 302, loss = 0.54245589\n",
      "Iteration 303, loss = 0.54231959\n",
      "Iteration 304, loss = 0.54218212\n",
      "Iteration 305, loss = 0.54203653\n",
      "Iteration 306, loss = 0.54190526\n",
      "Iteration 307, loss = 0.54176985\n",
      "Iteration 308, loss = 0.54163566\n",
      "Iteration 309, loss = 0.54151156\n",
      "Iteration 310, loss = 0.54137810\n",
      "Iteration 311, loss = 0.54125181\n",
      "Iteration 312, loss = 0.54112453\n",
      "Iteration 313, loss = 0.54099417\n",
      "Iteration 314, loss = 0.54086834\n",
      "Iteration 315, loss = 0.54074338\n",
      "Iteration 316, loss = 0.54062411\n",
      "Iteration 317, loss = 0.54050755\n",
      "Iteration 318, loss = 0.54038951\n",
      "Iteration 319, loss = 0.54027097\n",
      "Iteration 320, loss = 0.54015202\n",
      "Iteration 321, loss = 0.54003840\n",
      "Iteration 322, loss = 0.53992558\n",
      "Iteration 323, loss = 0.53981612\n",
      "Iteration 324, loss = 0.53969928\n",
      "Iteration 325, loss = 0.53959352\n",
      "Iteration 326, loss = 0.53948171\n",
      "Iteration 327, loss = 0.53937505\n",
      "Iteration 328, loss = 0.53927244\n",
      "Iteration 329, loss = 0.53916964\n",
      "Iteration 330, loss = 0.53906330\n",
      "Iteration 331, loss = 0.53896227\n",
      "Iteration 332, loss = 0.53886185\n",
      "Iteration 333, loss = 0.53876207\n",
      "Iteration 334, loss = 0.53866139\n",
      "Iteration 335, loss = 0.53857428\n",
      "Iteration 336, loss = 0.53846861\n",
      "Iteration 337, loss = 0.53837617\n",
      "Iteration 338, loss = 0.53828642\n",
      "Iteration 339, loss = 0.53818356\n",
      "Iteration 340, loss = 0.53808855\n",
      "Iteration 341, loss = 0.53800305\n",
      "Iteration 342, loss = 0.53791970\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69898113\n",
      "Iteration 2, loss = 0.69644161\n",
      "Iteration 3, loss = 0.69421607\n",
      "Iteration 4, loss = 0.69227149\n",
      "Iteration 5, loss = 0.69057385\n",
      "Iteration 6, loss = 0.68906406\n",
      "Iteration 7, loss = 0.68772505\n",
      "Iteration 8, loss = 0.68652055\n",
      "Iteration 9, loss = 0.68541210\n",
      "Iteration 10, loss = 0.68438187\n",
      "Iteration 11, loss = 0.68341644\n",
      "Iteration 12, loss = 0.68248509\n",
      "Iteration 13, loss = 0.68158045\n",
      "Iteration 14, loss = 0.68070704\n",
      "Iteration 15, loss = 0.67985149\n",
      "Iteration 16, loss = 0.67900520\n",
      "Iteration 17, loss = 0.67817191\n",
      "Iteration 18, loss = 0.67734309\n",
      "Iteration 19, loss = 0.67651267\n",
      "Iteration 20, loss = 0.67569032\n",
      "Iteration 21, loss = 0.67487357\n",
      "Iteration 22, loss = 0.67405615\n",
      "Iteration 23, loss = 0.67323979\n",
      "Iteration 24, loss = 0.67242013\n",
      "Iteration 25, loss = 0.67160625\n",
      "Iteration 26, loss = 0.67079787\n",
      "Iteration 27, loss = 0.66997928\n",
      "Iteration 28, loss = 0.66916602\n",
      "Iteration 29, loss = 0.66835042\n",
      "Iteration 30, loss = 0.66754021\n",
      "Iteration 31, loss = 0.66672901\n",
      "Iteration 32, loss = 0.66592622\n",
      "Iteration 33, loss = 0.66510990\n",
      "Iteration 34, loss = 0.66430289\n",
      "Iteration 35, loss = 0.66349548\n",
      "Iteration 36, loss = 0.66268931\n",
      "Iteration 37, loss = 0.66188855\n",
      "Iteration 38, loss = 0.66107502\n",
      "Iteration 39, loss = 0.66027027\n",
      "Iteration 40, loss = 0.65946886\n",
      "Iteration 41, loss = 0.65866200\n",
      "Iteration 42, loss = 0.65786125\n",
      "Iteration 43, loss = 0.65705478\n",
      "Iteration 44, loss = 0.65625442\n",
      "Iteration 45, loss = 0.65546221\n",
      "Iteration 46, loss = 0.65465836\n",
      "Iteration 47, loss = 0.65385548\n",
      "Iteration 48, loss = 0.65306100\n",
      "Iteration 49, loss = 0.65226235\n",
      "Iteration 50, loss = 0.65147174\n",
      "Iteration 51, loss = 0.65067408\n",
      "Iteration 52, loss = 0.64988376\n",
      "Iteration 53, loss = 0.64908800\n",
      "Iteration 54, loss = 0.64829800\n",
      "Iteration 55, loss = 0.64750493\n",
      "Iteration 56, loss = 0.64671736\n",
      "Iteration 57, loss = 0.64592097\n",
      "Iteration 58, loss = 0.64513902\n",
      "Iteration 59, loss = 0.64434565\n",
      "Iteration 60, loss = 0.64356568\n",
      "Iteration 61, loss = 0.64276968\n",
      "Iteration 62, loss = 0.64198835\n",
      "Iteration 63, loss = 0.64119086\n",
      "Iteration 64, loss = 0.64040981\n",
      "Iteration 65, loss = 0.63961707\n",
      "Iteration 66, loss = 0.63883140\n",
      "Iteration 67, loss = 0.63804734\n",
      "Iteration 68, loss = 0.63725357\n",
      "Iteration 69, loss = 0.63647166\n",
      "Iteration 70, loss = 0.63568571\n",
      "Iteration 71, loss = 0.63490476\n",
      "Iteration 72, loss = 0.63412329\n",
      "Iteration 73, loss = 0.63334802\n",
      "Iteration 74, loss = 0.63256943\n",
      "Iteration 75, loss = 0.63179119\n",
      "Iteration 76, loss = 0.63102217\n",
      "Iteration 77, loss = 0.63024060\n",
      "Iteration 78, loss = 0.62946606\n",
      "Iteration 79, loss = 0.62869494\n",
      "Iteration 80, loss = 0.62792261\n",
      "Iteration 81, loss = 0.62715064\n",
      "Iteration 82, loss = 0.62638132\n",
      "Iteration 83, loss = 0.62561762\n",
      "Iteration 84, loss = 0.62484579\n",
      "Iteration 85, loss = 0.62408226\n",
      "Iteration 86, loss = 0.62332413\n",
      "Iteration 87, loss = 0.62256100\n",
      "Iteration 88, loss = 0.62179787\n",
      "Iteration 89, loss = 0.62104110\n",
      "Iteration 90, loss = 0.62029391\n",
      "Iteration 91, loss = 0.61953050\n",
      "Iteration 92, loss = 0.61877963\n",
      "Iteration 93, loss = 0.61802242\n",
      "Iteration 94, loss = 0.61727092\n",
      "Iteration 95, loss = 0.61653380\n",
      "Iteration 96, loss = 0.61577632\n",
      "Iteration 97, loss = 0.61503019\n",
      "Iteration 98, loss = 0.61428415\n",
      "Iteration 99, loss = 0.61355125\n",
      "Iteration 100, loss = 0.61280864\n",
      "Iteration 101, loss = 0.61206571\n",
      "Iteration 102, loss = 0.61133055\n",
      "Iteration 103, loss = 0.61059331\n",
      "Iteration 104, loss = 0.60985425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 105, loss = 0.60912099\n",
      "Iteration 106, loss = 0.60838847\n",
      "Iteration 107, loss = 0.60765117\n",
      "Iteration 108, loss = 0.60691810\n",
      "Iteration 109, loss = 0.60619031\n",
      "Iteration 110, loss = 0.60546225\n",
      "Iteration 111, loss = 0.60472952\n",
      "Iteration 112, loss = 0.60400577\n",
      "Iteration 113, loss = 0.60327550\n",
      "Iteration 114, loss = 0.60254795\n",
      "Iteration 115, loss = 0.60182731\n",
      "Iteration 116, loss = 0.60110037\n",
      "Iteration 117, loss = 0.60038305\n",
      "Iteration 118, loss = 0.59966345\n",
      "Iteration 119, loss = 0.59895020\n",
      "Iteration 120, loss = 0.59823331\n",
      "Iteration 121, loss = 0.59751265\n",
      "Iteration 122, loss = 0.59680374\n",
      "Iteration 123, loss = 0.59609005\n",
      "Iteration 124, loss = 0.59538512\n",
      "Iteration 125, loss = 0.59467003\n",
      "Iteration 126, loss = 0.59396138\n",
      "Iteration 127, loss = 0.59325843\n",
      "Iteration 128, loss = 0.59254646\n",
      "Iteration 129, loss = 0.59184181\n",
      "Iteration 130, loss = 0.59114092\n",
      "Iteration 131, loss = 0.59043924\n",
      "Iteration 132, loss = 0.58973333\n",
      "Iteration 133, loss = 0.58903574\n",
      "Iteration 134, loss = 0.58833679\n",
      "Iteration 135, loss = 0.58764250\n",
      "Iteration 136, loss = 0.58694695\n",
      "Iteration 137, loss = 0.58625622\n",
      "Iteration 138, loss = 0.58557257\n",
      "Iteration 139, loss = 0.58488140\n",
      "Iteration 140, loss = 0.58419665\n",
      "Iteration 141, loss = 0.58351363\n",
      "Iteration 142, loss = 0.58283561\n",
      "Iteration 143, loss = 0.58215581\n",
      "Iteration 144, loss = 0.58147340\n",
      "Iteration 145, loss = 0.58079511\n",
      "Iteration 146, loss = 0.58012034\n",
      "Iteration 147, loss = 0.57946465\n",
      "Iteration 148, loss = 0.57878600\n",
      "Iteration 149, loss = 0.57811074\n",
      "Iteration 150, loss = 0.57744833\n",
      "Iteration 151, loss = 0.57678150\n",
      "Iteration 152, loss = 0.57612251\n",
      "Iteration 153, loss = 0.57546342\n",
      "Iteration 154, loss = 0.57481250\n",
      "Iteration 155, loss = 0.57415071\n",
      "Iteration 156, loss = 0.57349870\n",
      "Iteration 157, loss = 0.57284652\n",
      "Iteration 158, loss = 0.57219987\n",
      "Iteration 159, loss = 0.57154815\n",
      "Iteration 160, loss = 0.57090828\n",
      "Iteration 161, loss = 0.57026362\n",
      "Iteration 162, loss = 0.56963128\n",
      "Iteration 163, loss = 0.56898879\n",
      "Iteration 164, loss = 0.56836180\n",
      "Iteration 165, loss = 0.56772798\n",
      "Iteration 166, loss = 0.56710814\n",
      "Iteration 167, loss = 0.56647458\n",
      "Iteration 168, loss = 0.56585801\n",
      "Iteration 169, loss = 0.56524149\n",
      "Iteration 170, loss = 0.56462081\n",
      "Iteration 171, loss = 0.56400647\n",
      "Iteration 172, loss = 0.56339280\n",
      "Iteration 173, loss = 0.56278377\n",
      "Iteration 174, loss = 0.56217861\n",
      "Iteration 175, loss = 0.56157592\n",
      "Iteration 176, loss = 0.56097720\n",
      "Iteration 177, loss = 0.56037424\n",
      "Iteration 178, loss = 0.55978543\n",
      "Iteration 179, loss = 0.55919000\n",
      "Iteration 180, loss = 0.55860970\n",
      "Iteration 181, loss = 0.55801992\n",
      "Iteration 182, loss = 0.55743354\n",
      "Iteration 183, loss = 0.55685856\n",
      "Iteration 184, loss = 0.55628529\n",
      "Iteration 185, loss = 0.55571010\n",
      "Iteration 186, loss = 0.55514145\n",
      "Iteration 187, loss = 0.55457801\n",
      "Iteration 188, loss = 0.55401223\n",
      "Iteration 189, loss = 0.55345721\n",
      "Iteration 190, loss = 0.55289684\n",
      "Iteration 191, loss = 0.55234342\n",
      "Iteration 192, loss = 0.55179992\n",
      "Iteration 193, loss = 0.55124655\n",
      "Iteration 194, loss = 0.55070464\n",
      "Iteration 195, loss = 0.55015841\n",
      "Iteration 196, loss = 0.54962003\n",
      "Iteration 197, loss = 0.54908375\n",
      "Iteration 198, loss = 0.54855149\n",
      "Iteration 199, loss = 0.54802597\n",
      "Iteration 200, loss = 0.54749720\n",
      "Iteration 201, loss = 0.54697532\n",
      "Iteration 202, loss = 0.54646339\n",
      "Iteration 203, loss = 0.54593457\n",
      "Iteration 204, loss = 0.54541892\n",
      "Iteration 205, loss = 0.54491604\n",
      "Iteration 206, loss = 0.54440819\n",
      "Iteration 207, loss = 0.54391530\n",
      "Iteration 208, loss = 0.54341394\n",
      "Iteration 209, loss = 0.54290919\n",
      "Iteration 210, loss = 0.54241925\n",
      "Iteration 211, loss = 0.54193029\n",
      "Iteration 212, loss = 0.54144322\n",
      "Iteration 213, loss = 0.54095803\n",
      "Iteration 214, loss = 0.54047360\n",
      "Iteration 215, loss = 0.54000054\n",
      "Iteration 216, loss = 0.53953776\n",
      "Iteration 217, loss = 0.53906097\n",
      "Iteration 218, loss = 0.53858732\n",
      "Iteration 219, loss = 0.53812683\n",
      "Iteration 220, loss = 0.53766745\n",
      "Iteration 221, loss = 0.53721275\n",
      "Iteration 222, loss = 0.53676298\n",
      "Iteration 223, loss = 0.53631573\n",
      "Iteration 224, loss = 0.53587706\n",
      "Iteration 225, loss = 0.53543910\n",
      "Iteration 226, loss = 0.53499658\n",
      "Iteration 227, loss = 0.53456959\n",
      "Iteration 228, loss = 0.53414364\n",
      "Iteration 229, loss = 0.53372084\n",
      "Iteration 230, loss = 0.53330158\n",
      "Iteration 231, loss = 0.53288535\n",
      "Iteration 232, loss = 0.53247328\n",
      "Iteration 233, loss = 0.53205928\n",
      "Iteration 234, loss = 0.53164853\n",
      "Iteration 235, loss = 0.53124827\n",
      "Iteration 236, loss = 0.53084270\n",
      "Iteration 237, loss = 0.53044991\n",
      "Iteration 238, loss = 0.53005967\n",
      "Iteration 239, loss = 0.52966543\n",
      "Iteration 240, loss = 0.52927999\n",
      "Iteration 241, loss = 0.52888660\n",
      "Iteration 242, loss = 0.52850523\n",
      "Iteration 243, loss = 0.52813428\n",
      "Iteration 244, loss = 0.52775159\n",
      "Iteration 245, loss = 0.52738745\n",
      "Iteration 246, loss = 0.52700994\n",
      "Iteration 247, loss = 0.52664994\n",
      "Iteration 248, loss = 0.52628079\n",
      "Iteration 249, loss = 0.52592805\n",
      "Iteration 250, loss = 0.52556123\n",
      "Iteration 251, loss = 0.52521710\n",
      "Iteration 252, loss = 0.52485930\n",
      "Iteration 253, loss = 0.52451853\n",
      "Iteration 254, loss = 0.52416779\n",
      "Iteration 255, loss = 0.52383144\n",
      "Iteration 256, loss = 0.52349493\n",
      "Iteration 257, loss = 0.52315613\n",
      "Iteration 258, loss = 0.52282551\n",
      "Iteration 259, loss = 0.52249610\n",
      "Iteration 260, loss = 0.52217379\n",
      "Iteration 261, loss = 0.52185496\n",
      "Iteration 262, loss = 0.52154057\n",
      "Iteration 263, loss = 0.52121937\n",
      "Iteration 264, loss = 0.52090194\n",
      "Iteration 265, loss = 0.52059705\n",
      "Iteration 266, loss = 0.52028798\n",
      "Iteration 267, loss = 0.51998574\n",
      "Iteration 268, loss = 0.51968030\n",
      "Iteration 269, loss = 0.51937909\n",
      "Iteration 270, loss = 0.51909203\n",
      "Iteration 271, loss = 0.51879237\n",
      "Iteration 272, loss = 0.51851495\n",
      "Iteration 273, loss = 0.51821916\n",
      "Iteration 274, loss = 0.51793346\n",
      "Iteration 275, loss = 0.51765570\n",
      "Iteration 276, loss = 0.51738451\n",
      "Iteration 277, loss = 0.51710421\n",
      "Iteration 278, loss = 0.51683502\n",
      "Iteration 279, loss = 0.51656742\n",
      "Iteration 280, loss = 0.51630215\n",
      "Iteration 281, loss = 0.51603076\n",
      "Iteration 282, loss = 0.51577178\n",
      "Iteration 283, loss = 0.51551963\n",
      "Iteration 284, loss = 0.51526881\n",
      "Iteration 285, loss = 0.51501449\n",
      "Iteration 286, loss = 0.51476211\n",
      "Iteration 287, loss = 0.51451514\n",
      "Iteration 288, loss = 0.51428197\n",
      "Iteration 289, loss = 0.51403743\n",
      "Iteration 290, loss = 0.51380651\n",
      "Iteration 291, loss = 0.51356023\n",
      "Iteration 292, loss = 0.51332674\n",
      "Iteration 293, loss = 0.51310299\n",
      "Iteration 294, loss = 0.51286945\n",
      "Iteration 295, loss = 0.51264194\n",
      "Iteration 296, loss = 0.51242053\n",
      "Iteration 297, loss = 0.51220574\n",
      "Iteration 298, loss = 0.51198144\n",
      "Iteration 299, loss = 0.51176558\n",
      "Iteration 300, loss = 0.51156359\n",
      "Iteration 301, loss = 0.51135039\n",
      "Iteration 302, loss = 0.51113276\n",
      "Iteration 303, loss = 0.51093633\n",
      "Iteration 304, loss = 0.51072755\n",
      "Iteration 305, loss = 0.51052407\n",
      "Iteration 306, loss = 0.51033081\n",
      "Iteration 307, loss = 0.51013385\n",
      "Iteration 308, loss = 0.50993674\n",
      "Iteration 309, loss = 0.50974909\n",
      "Iteration 310, loss = 0.50956172\n",
      "Iteration 311, loss = 0.50937010\n",
      "Iteration 312, loss = 0.50918736\n",
      "Iteration 313, loss = 0.50899587\n",
      "Iteration 314, loss = 0.50881626\n",
      "Iteration 315, loss = 0.50863497\n",
      "Iteration 316, loss = 0.50846204\n",
      "Iteration 317, loss = 0.50828537\n",
      "Iteration 318, loss = 0.50811560\n",
      "Iteration 319, loss = 0.50794041\n",
      "Iteration 320, loss = 0.50777059\n",
      "Iteration 321, loss = 0.50760331\n",
      "Iteration 322, loss = 0.50743963\n",
      "Iteration 323, loss = 0.50728716\n",
      "Iteration 324, loss = 0.50711387\n",
      "Iteration 325, loss = 0.50696954\n",
      "Iteration 326, loss = 0.50680682\n",
      "Iteration 327, loss = 0.50664859\n",
      "Iteration 328, loss = 0.50649480\n",
      "Iteration 329, loss = 0.50635244\n",
      "Iteration 330, loss = 0.50619833\n",
      "Iteration 331, loss = 0.50605342\n",
      "Iteration 332, loss = 0.50591033\n",
      "Iteration 333, loss = 0.50576517\n",
      "Iteration 334, loss = 0.50562457\n",
      "Iteration 335, loss = 0.50549963\n",
      "Iteration 336, loss = 0.50535387\n",
      "Iteration 337, loss = 0.50521411\n",
      "Iteration 338, loss = 0.50509389\n",
      "Iteration 339, loss = 0.50494830\n",
      "Iteration 340, loss = 0.50481477\n",
      "Iteration 341, loss = 0.50468845\n",
      "Iteration 342, loss = 0.50456032\n",
      "Iteration 343, loss = 0.50443052\n",
      "Iteration 344, loss = 0.50430650\n",
      "Iteration 345, loss = 0.50418967\n",
      "Iteration 346, loss = 0.50406440\n",
      "Iteration 347, loss = 0.50394269\n",
      "Iteration 348, loss = 0.50382541\n",
      "Iteration 349, loss = 0.50371324\n",
      "Iteration 350, loss = 0.50360444\n",
      "Iteration 351, loss = 0.50348915\n",
      "Iteration 352, loss = 0.50337457\n",
      "Iteration 353, loss = 0.50325462\n",
      "Iteration 354, loss = 0.50315024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 355, loss = 0.50303584\n",
      "Iteration 356, loss = 0.50294290\n",
      "Iteration 357, loss = 0.50282774\n",
      "Iteration 358, loss = 0.50272964\n",
      "Iteration 359, loss = 0.50261954\n",
      "Iteration 360, loss = 0.50251434\n",
      "Iteration 361, loss = 0.50241206\n",
      "Iteration 362, loss = 0.50230937\n",
      "Iteration 363, loss = 0.50221241\n",
      "Iteration 364, loss = 0.50211715\n",
      "Iteration 365, loss = 0.50202178\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69598480\n",
      "Iteration 2, loss = 0.69492170\n",
      "Iteration 3, loss = 0.69393855\n",
      "Iteration 4, loss = 0.69300734\n",
      "Iteration 5, loss = 0.69208509\n",
      "Iteration 6, loss = 0.69119015\n",
      "Iteration 7, loss = 0.69031696\n",
      "Iteration 8, loss = 0.68945737\n",
      "Iteration 9, loss = 0.68859632\n",
      "Iteration 10, loss = 0.68774524\n",
      "Iteration 11, loss = 0.68690089\n",
      "Iteration 12, loss = 0.68605508\n",
      "Iteration 13, loss = 0.68521515\n",
      "Iteration 14, loss = 0.68437893\n",
      "Iteration 15, loss = 0.68354680\n",
      "Iteration 16, loss = 0.68271396\n",
      "Iteration 17, loss = 0.68188595\n",
      "Iteration 18, loss = 0.68106110\n",
      "Iteration 19, loss = 0.68023397\n",
      "Iteration 20, loss = 0.67941335\n",
      "Iteration 21, loss = 0.67859300\n",
      "Iteration 22, loss = 0.67777706\n",
      "Iteration 23, loss = 0.67696187\n",
      "Iteration 24, loss = 0.67614276\n",
      "Iteration 25, loss = 0.67532800\n",
      "Iteration 26, loss = 0.67452025\n",
      "Iteration 27, loss = 0.67370306\n",
      "Iteration 28, loss = 0.67289896\n",
      "Iteration 29, loss = 0.67208420\n",
      "Iteration 30, loss = 0.67128120\n",
      "Iteration 31, loss = 0.67047103\n",
      "Iteration 32, loss = 0.66967048\n",
      "Iteration 33, loss = 0.66887041\n",
      "Iteration 34, loss = 0.66807115\n",
      "Iteration 35, loss = 0.66726882\n",
      "Iteration 36, loss = 0.66647163\n",
      "Iteration 37, loss = 0.66568094\n",
      "Iteration 38, loss = 0.66488232\n",
      "Iteration 39, loss = 0.66409212\n",
      "Iteration 40, loss = 0.66330598\n",
      "Iteration 41, loss = 0.66251167\n",
      "Iteration 42, loss = 0.66172444\n",
      "Iteration 43, loss = 0.66093400\n",
      "Iteration 44, loss = 0.66014967\n",
      "Iteration 45, loss = 0.65937123\n",
      "Iteration 46, loss = 0.65858212\n",
      "Iteration 47, loss = 0.65779339\n",
      "Iteration 48, loss = 0.65701729\n",
      "Iteration 49, loss = 0.65623697\n",
      "Iteration 50, loss = 0.65546340\n",
      "Iteration 51, loss = 0.65468538\n",
      "Iteration 52, loss = 0.65391388\n",
      "Iteration 53, loss = 0.65313995\n",
      "Iteration 54, loss = 0.65236972\n",
      "Iteration 55, loss = 0.65160322\n",
      "Iteration 56, loss = 0.65083304\n",
      "Iteration 57, loss = 0.65006264\n",
      "Iteration 58, loss = 0.64929995\n",
      "Iteration 59, loss = 0.64853254\n",
      "Iteration 60, loss = 0.64777233\n",
      "Iteration 61, loss = 0.64700184\n",
      "Iteration 62, loss = 0.64624468\n",
      "Iteration 63, loss = 0.64547336\n",
      "Iteration 64, loss = 0.64471629\n",
      "Iteration 65, loss = 0.64395011\n",
      "Iteration 66, loss = 0.64318767\n",
      "Iteration 67, loss = 0.64243046\n",
      "Iteration 68, loss = 0.64166141\n",
      "Iteration 69, loss = 0.64090622\n",
      "Iteration 70, loss = 0.64014741\n",
      "Iteration 71, loss = 0.63938998\n",
      "Iteration 72, loss = 0.63863797\n",
      "Iteration 73, loss = 0.63789482\n",
      "Iteration 74, loss = 0.63712957\n",
      "Iteration 75, loss = 0.63638553\n",
      "Iteration 76, loss = 0.63564142\n",
      "Iteration 77, loss = 0.63488363\n",
      "Iteration 78, loss = 0.63413774\n",
      "Iteration 79, loss = 0.63339205\n",
      "Iteration 80, loss = 0.63264963\n",
      "Iteration 81, loss = 0.63190732\n",
      "Iteration 82, loss = 0.63116296\n",
      "Iteration 83, loss = 0.63042157\n",
      "Iteration 84, loss = 0.62968064\n",
      "Iteration 85, loss = 0.62894173\n",
      "Iteration 86, loss = 0.62820919\n",
      "Iteration 87, loss = 0.62747150\n",
      "Iteration 88, loss = 0.62673574\n",
      "Iteration 89, loss = 0.62600509\n",
      "Iteration 90, loss = 0.62528107\n",
      "Iteration 91, loss = 0.62455051\n",
      "Iteration 92, loss = 0.62382258\n",
      "Iteration 93, loss = 0.62309687\n",
      "Iteration 94, loss = 0.62236712\n",
      "Iteration 95, loss = 0.62165895\n",
      "Iteration 96, loss = 0.62093136\n",
      "Iteration 97, loss = 0.62021171\n",
      "Iteration 98, loss = 0.61949088\n",
      "Iteration 99, loss = 0.61878660\n",
      "Iteration 100, loss = 0.61807218\n",
      "Iteration 101, loss = 0.61735435\n",
      "Iteration 102, loss = 0.61664713\n",
      "Iteration 103, loss = 0.61593983\n",
      "Iteration 104, loss = 0.61522869\n",
      "Iteration 105, loss = 0.61452265\n",
      "Iteration 106, loss = 0.61381807\n",
      "Iteration 107, loss = 0.61311303\n",
      "Iteration 108, loss = 0.61240916\n",
      "Iteration 109, loss = 0.61171501\n",
      "Iteration 110, loss = 0.61102028\n",
      "Iteration 111, loss = 0.61031788\n",
      "Iteration 112, loss = 0.60962574\n",
      "Iteration 113, loss = 0.60893043\n",
      "Iteration 114, loss = 0.60823590\n",
      "Iteration 115, loss = 0.60754979\n",
      "Iteration 116, loss = 0.60685370\n",
      "Iteration 117, loss = 0.60617103\n",
      "Iteration 118, loss = 0.60548307\n",
      "Iteration 119, loss = 0.60480120\n",
      "Iteration 120, loss = 0.60412034\n",
      "Iteration 121, loss = 0.60343341\n",
      "Iteration 122, loss = 0.60275631\n",
      "Iteration 123, loss = 0.60208339\n",
      "Iteration 124, loss = 0.60140770\n",
      "Iteration 125, loss = 0.60073397\n",
      "Iteration 126, loss = 0.60005879\n",
      "Iteration 127, loss = 0.59939263\n",
      "Iteration 128, loss = 0.59871779\n",
      "Iteration 129, loss = 0.59805158\n",
      "Iteration 130, loss = 0.59738814\n",
      "Iteration 131, loss = 0.59672755\n",
      "Iteration 132, loss = 0.59606103\n",
      "Iteration 133, loss = 0.59540333\n",
      "Iteration 134, loss = 0.59474394\n",
      "Iteration 135, loss = 0.59409003\n",
      "Iteration 136, loss = 0.59343385\n",
      "Iteration 137, loss = 0.59278489\n",
      "Iteration 138, loss = 0.59214011\n",
      "Iteration 139, loss = 0.59148752\n",
      "Iteration 140, loss = 0.59084128\n",
      "Iteration 141, loss = 0.59019500\n",
      "Iteration 142, loss = 0.58955539\n",
      "Iteration 143, loss = 0.58891502\n",
      "Iteration 144, loss = 0.58827372\n",
      "Iteration 145, loss = 0.58763537\n",
      "Iteration 146, loss = 0.58700323\n",
      "Iteration 147, loss = 0.58638499\n",
      "Iteration 148, loss = 0.58574928\n",
      "Iteration 149, loss = 0.58512040\n",
      "Iteration 150, loss = 0.58449924\n",
      "Iteration 151, loss = 0.58387784\n",
      "Iteration 152, loss = 0.58326202\n",
      "Iteration 153, loss = 0.58264543\n",
      "Iteration 154, loss = 0.58203682\n",
      "Iteration 155, loss = 0.58141841\n",
      "Iteration 156, loss = 0.58081139\n",
      "Iteration 157, loss = 0.58020539\n",
      "Iteration 158, loss = 0.57960169\n",
      "Iteration 159, loss = 0.57899639\n",
      "Iteration 160, loss = 0.57840367\n",
      "Iteration 161, loss = 0.57780381\n",
      "Iteration 162, loss = 0.57721979\n",
      "Iteration 163, loss = 0.57662495\n",
      "Iteration 164, loss = 0.57604157\n",
      "Iteration 165, loss = 0.57545623\n",
      "Iteration 166, loss = 0.57488243\n",
      "Iteration 167, loss = 0.57430118\n",
      "Iteration 168, loss = 0.57373075\n",
      "Iteration 169, loss = 0.57316236\n",
      "Iteration 170, loss = 0.57259321\n",
      "Iteration 171, loss = 0.57203188\n",
      "Iteration 172, loss = 0.57146333\n",
      "Iteration 173, loss = 0.57090559\n",
      "Iteration 174, loss = 0.57035229\n",
      "Iteration 175, loss = 0.56980036\n",
      "Iteration 176, loss = 0.56925329\n",
      "Iteration 177, loss = 0.56870412\n",
      "Iteration 178, loss = 0.56816297\n",
      "Iteration 179, loss = 0.56762218\n",
      "Iteration 180, loss = 0.56709173\n",
      "Iteration 181, loss = 0.56655489\n",
      "Iteration 182, loss = 0.56602314\n",
      "Iteration 183, loss = 0.56550099\n",
      "Iteration 184, loss = 0.56498028\n",
      "Iteration 185, loss = 0.56446138\n",
      "Iteration 186, loss = 0.56394266\n",
      "Iteration 187, loss = 0.56343441\n",
      "Iteration 188, loss = 0.56292387\n",
      "Iteration 189, loss = 0.56242110\n",
      "Iteration 190, loss = 0.56191616\n",
      "Iteration 191, loss = 0.56142273\n",
      "Iteration 192, loss = 0.56092689\n",
      "Iteration 193, loss = 0.56043077\n",
      "Iteration 194, loss = 0.55994193\n",
      "Iteration 195, loss = 0.55945033\n",
      "Iteration 196, loss = 0.55896849\n",
      "Iteration 197, loss = 0.55848970\n",
      "Iteration 198, loss = 0.55801029\n",
      "Iteration 199, loss = 0.55753982\n",
      "Iteration 200, loss = 0.55707084\n",
      "Iteration 201, loss = 0.55660421\n",
      "Iteration 202, loss = 0.55614821\n",
      "Iteration 203, loss = 0.55567836\n",
      "Iteration 204, loss = 0.55522458\n",
      "Iteration 205, loss = 0.55477151\n",
      "Iteration 206, loss = 0.55431729\n",
      "Iteration 207, loss = 0.55388090\n",
      "Iteration 208, loss = 0.55343464\n",
      "Iteration 209, loss = 0.55298816\n",
      "Iteration 210, loss = 0.55255286\n",
      "Iteration 211, loss = 0.55212410\n",
      "Iteration 212, loss = 0.55168710\n",
      "Iteration 213, loss = 0.55126289\n",
      "Iteration 214, loss = 0.55083217\n",
      "Iteration 215, loss = 0.55041074\n",
      "Iteration 216, loss = 0.55000240\n",
      "Iteration 217, loss = 0.54958516\n",
      "Iteration 218, loss = 0.54916783\n",
      "Iteration 219, loss = 0.54875943\n",
      "Iteration 220, loss = 0.54835533\n",
      "Iteration 221, loss = 0.54795859\n",
      "Iteration 222, loss = 0.54756046\n",
      "Iteration 223, loss = 0.54717410\n",
      "Iteration 224, loss = 0.54678628\n",
      "Iteration 225, loss = 0.54640124\n",
      "Iteration 226, loss = 0.54601389\n",
      "Iteration 227, loss = 0.54564075\n",
      "Iteration 228, loss = 0.54526941\n",
      "Iteration 229, loss = 0.54489955\n",
      "Iteration 230, loss = 0.54453246\n",
      "Iteration 231, loss = 0.54417109\n",
      "Iteration 232, loss = 0.54381141\n",
      "Iteration 233, loss = 0.54345682\n",
      "Iteration 234, loss = 0.54309743\n",
      "Iteration 235, loss = 0.54274605\n",
      "Iteration 236, loss = 0.54239625\n",
      "Iteration 237, loss = 0.54205442\n",
      "Iteration 238, loss = 0.54171318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 239, loss = 0.54137434\n",
      "Iteration 240, loss = 0.54104501\n",
      "Iteration 241, loss = 0.54070616\n",
      "Iteration 242, loss = 0.54037520\n",
      "Iteration 243, loss = 0.54006024\n",
      "Iteration 244, loss = 0.53972878\n",
      "Iteration 245, loss = 0.53941404\n",
      "Iteration 246, loss = 0.53909248\n",
      "Iteration 247, loss = 0.53878650\n",
      "Iteration 248, loss = 0.53846710\n",
      "Iteration 249, loss = 0.53816444\n",
      "Iteration 250, loss = 0.53785144\n",
      "Iteration 251, loss = 0.53756529\n",
      "Iteration 252, loss = 0.53725866\n",
      "Iteration 253, loss = 0.53696701\n",
      "Iteration 254, loss = 0.53667126\n",
      "Iteration 255, loss = 0.53638461\n",
      "Iteration 256, loss = 0.53610398\n",
      "Iteration 257, loss = 0.53581625\n",
      "Iteration 258, loss = 0.53553437\n",
      "Iteration 259, loss = 0.53525863\n",
      "Iteration 260, loss = 0.53499156\n",
      "Iteration 261, loss = 0.53472786\n",
      "Iteration 262, loss = 0.53446604\n",
      "Iteration 263, loss = 0.53419697\n",
      "Iteration 264, loss = 0.53392420\n",
      "Iteration 265, loss = 0.53367102\n",
      "Iteration 266, loss = 0.53341438\n",
      "Iteration 267, loss = 0.53315943\n",
      "Iteration 268, loss = 0.53290865\n",
      "Iteration 269, loss = 0.53265747\n",
      "Iteration 270, loss = 0.53241798\n",
      "Iteration 271, loss = 0.53217096\n",
      "Iteration 272, loss = 0.53194218\n",
      "Iteration 273, loss = 0.53169684\n",
      "Iteration 274, loss = 0.53146068\n",
      "Iteration 275, loss = 0.53123097\n",
      "Iteration 276, loss = 0.53101123\n",
      "Iteration 277, loss = 0.53077710\n",
      "Iteration 278, loss = 0.53055647\n",
      "Iteration 279, loss = 0.53034505\n",
      "Iteration 280, loss = 0.53011983\n",
      "Iteration 281, loss = 0.52990000\n",
      "Iteration 282, loss = 0.52968671\n",
      "Iteration 283, loss = 0.52948223\n",
      "Iteration 284, loss = 0.52928213\n",
      "Iteration 285, loss = 0.52906757\n",
      "Iteration 286, loss = 0.52886670\n",
      "Iteration 287, loss = 0.52866066\n",
      "Iteration 288, loss = 0.52847964\n",
      "Iteration 289, loss = 0.52827382\n",
      "Iteration 290, loss = 0.52809504\n",
      "Iteration 291, loss = 0.52788630\n",
      "Iteration 292, loss = 0.52769871\n",
      "Iteration 293, loss = 0.52752330\n",
      "Iteration 294, loss = 0.52733353\n",
      "Iteration 295, loss = 0.52714868\n",
      "Iteration 296, loss = 0.52696947\n",
      "Iteration 297, loss = 0.52679725\n",
      "Iteration 298, loss = 0.52661864\n",
      "Iteration 299, loss = 0.52644261\n",
      "Iteration 300, loss = 0.52628257\n",
      "Iteration 301, loss = 0.52611381\n",
      "Iteration 302, loss = 0.52593921\n",
      "Iteration 303, loss = 0.52577854\n",
      "Iteration 304, loss = 0.52561635\n",
      "Iteration 305, loss = 0.52545170\n",
      "Iteration 306, loss = 0.52529747\n",
      "Iteration 307, loss = 0.52514032\n",
      "Iteration 308, loss = 0.52498512\n",
      "Iteration 309, loss = 0.52483767\n",
      "Iteration 310, loss = 0.52469066\n",
      "Iteration 311, loss = 0.52453568\n",
      "Iteration 312, loss = 0.52439363\n",
      "Iteration 313, loss = 0.52424389\n",
      "Iteration 314, loss = 0.52410539\n",
      "Iteration 315, loss = 0.52396236\n",
      "Iteration 316, loss = 0.52382780\n",
      "Iteration 317, loss = 0.52368506\n",
      "Iteration 318, loss = 0.52355915\n",
      "Iteration 319, loss = 0.52341966\n",
      "Iteration 320, loss = 0.52328783\n",
      "Iteration 321, loss = 0.52316017\n",
      "Iteration 322, loss = 0.52303367\n",
      "Iteration 323, loss = 0.52290879\n",
      "Iteration 324, loss = 0.52277906\n",
      "Iteration 325, loss = 0.52266577\n",
      "Iteration 326, loss = 0.52253358\n",
      "Iteration 327, loss = 0.52241628\n",
      "Iteration 328, loss = 0.52229689\n",
      "Iteration 329, loss = 0.52218298\n",
      "Iteration 330, loss = 0.52206917\n",
      "Iteration 331, loss = 0.52195485\n",
      "Iteration 332, loss = 0.52184713\n",
      "Iteration 333, loss = 0.52172828\n",
      "Iteration 334, loss = 0.52162072\n",
      "Iteration 335, loss = 0.52152564\n",
      "Iteration 336, loss = 0.52141016\n",
      "Iteration 337, loss = 0.52130924\n",
      "Iteration 338, loss = 0.52121225\n",
      "Iteration 339, loss = 0.52110552\n",
      "Iteration 340, loss = 0.52100039\n",
      "Iteration 341, loss = 0.52090382\n",
      "Iteration 342, loss = 0.52080694\n",
      "Iteration 343, loss = 0.52070681\n",
      "Iteration 344, loss = 0.52061320\n",
      "Iteration 345, loss = 0.52052280\n",
      "Iteration 346, loss = 0.52042728\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69533195\n",
      "Iteration 2, loss = 0.69442522\n",
      "Iteration 3, loss = 0.69354949\n",
      "Iteration 4, loss = 0.69269949\n",
      "Iteration 5, loss = 0.69183219\n",
      "Iteration 6, loss = 0.69098160\n",
      "Iteration 7, loss = 0.69014282\n",
      "Iteration 8, loss = 0.68930975\n",
      "Iteration 9, loss = 0.68847487\n",
      "Iteration 10, loss = 0.68764322\n",
      "Iteration 11, loss = 0.68681833\n",
      "Iteration 12, loss = 0.68599426\n",
      "Iteration 13, loss = 0.68517414\n",
      "Iteration 14, loss = 0.68435817\n",
      "Iteration 15, loss = 0.68354733\n",
      "Iteration 16, loss = 0.68273403\n",
      "Iteration 17, loss = 0.68192667\n",
      "Iteration 18, loss = 0.68112343\n",
      "Iteration 19, loss = 0.68031766\n",
      "Iteration 20, loss = 0.67951869\n",
      "Iteration 21, loss = 0.67871967\n",
      "Iteration 22, loss = 0.67792673\n",
      "Iteration 23, loss = 0.67713284\n",
      "Iteration 24, loss = 0.67633745\n",
      "Iteration 25, loss = 0.67554757\n",
      "Iteration 26, loss = 0.67476041\n",
      "Iteration 27, loss = 0.67396983\n",
      "Iteration 28, loss = 0.67318619\n",
      "Iteration 29, loss = 0.67239828\n",
      "Iteration 30, loss = 0.67162162\n",
      "Iteration 31, loss = 0.67083570\n",
      "Iteration 32, loss = 0.67005734\n",
      "Iteration 33, loss = 0.66928544\n",
      "Iteration 34, loss = 0.66851195\n",
      "Iteration 35, loss = 0.66773457\n",
      "Iteration 36, loss = 0.66696300\n",
      "Iteration 37, loss = 0.66619759\n",
      "Iteration 38, loss = 0.66542625\n",
      "Iteration 39, loss = 0.66466151\n",
      "Iteration 40, loss = 0.66390423\n",
      "Iteration 41, loss = 0.66313717\n",
      "Iteration 42, loss = 0.66237625\n",
      "Iteration 43, loss = 0.66161264\n",
      "Iteration 44, loss = 0.66085607\n",
      "Iteration 45, loss = 0.66010434\n",
      "Iteration 46, loss = 0.65934251\n",
      "Iteration 47, loss = 0.65858204\n",
      "Iteration 48, loss = 0.65783275\n",
      "Iteration 49, loss = 0.65707983\n",
      "Iteration 50, loss = 0.65633209\n",
      "Iteration 51, loss = 0.65558288\n",
      "Iteration 52, loss = 0.65484139\n",
      "Iteration 53, loss = 0.65409290\n",
      "Iteration 54, loss = 0.65335101\n",
      "Iteration 55, loss = 0.65261512\n",
      "Iteration 56, loss = 0.65187202\n",
      "Iteration 57, loss = 0.65113058\n",
      "Iteration 58, loss = 0.65039964\n",
      "Iteration 59, loss = 0.64966133\n",
      "Iteration 60, loss = 0.64893142\n",
      "Iteration 61, loss = 0.64819124\n",
      "Iteration 62, loss = 0.64746355\n",
      "Iteration 63, loss = 0.64672441\n",
      "Iteration 64, loss = 0.64599935\n",
      "Iteration 65, loss = 0.64526440\n",
      "Iteration 66, loss = 0.64453341\n",
      "Iteration 67, loss = 0.64380887\n",
      "Iteration 68, loss = 0.64307151\n",
      "Iteration 69, loss = 0.64234919\n",
      "Iteration 70, loss = 0.64162462\n",
      "Iteration 71, loss = 0.64089855\n",
      "Iteration 72, loss = 0.64018192\n",
      "Iteration 73, loss = 0.63947138\n",
      "Iteration 74, loss = 0.63873860\n",
      "Iteration 75, loss = 0.63802870\n",
      "Iteration 76, loss = 0.63732249\n",
      "Iteration 77, loss = 0.63659544\n",
      "Iteration 78, loss = 0.63588629\n",
      "Iteration 79, loss = 0.63517693\n",
      "Iteration 80, loss = 0.63446887\n",
      "Iteration 81, loss = 0.63376422\n",
      "Iteration 82, loss = 0.63305729\n",
      "Iteration 83, loss = 0.63235222\n",
      "Iteration 84, loss = 0.63164586\n",
      "Iteration 85, loss = 0.63094246\n",
      "Iteration 86, loss = 0.63024580\n",
      "Iteration 87, loss = 0.62954513\n",
      "Iteration 88, loss = 0.62884519\n",
      "Iteration 89, loss = 0.62815246\n",
      "Iteration 90, loss = 0.62746491\n",
      "Iteration 91, loss = 0.62677358\n",
      "Iteration 92, loss = 0.62608449\n",
      "Iteration 93, loss = 0.62539300\n",
      "Iteration 94, loss = 0.62470137\n",
      "Iteration 95, loss = 0.62403155\n",
      "Iteration 96, loss = 0.62334017\n",
      "Iteration 97, loss = 0.62265743\n",
      "Iteration 98, loss = 0.62197696\n",
      "Iteration 99, loss = 0.62131041\n",
      "Iteration 100, loss = 0.62063386\n",
      "Iteration 101, loss = 0.61995518\n",
      "Iteration 102, loss = 0.61928761\n",
      "Iteration 103, loss = 0.61861907\n",
      "Iteration 104, loss = 0.61794682\n",
      "Iteration 105, loss = 0.61728005\n",
      "Iteration 106, loss = 0.61661658\n",
      "Iteration 107, loss = 0.61594871\n",
      "Iteration 108, loss = 0.61528761\n",
      "Iteration 109, loss = 0.61463486\n",
      "Iteration 110, loss = 0.61398030\n",
      "Iteration 111, loss = 0.61331941\n",
      "Iteration 112, loss = 0.61266652\n",
      "Iteration 113, loss = 0.61201217\n",
      "Iteration 114, loss = 0.61136305\n",
      "Iteration 115, loss = 0.61071459\n",
      "Iteration 116, loss = 0.61006074\n",
      "Iteration 117, loss = 0.60942151\n",
      "Iteration 118, loss = 0.60877562\n",
      "Iteration 119, loss = 0.60813580\n",
      "Iteration 120, loss = 0.60749922\n",
      "Iteration 121, loss = 0.60685542\n",
      "Iteration 122, loss = 0.60622025\n",
      "Iteration 123, loss = 0.60558959\n",
      "Iteration 124, loss = 0.60495864\n",
      "Iteration 125, loss = 0.60432814\n",
      "Iteration 126, loss = 0.60369791\n",
      "Iteration 127, loss = 0.60307792\n",
      "Iteration 128, loss = 0.60244630\n",
      "Iteration 129, loss = 0.60182539\n",
      "Iteration 130, loss = 0.60120627\n",
      "Iteration 131, loss = 0.60059106\n",
      "Iteration 132, loss = 0.59996988\n",
      "Iteration 133, loss = 0.59935693\n",
      "Iteration 134, loss = 0.59874309\n",
      "Iteration 135, loss = 0.59813722\n",
      "Iteration 136, loss = 0.59752653\n",
      "Iteration 137, loss = 0.59692594\n",
      "Iteration 138, loss = 0.59632370\n",
      "Iteration 139, loss = 0.59572003\n",
      "Iteration 140, loss = 0.59512254\n",
      "Iteration 141, loss = 0.59452321\n",
      "Iteration 142, loss = 0.59393050\n",
      "Iteration 143, loss = 0.59333804\n",
      "Iteration 144, loss = 0.59274608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 145, loss = 0.59215774\n",
      "Iteration 146, loss = 0.59157340\n",
      "Iteration 147, loss = 0.59100126\n",
      "Iteration 148, loss = 0.59041681\n",
      "Iteration 149, loss = 0.58983717\n",
      "Iteration 150, loss = 0.58926507\n",
      "Iteration 151, loss = 0.58869496\n",
      "Iteration 152, loss = 0.58812543\n",
      "Iteration 153, loss = 0.58755989\n",
      "Iteration 154, loss = 0.58700219\n",
      "Iteration 155, loss = 0.58643272\n",
      "Iteration 156, loss = 0.58587659\n",
      "Iteration 157, loss = 0.58532146\n",
      "Iteration 158, loss = 0.58476800\n",
      "Iteration 159, loss = 0.58421503\n",
      "Iteration 160, loss = 0.58367386\n",
      "Iteration 161, loss = 0.58312469\n",
      "Iteration 162, loss = 0.58259090\n",
      "Iteration 163, loss = 0.58204745\n",
      "Iteration 164, loss = 0.58151513\n",
      "Iteration 165, loss = 0.58098482\n",
      "Iteration 166, loss = 0.58046247\n",
      "Iteration 167, loss = 0.57993385\n",
      "Iteration 168, loss = 0.57941893\n",
      "Iteration 169, loss = 0.57890082\n",
      "Iteration 170, loss = 0.57838438\n",
      "Iteration 171, loss = 0.57787154\n",
      "Iteration 172, loss = 0.57735916\n",
      "Iteration 173, loss = 0.57685434\n",
      "Iteration 174, loss = 0.57635451\n",
      "Iteration 175, loss = 0.57585462\n",
      "Iteration 176, loss = 0.57536159\n",
      "Iteration 177, loss = 0.57486950\n",
      "Iteration 178, loss = 0.57437963\n",
      "Iteration 179, loss = 0.57389372\n",
      "Iteration 180, loss = 0.57341798\n",
      "Iteration 181, loss = 0.57293430\n",
      "Iteration 182, loss = 0.57245664\n",
      "Iteration 183, loss = 0.57198856\n",
      "Iteration 184, loss = 0.57152286\n",
      "Iteration 185, loss = 0.57105779\n",
      "Iteration 186, loss = 0.57059441\n",
      "Iteration 187, loss = 0.57013996\n",
      "Iteration 188, loss = 0.56968632\n",
      "Iteration 189, loss = 0.56923961\n",
      "Iteration 190, loss = 0.56878720\n",
      "Iteration 191, loss = 0.56835089\n",
      "Iteration 192, loss = 0.56790929\n",
      "Iteration 193, loss = 0.56747145\n",
      "Iteration 194, loss = 0.56703346\n",
      "Iteration 195, loss = 0.56659901\n",
      "Iteration 196, loss = 0.56617060\n",
      "Iteration 197, loss = 0.56574995\n",
      "Iteration 198, loss = 0.56532700\n",
      "Iteration 199, loss = 0.56491092\n",
      "Iteration 200, loss = 0.56449862\n",
      "Iteration 201, loss = 0.56408700\n",
      "Iteration 202, loss = 0.56368501\n",
      "Iteration 203, loss = 0.56327419\n",
      "Iteration 204, loss = 0.56287469\n",
      "Iteration 205, loss = 0.56247887\n",
      "Iteration 206, loss = 0.56208251\n",
      "Iteration 207, loss = 0.56170200\n",
      "Iteration 208, loss = 0.56131428\n",
      "Iteration 209, loss = 0.56092273\n",
      "Iteration 210, loss = 0.56054468\n",
      "Iteration 211, loss = 0.56017339\n",
      "Iteration 212, loss = 0.55979401\n",
      "Iteration 213, loss = 0.55942565\n",
      "Iteration 214, loss = 0.55905246\n",
      "Iteration 215, loss = 0.55868814\n",
      "Iteration 216, loss = 0.55833778\n",
      "Iteration 217, loss = 0.55797883\n",
      "Iteration 218, loss = 0.55761545\n",
      "Iteration 219, loss = 0.55726798\n",
      "Iteration 220, loss = 0.55692132\n",
      "Iteration 221, loss = 0.55658011\n",
      "Iteration 222, loss = 0.55623807\n",
      "Iteration 223, loss = 0.55590955\n",
      "Iteration 224, loss = 0.55558138\n",
      "Iteration 225, loss = 0.55524982\n",
      "Iteration 226, loss = 0.55492089\n",
      "Iteration 227, loss = 0.55460228\n",
      "Iteration 228, loss = 0.55428534\n",
      "Iteration 229, loss = 0.55397428\n",
      "Iteration 230, loss = 0.55366046\n",
      "Iteration 231, loss = 0.55335450\n",
      "Iteration 232, loss = 0.55305211\n",
      "Iteration 233, loss = 0.55275132\n",
      "Iteration 234, loss = 0.55245053\n",
      "Iteration 235, loss = 0.55215519\n",
      "Iteration 236, loss = 0.55186063\n",
      "Iteration 237, loss = 0.55157385\n",
      "Iteration 238, loss = 0.55128819\n",
      "Iteration 239, loss = 0.55100404\n",
      "Iteration 240, loss = 0.55073338\n",
      "Iteration 241, loss = 0.55045002\n",
      "Iteration 242, loss = 0.55017652\n",
      "Iteration 243, loss = 0.54991676\n",
      "Iteration 244, loss = 0.54964120\n",
      "Iteration 245, loss = 0.54938077\n",
      "Iteration 246, loss = 0.54911484\n",
      "Iteration 247, loss = 0.54886518\n",
      "Iteration 248, loss = 0.54859901\n",
      "Iteration 249, loss = 0.54834726\n",
      "Iteration 250, loss = 0.54809210\n",
      "Iteration 251, loss = 0.54786279\n",
      "Iteration 252, loss = 0.54760808\n",
      "Iteration 253, loss = 0.54737216\n",
      "Iteration 254, loss = 0.54713287\n",
      "Iteration 255, loss = 0.54689973\n",
      "Iteration 256, loss = 0.54667342\n",
      "Iteration 257, loss = 0.54643952\n",
      "Iteration 258, loss = 0.54621209\n",
      "Iteration 259, loss = 0.54599136\n",
      "Iteration 260, loss = 0.54577659\n",
      "Iteration 261, loss = 0.54556622\n",
      "Iteration 262, loss = 0.54535523\n",
      "Iteration 263, loss = 0.54514315\n",
      "Iteration 264, loss = 0.54492499\n",
      "Iteration 265, loss = 0.54472384\n",
      "Iteration 266, loss = 0.54452048\n",
      "Iteration 267, loss = 0.54431768\n",
      "Iteration 268, loss = 0.54412170\n",
      "Iteration 269, loss = 0.54392136\n",
      "Iteration 270, loss = 0.54373362\n",
      "Iteration 271, loss = 0.54354001\n",
      "Iteration 272, loss = 0.54335853\n",
      "Iteration 273, loss = 0.54316969\n",
      "Iteration 274, loss = 0.54298629\n",
      "Iteration 275, loss = 0.54280638\n",
      "Iteration 276, loss = 0.54263577\n",
      "Iteration 277, loss = 0.54245454\n",
      "Iteration 278, loss = 0.54228237\n",
      "Iteration 279, loss = 0.54212107\n",
      "Iteration 280, loss = 0.54194742\n",
      "Iteration 281, loss = 0.54177748\n",
      "Iteration 282, loss = 0.54161409\n",
      "Iteration 283, loss = 0.54145753\n",
      "Iteration 284, loss = 0.54130551\n",
      "Iteration 285, loss = 0.54114137\n",
      "Iteration 286, loss = 0.54099211\n",
      "Iteration 287, loss = 0.54082803\n",
      "Iteration 288, loss = 0.54069661\n",
      "Iteration 289, loss = 0.54053632\n",
      "Iteration 290, loss = 0.54040205\n",
      "Iteration 291, loss = 0.54024403\n",
      "Iteration 292, loss = 0.54010337\n",
      "Iteration 293, loss = 0.53997350\n",
      "Iteration 294, loss = 0.53983119\n",
      "Iteration 295, loss = 0.53968824\n",
      "Iteration 296, loss = 0.53955713\n",
      "Iteration 297, loss = 0.53942748\n",
      "Iteration 298, loss = 0.53929343\n",
      "Iteration 299, loss = 0.53916264\n",
      "Iteration 300, loss = 0.53904278\n",
      "Iteration 301, loss = 0.53892046\n",
      "Iteration 302, loss = 0.53879124\n",
      "Iteration 303, loss = 0.53867233\n",
      "Iteration 304, loss = 0.53855118\n",
      "Iteration 305, loss = 0.53843021\n",
      "Iteration 306, loss = 0.53831680\n",
      "Iteration 307, loss = 0.53820371\n",
      "Iteration 308, loss = 0.53809172\n",
      "Iteration 309, loss = 0.53798556\n",
      "Iteration 310, loss = 0.53787580\n",
      "Iteration 311, loss = 0.53776475\n",
      "Iteration 312, loss = 0.53766943\n",
      "Iteration 313, loss = 0.53755389\n",
      "Iteration 314, loss = 0.53745195\n",
      "Iteration 315, loss = 0.53735041\n",
      "Iteration 316, loss = 0.53725482\n",
      "Iteration 317, loss = 0.53715406\n",
      "Iteration 318, loss = 0.53706217\n",
      "Iteration 319, loss = 0.53696187\n",
      "Iteration 320, loss = 0.53686817\n",
      "Iteration 321, loss = 0.53677692\n",
      "Iteration 322, loss = 0.53669034\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "y = y.ravel()\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "scoring = 'accuracy'\n",
    "cv_results = model_selection.cross_val_score(clf, x, y, cv=kfold, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliar a performance dos algoritmos foi utilizada a função Cross-validation da biblioteca scikit-learn. A avaliação dos dados não deve ser feita utilizando o conjunto de treinamento. Uma das metologias de avaliação consiste em dividir os dados em dois conjuntos: teste e treinamento, calculando a pontuação 7 vezes consecutivas (com diferentes divisões de cada vez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706126356094448\n"
     ]
    }
   ],
   "source": [
    "print(cv_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrada os atributos dos índices espectrais NDVI, Diferença do NVDI, NBRL, Diferença do NBRL, e as medianas das bandas [2,5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73628137\n",
      "Iteration 2, loss = 0.73219083\n",
      "Iteration 3, loss = 0.72831176\n",
      "Iteration 4, loss = 0.72461181\n",
      "Iteration 5, loss = 0.72109237\n",
      "Iteration 6, loss = 0.71773563\n",
      "Iteration 7, loss = 0.71456291\n",
      "Iteration 8, loss = 0.71156365\n",
      "Iteration 9, loss = 0.70871941\n",
      "Iteration 10, loss = 0.70601857\n",
      "Iteration 11, loss = 0.70349401\n",
      "Iteration 12, loss = 0.70110945\n",
      "Iteration 13, loss = 0.69887461\n",
      "Iteration 14, loss = 0.69676361\n",
      "Iteration 15, loss = 0.69477623\n",
      "Iteration 16, loss = 0.69292899\n",
      "Iteration 17, loss = 0.69117934\n",
      "Iteration 18, loss = 0.68954142\n",
      "Iteration 19, loss = 0.68800179\n",
      "Iteration 20, loss = 0.68656979\n",
      "Iteration 21, loss = 0.68521460\n",
      "Iteration 22, loss = 0.68394877\n",
      "Iteration 23, loss = 0.68276039\n",
      "Iteration 24, loss = 0.68163338\n",
      "Iteration 25, loss = 0.68057293\n",
      "Iteration 26, loss = 0.67955545\n",
      "Iteration 27, loss = 0.67859946\n",
      "Iteration 28, loss = 0.67768865\n",
      "Iteration 29, loss = 0.67681056\n",
      "Iteration 30, loss = 0.67597170\n",
      "Iteration 31, loss = 0.67516794\n",
      "Iteration 32, loss = 0.67438381\n",
      "Iteration 33, loss = 0.67362621\n",
      "Iteration 34, loss = 0.67287611\n",
      "Iteration 35, loss = 0.67214959\n",
      "Iteration 36, loss = 0.67143540\n",
      "Iteration 37, loss = 0.67073098\n",
      "Iteration 38, loss = 0.67003497\n",
      "Iteration 39, loss = 0.66934426\n",
      "Iteration 40, loss = 0.66866146\n",
      "Iteration 41, loss = 0.66797624\n",
      "Iteration 42, loss = 0.66729459\n",
      "Iteration 43, loss = 0.66661517\n",
      "Iteration 44, loss = 0.66593629\n",
      "Iteration 45, loss = 0.66525781\n",
      "Iteration 46, loss = 0.66457596\n",
      "Iteration 47, loss = 0.66389547\n",
      "Iteration 48, loss = 0.66321429\n",
      "Iteration 49, loss = 0.66252847\n",
      "Iteration 50, loss = 0.66183710\n",
      "Iteration 51, loss = 0.66114619\n",
      "Iteration 52, loss = 0.66045179\n",
      "Iteration 53, loss = 0.65975544\n",
      "Iteration 54, loss = 0.65905505\n",
      "Iteration 55, loss = 0.65835477\n",
      "Iteration 56, loss = 0.65764534\n",
      "Iteration 57, loss = 0.65693610\n",
      "Iteration 58, loss = 0.65622494\n",
      "Iteration 59, loss = 0.65551294\n",
      "Iteration 60, loss = 0.65478918\n",
      "Iteration 61, loss = 0.65407108\n",
      "Iteration 62, loss = 0.65334836\n",
      "Iteration 63, loss = 0.65262713\n",
      "Iteration 64, loss = 0.65190054\n",
      "Iteration 65, loss = 0.65117428\n",
      "Iteration 66, loss = 0.65044702\n",
      "Iteration 67, loss = 0.64971611\n",
      "Iteration 68, loss = 0.64898284\n",
      "Iteration 69, loss = 0.64824722\n",
      "Iteration 70, loss = 0.64751006\n",
      "Iteration 71, loss = 0.64677370\n",
      "Iteration 72, loss = 0.64603344\n",
      "Iteration 73, loss = 0.64529157\n",
      "Iteration 74, loss = 0.64454780\n",
      "Iteration 75, loss = 0.64380024\n",
      "Iteration 76, loss = 0.64305030\n",
      "Iteration 77, loss = 0.64229875\n",
      "Iteration 78, loss = 0.64154379\n",
      "Iteration 79, loss = 0.64078835\n",
      "Iteration 80, loss = 0.64003503\n",
      "Iteration 81, loss = 0.63927475\n",
      "Iteration 82, loss = 0.63851618\n",
      "Iteration 83, loss = 0.63776177\n",
      "Iteration 84, loss = 0.63700210\n",
      "Iteration 85, loss = 0.63624272\n",
      "Iteration 86, loss = 0.63548130\n",
      "Iteration 87, loss = 0.63472453\n",
      "Iteration 88, loss = 0.63395962\n",
      "Iteration 89, loss = 0.63319681\n",
      "Iteration 90, loss = 0.63243857\n",
      "Iteration 91, loss = 0.63167698\n",
      "Iteration 92, loss = 0.63091500\n",
      "Iteration 93, loss = 0.63015188\n",
      "Iteration 94, loss = 0.62938811\n",
      "Iteration 95, loss = 0.62862456\n",
      "Iteration 96, loss = 0.62787169\n",
      "Iteration 97, loss = 0.62710432\n",
      "Iteration 98, loss = 0.62635104\n",
      "Iteration 99, loss = 0.62558310\n",
      "Iteration 100, loss = 0.62482282\n",
      "Iteration 101, loss = 0.62406055\n",
      "Iteration 102, loss = 0.62329914\n",
      "Iteration 103, loss = 0.62253217\n",
      "Iteration 104, loss = 0.62176660\n",
      "Iteration 105, loss = 0.62100286\n",
      "Iteration 106, loss = 0.62023791\n",
      "Iteration 107, loss = 0.61947228\n",
      "Iteration 108, loss = 0.61871222\n",
      "Iteration 109, loss = 0.61794241\n",
      "Iteration 110, loss = 0.61717518\n",
      "Iteration 111, loss = 0.61640500\n",
      "Iteration 112, loss = 0.61563736\n",
      "Iteration 113, loss = 0.61486514\n",
      "Iteration 114, loss = 0.61409397\n",
      "Iteration 115, loss = 0.61332668\n",
      "Iteration 116, loss = 0.61255561\n",
      "Iteration 117, loss = 0.61177981\n",
      "Iteration 118, loss = 0.61101074\n",
      "Iteration 119, loss = 0.61023464\n",
      "Iteration 120, loss = 0.60946380\n",
      "Iteration 121, loss = 0.60869553\n",
      "Iteration 122, loss = 0.60792376\n",
      "Iteration 123, loss = 0.60715693\n",
      "Iteration 124, loss = 0.60638933\n",
      "Iteration 125, loss = 0.60562073\n",
      "Iteration 126, loss = 0.60485814\n",
      "Iteration 127, loss = 0.60408952\n",
      "Iteration 128, loss = 0.60332458\n",
      "Iteration 129, loss = 0.60256284\n",
      "Iteration 130, loss = 0.60180100\n",
      "Iteration 131, loss = 0.60103207\n",
      "Iteration 132, loss = 0.60027401\n",
      "Iteration 133, loss = 0.59951991\n",
      "Iteration 134, loss = 0.59876122\n",
      "Iteration 135, loss = 0.59801467\n",
      "Iteration 136, loss = 0.59725121\n",
      "Iteration 137, loss = 0.59649788\n",
      "Iteration 138, loss = 0.59574325\n",
      "Iteration 139, loss = 0.59499528\n",
      "Iteration 140, loss = 0.59424674\n",
      "Iteration 141, loss = 0.59350154\n",
      "Iteration 142, loss = 0.59274910\n",
      "Iteration 143, loss = 0.59200671\n",
      "Iteration 144, loss = 0.59126015\n",
      "Iteration 145, loss = 0.59051371\n",
      "Iteration 146, loss = 0.58977279\n",
      "Iteration 147, loss = 0.58902848\n",
      "Iteration 148, loss = 0.58829015\n",
      "Iteration 149, loss = 0.58754849\n",
      "Iteration 150, loss = 0.58681049\n",
      "Iteration 151, loss = 0.58607520\n",
      "Iteration 152, loss = 0.58533891\n",
      "Iteration 153, loss = 0.58460581\n",
      "Iteration 154, loss = 0.58387565\n",
      "Iteration 155, loss = 0.58314512\n",
      "Iteration 156, loss = 0.58241821\n",
      "Iteration 157, loss = 0.58169337\n",
      "Iteration 158, loss = 0.58097979\n",
      "Iteration 159, loss = 0.58025403\n",
      "Iteration 160, loss = 0.57953541\n",
      "Iteration 161, loss = 0.57882219\n",
      "Iteration 162, loss = 0.57810761\n",
      "Iteration 163, loss = 0.57739716\n",
      "Iteration 164, loss = 0.57668635\n",
      "Iteration 165, loss = 0.57598004\n",
      "Iteration 166, loss = 0.57526975\n",
      "Iteration 167, loss = 0.57456421\n",
      "Iteration 168, loss = 0.57386691\n",
      "Iteration 169, loss = 0.57316636\n",
      "Iteration 170, loss = 0.57246608\n",
      "Iteration 171, loss = 0.57176788\n",
      "Iteration 172, loss = 0.57107772\n",
      "Iteration 173, loss = 0.57038282\n",
      "Iteration 174, loss = 0.56969340\n",
      "Iteration 175, loss = 0.56901044\n",
      "Iteration 176, loss = 0.56832096\n",
      "Iteration 177, loss = 0.56763832\n",
      "Iteration 178, loss = 0.56695413\n",
      "Iteration 179, loss = 0.56627579\n",
      "Iteration 180, loss = 0.56560599\n",
      "Iteration 181, loss = 0.56493310\n",
      "Iteration 182, loss = 0.56426704\n",
      "Iteration 183, loss = 0.56360413\n",
      "Iteration 184, loss = 0.56294557\n",
      "Iteration 185, loss = 0.56229185\n",
      "Iteration 186, loss = 0.56164597\n",
      "Iteration 187, loss = 0.56098925\n",
      "Iteration 188, loss = 0.56033870\n",
      "Iteration 189, loss = 0.55969557\n",
      "Iteration 190, loss = 0.55904926\n",
      "Iteration 191, loss = 0.55841181\n",
      "Iteration 192, loss = 0.55777286\n",
      "Iteration 193, loss = 0.55714491\n",
      "Iteration 194, loss = 0.55651836\n",
      "Iteration 195, loss = 0.55589092\n",
      "Iteration 196, loss = 0.55527026\n",
      "Iteration 197, loss = 0.55465146\n",
      "Iteration 198, loss = 0.55403975\n",
      "Iteration 199, loss = 0.55342538\n",
      "Iteration 200, loss = 0.55281371\n",
      "Iteration 201, loss = 0.55220937\n",
      "Iteration 202, loss = 0.55160486\n",
      "Iteration 203, loss = 0.55100673\n",
      "Iteration 204, loss = 0.55041195\n",
      "Iteration 205, loss = 0.54981217\n",
      "Iteration 206, loss = 0.54922174\n",
      "Iteration 207, loss = 0.54864629\n",
      "Iteration 208, loss = 0.54805123\n",
      "Iteration 209, loss = 0.54746198\n",
      "Iteration 210, loss = 0.54688371\n",
      "Iteration 211, loss = 0.54629707\n",
      "Iteration 212, loss = 0.54572664\n",
      "Iteration 213, loss = 0.54515519\n",
      "Iteration 214, loss = 0.54458088\n",
      "Iteration 215, loss = 0.54401706\n",
      "Iteration 216, loss = 0.54345344\n",
      "Iteration 217, loss = 0.54289371\n",
      "Iteration 218, loss = 0.54234022\n",
      "Iteration 219, loss = 0.54178331\n",
      "Iteration 220, loss = 0.54122998\n",
      "Iteration 221, loss = 0.54068529\n",
      "Iteration 222, loss = 0.54014149\n",
      "Iteration 223, loss = 0.53960273\n",
      "Iteration 224, loss = 0.53906328\n",
      "Iteration 225, loss = 0.53852821\n",
      "Iteration 226, loss = 0.53800142\n",
      "Iteration 227, loss = 0.53748269\n",
      "Iteration 228, loss = 0.53695993\n",
      "Iteration 229, loss = 0.53643929\n",
      "Iteration 230, loss = 0.53592161\n",
      "Iteration 231, loss = 0.53540982\n",
      "Iteration 232, loss = 0.53490099\n",
      "Iteration 233, loss = 0.53439626\n",
      "Iteration 234, loss = 0.53389299\n",
      "Iteration 235, loss = 0.53339127\n",
      "Iteration 236, loss = 0.53289780\n",
      "Iteration 237, loss = 0.53239838\n",
      "Iteration 238, loss = 0.53191193\n",
      "Iteration 239, loss = 0.53142046\n",
      "Iteration 240, loss = 0.53093878\n",
      "Iteration 241, loss = 0.53046254\n",
      "Iteration 242, loss = 0.52998874\n",
      "Iteration 243, loss = 0.52950699\n",
      "Iteration 244, loss = 0.52903881\n",
      "Iteration 245, loss = 0.52857291\n",
      "Iteration 246, loss = 0.52810683\n",
      "Iteration 247, loss = 0.52764764\n",
      "Iteration 248, loss = 0.52719772\n",
      "Iteration 249, loss = 0.52673404\n",
      "Iteration 250, loss = 0.52627543\n",
      "Iteration 251, loss = 0.52582814\n",
      "Iteration 252, loss = 0.52537865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.52494155\n",
      "Iteration 254, loss = 0.52450168\n",
      "Iteration 255, loss = 0.52406489\n",
      "Iteration 256, loss = 0.52363496\n",
      "Iteration 257, loss = 0.52320376\n",
      "Iteration 258, loss = 0.52277501\n",
      "Iteration 259, loss = 0.52235900\n",
      "Iteration 260, loss = 0.52193361\n",
      "Iteration 261, loss = 0.52151460\n",
      "Iteration 262, loss = 0.52110239\n",
      "Iteration 263, loss = 0.52068788\n",
      "Iteration 264, loss = 0.52027936\n",
      "Iteration 265, loss = 0.51987975\n",
      "Iteration 266, loss = 0.51947562\n",
      "Iteration 267, loss = 0.51908147\n",
      "Iteration 268, loss = 0.51868574\n",
      "Iteration 269, loss = 0.51829570\n",
      "Iteration 270, loss = 0.51790209\n",
      "Iteration 271, loss = 0.51751952\n",
      "Iteration 272, loss = 0.51714860\n",
      "Iteration 273, loss = 0.51676331\n",
      "Iteration 274, loss = 0.51638674\n",
      "Iteration 275, loss = 0.51601363\n",
      "Iteration 276, loss = 0.51564672\n",
      "Iteration 277, loss = 0.51527569\n",
      "Iteration 278, loss = 0.51491768\n",
      "Iteration 279, loss = 0.51455985\n",
      "Iteration 280, loss = 0.51419391\n",
      "Iteration 281, loss = 0.51384367\n",
      "Iteration 282, loss = 0.51349712\n",
      "Iteration 283, loss = 0.51314235\n",
      "Iteration 284, loss = 0.51280704\n",
      "Iteration 285, loss = 0.51245952\n",
      "Iteration 286, loss = 0.51211913\n",
      "Iteration 287, loss = 0.51178843\n",
      "Iteration 288, loss = 0.51145525\n",
      "Iteration 289, loss = 0.51113398\n",
      "Iteration 290, loss = 0.51081639\n",
      "Iteration 291, loss = 0.51048357\n",
      "Iteration 292, loss = 0.51018139\n",
      "Iteration 293, loss = 0.50985842\n",
      "Iteration 294, loss = 0.50953886\n",
      "Iteration 295, loss = 0.50923216\n",
      "Iteration 296, loss = 0.50893110\n",
      "Iteration 297, loss = 0.50862429\n",
      "Iteration 298, loss = 0.50832698\n",
      "Iteration 299, loss = 0.50802945\n",
      "Iteration 300, loss = 0.50773753\n",
      "Iteration 301, loss = 0.50744186\n",
      "Iteration 302, loss = 0.50714839\n",
      "Iteration 303, loss = 0.50686431\n",
      "Iteration 304, loss = 0.50657654\n",
      "Iteration 305, loss = 0.50629683\n",
      "Iteration 306, loss = 0.50601674\n",
      "Iteration 307, loss = 0.50574210\n",
      "Iteration 308, loss = 0.50546492\n",
      "Iteration 309, loss = 0.50519435\n",
      "Iteration 310, loss = 0.50492708\n",
      "Iteration 311, loss = 0.50465594\n",
      "Iteration 312, loss = 0.50439168\n",
      "Iteration 313, loss = 0.50413294\n",
      "Iteration 314, loss = 0.50386953\n",
      "Iteration 315, loss = 0.50361541\n",
      "Iteration 316, loss = 0.50335832\n",
      "Iteration 317, loss = 0.50311010\n",
      "Iteration 318, loss = 0.50286364\n",
      "Iteration 319, loss = 0.50262240\n",
      "Iteration 320, loss = 0.50237082\n",
      "Iteration 321, loss = 0.50213202\n",
      "Iteration 322, loss = 0.50189325\n",
      "Iteration 323, loss = 0.50165118\n",
      "Iteration 324, loss = 0.50141846\n",
      "Iteration 325, loss = 0.50118647\n",
      "Iteration 326, loss = 0.50095746\n",
      "Iteration 327, loss = 0.50072997\n",
      "Iteration 328, loss = 0.50050470\n",
      "Iteration 329, loss = 0.50028113\n",
      "Iteration 330, loss = 0.50005989\n",
      "Iteration 331, loss = 0.49984298\n",
      "Iteration 332, loss = 0.49962632\n",
      "Iteration 333, loss = 0.49941328\n",
      "Iteration 334, loss = 0.49920371\n",
      "Iteration 335, loss = 0.49899623\n",
      "Iteration 336, loss = 0.49878385\n",
      "Iteration 337, loss = 0.49857882\n",
      "Iteration 338, loss = 0.49837493\n",
      "Iteration 339, loss = 0.49818271\n",
      "Iteration 340, loss = 0.49797906\n",
      "Iteration 341, loss = 0.49778607\n",
      "Iteration 342, loss = 0.49758985\n",
      "Iteration 343, loss = 0.49739907\n",
      "Iteration 344, loss = 0.49721099\n",
      "Iteration 345, loss = 0.49702916\n",
      "Iteration 346, loss = 0.49684196\n",
      "Iteration 347, loss = 0.49665728\n",
      "Iteration 348, loss = 0.49647925\n",
      "Iteration 349, loss = 0.49630060\n",
      "Iteration 350, loss = 0.49612344\n",
      "Iteration 351, loss = 0.49595544\n",
      "Iteration 352, loss = 0.49578326\n",
      "Iteration 353, loss = 0.49561280\n",
      "Iteration 354, loss = 0.49544189\n",
      "Iteration 355, loss = 0.49529324\n",
      "Iteration 356, loss = 0.49511506\n",
      "Iteration 357, loss = 0.49494850\n",
      "Iteration 358, loss = 0.49479105\n",
      "Iteration 359, loss = 0.49462962\n",
      "Iteration 360, loss = 0.49447601\n",
      "Iteration 361, loss = 0.49432486\n",
      "Iteration 362, loss = 0.49417198\n",
      "Iteration 363, loss = 0.49402180\n",
      "Iteration 364, loss = 0.49386620\n",
      "Iteration 365, loss = 0.49372390\n",
      "Iteration 366, loss = 0.49358179\n",
      "Iteration 367, loss = 0.49343609\n",
      "Iteration 368, loss = 0.49329573\n",
      "Iteration 369, loss = 0.49315629\n",
      "Iteration 370, loss = 0.49302009\n",
      "Iteration 371, loss = 0.49289373\n",
      "Iteration 372, loss = 0.49275264\n",
      "Iteration 373, loss = 0.49262079\n",
      "Iteration 374, loss = 0.49249211\n",
      "Iteration 375, loss = 0.49236182\n",
      "Iteration 376, loss = 0.49223580\n",
      "Iteration 377, loss = 0.49210598\n",
      "Iteration 378, loss = 0.49197716\n",
      "Iteration 379, loss = 0.49185896\n",
      "Iteration 380, loss = 0.49174115\n",
      "Iteration 381, loss = 0.49162062\n",
      "Iteration 382, loss = 0.49150653\n",
      "Iteration 383, loss = 0.49138692\n",
      "Iteration 384, loss = 0.49127392\n",
      "Iteration 385, loss = 0.49116714\n",
      "Iteration 386, loss = 0.49105039\n",
      "Iteration 387, loss = 0.49094914\n",
      "Iteration 388, loss = 0.49083937\n",
      "Iteration 389, loss = 0.49073024\n",
      "Iteration 390, loss = 0.49062692\n",
      "Iteration 391, loss = 0.49052977\n",
      "Iteration 392, loss = 0.49042523\n",
      "Iteration 393, loss = 0.49031985\n",
      "Iteration 394, loss = 0.49022176\n",
      "Iteration 395, loss = 0.49012792\n",
      "Iteration 396, loss = 0.49002877\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72994337\n",
      "Iteration 2, loss = 0.72627489\n",
      "Iteration 3, loss = 0.72282017\n",
      "Iteration 4, loss = 0.71954049\n",
      "Iteration 5, loss = 0.71643411\n",
      "Iteration 6, loss = 0.71348713\n",
      "Iteration 7, loss = 0.71071347\n",
      "Iteration 8, loss = 0.70810712\n",
      "Iteration 9, loss = 0.70564696\n",
      "Iteration 10, loss = 0.70332915\n",
      "Iteration 11, loss = 0.70117405\n",
      "Iteration 12, loss = 0.69915319\n",
      "Iteration 13, loss = 0.69727066\n",
      "Iteration 14, loss = 0.69550142\n",
      "Iteration 15, loss = 0.69384151\n",
      "Iteration 16, loss = 0.69230606\n",
      "Iteration 17, loss = 0.69085781\n",
      "Iteration 18, loss = 0.68950890\n",
      "Iteration 19, loss = 0.68824335\n",
      "Iteration 20, loss = 0.68707153\n",
      "Iteration 21, loss = 0.68596250\n",
      "Iteration 22, loss = 0.68492775\n",
      "Iteration 23, loss = 0.68395472\n",
      "Iteration 24, loss = 0.68303520\n",
      "Iteration 25, loss = 0.68216509\n",
      "Iteration 26, loss = 0.68132584\n",
      "Iteration 27, loss = 0.68053347\n",
      "Iteration 28, loss = 0.67977843\n",
      "Iteration 29, loss = 0.67904387\n",
      "Iteration 30, loss = 0.67833734\n",
      "Iteration 31, loss = 0.67765590\n",
      "Iteration 32, loss = 0.67698796\n",
      "Iteration 33, loss = 0.67633687\n",
      "Iteration 34, loss = 0.67568892\n",
      "Iteration 35, loss = 0.67505555\n",
      "Iteration 36, loss = 0.67442801\n",
      "Iteration 37, loss = 0.67380752\n",
      "Iteration 38, loss = 0.67319063\n",
      "Iteration 39, loss = 0.67257734\n",
      "Iteration 40, loss = 0.67196490\n",
      "Iteration 41, loss = 0.67135305\n",
      "Iteration 42, loss = 0.67073885\n",
      "Iteration 43, loss = 0.67012890\n",
      "Iteration 44, loss = 0.66951590\n",
      "Iteration 45, loss = 0.66890567\n",
      "Iteration 46, loss = 0.66828862\n",
      "Iteration 47, loss = 0.66767358\n",
      "Iteration 48, loss = 0.66705589\n",
      "Iteration 49, loss = 0.66643631\n",
      "Iteration 50, loss = 0.66581336\n",
      "Iteration 51, loss = 0.66518985\n",
      "Iteration 52, loss = 0.66456255\n",
      "Iteration 53, loss = 0.66393370\n",
      "Iteration 54, loss = 0.66330295\n",
      "Iteration 55, loss = 0.66266967\n",
      "Iteration 56, loss = 0.66203207\n",
      "Iteration 57, loss = 0.66139364\n",
      "Iteration 58, loss = 0.66075244\n",
      "Iteration 59, loss = 0.66011216\n",
      "Iteration 60, loss = 0.65946342\n",
      "Iteration 61, loss = 0.65881802\n",
      "Iteration 62, loss = 0.65817059\n",
      "Iteration 63, loss = 0.65752356\n",
      "Iteration 64, loss = 0.65687162\n",
      "Iteration 65, loss = 0.65622359\n",
      "Iteration 66, loss = 0.65557226\n",
      "Iteration 67, loss = 0.65491703\n",
      "Iteration 68, loss = 0.65426398\n",
      "Iteration 69, loss = 0.65360670\n",
      "Iteration 70, loss = 0.65294847\n",
      "Iteration 71, loss = 0.65229028\n",
      "Iteration 72, loss = 0.65162953\n",
      "Iteration 73, loss = 0.65096730\n",
      "Iteration 74, loss = 0.65030563\n",
      "Iteration 75, loss = 0.64964085\n",
      "Iteration 76, loss = 0.64897085\n",
      "Iteration 77, loss = 0.64830283\n",
      "Iteration 78, loss = 0.64763240\n",
      "Iteration 79, loss = 0.64696346\n",
      "Iteration 80, loss = 0.64629501\n",
      "Iteration 81, loss = 0.64562291\n",
      "Iteration 82, loss = 0.64495252\n",
      "Iteration 83, loss = 0.64428468\n",
      "Iteration 84, loss = 0.64361751\n",
      "Iteration 85, loss = 0.64294338\n",
      "Iteration 86, loss = 0.64227085\n",
      "Iteration 87, loss = 0.64160298\n",
      "Iteration 88, loss = 0.64092641\n",
      "Iteration 89, loss = 0.64025290\n",
      "Iteration 90, loss = 0.63958603\n",
      "Iteration 91, loss = 0.63891339\n",
      "Iteration 92, loss = 0.63824219\n",
      "Iteration 93, loss = 0.63757136\n",
      "Iteration 94, loss = 0.63689964\n",
      "Iteration 95, loss = 0.63622604\n",
      "Iteration 96, loss = 0.63556386\n",
      "Iteration 97, loss = 0.63488588\n",
      "Iteration 98, loss = 0.63422740\n",
      "Iteration 99, loss = 0.63354828\n",
      "Iteration 100, loss = 0.63288075\n",
      "Iteration 101, loss = 0.63221092\n",
      "Iteration 102, loss = 0.63153806\n",
      "Iteration 103, loss = 0.63086098\n",
      "Iteration 104, loss = 0.63018889\n",
      "Iteration 105, loss = 0.62951403\n",
      "Iteration 106, loss = 0.62883894\n",
      "Iteration 107, loss = 0.62816531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 108, loss = 0.62749496\n",
      "Iteration 109, loss = 0.62681847\n",
      "Iteration 110, loss = 0.62614318\n",
      "Iteration 111, loss = 0.62546714\n",
      "Iteration 112, loss = 0.62479133\n",
      "Iteration 113, loss = 0.62411261\n",
      "Iteration 114, loss = 0.62343344\n",
      "Iteration 115, loss = 0.62276051\n",
      "Iteration 116, loss = 0.62208374\n",
      "Iteration 117, loss = 0.62140005\n",
      "Iteration 118, loss = 0.62072590\n",
      "Iteration 119, loss = 0.62004279\n",
      "Iteration 120, loss = 0.61936471\n",
      "Iteration 121, loss = 0.61869073\n",
      "Iteration 122, loss = 0.61801279\n",
      "Iteration 123, loss = 0.61733889\n",
      "Iteration 124, loss = 0.61666368\n",
      "Iteration 125, loss = 0.61598988\n",
      "Iteration 126, loss = 0.61532084\n",
      "Iteration 127, loss = 0.61464553\n",
      "Iteration 128, loss = 0.61397443\n",
      "Iteration 129, loss = 0.61330413\n",
      "Iteration 130, loss = 0.61264219\n",
      "Iteration 131, loss = 0.61196752\n",
      "Iteration 132, loss = 0.61130523\n",
      "Iteration 133, loss = 0.61064074\n",
      "Iteration 134, loss = 0.60997687\n",
      "Iteration 135, loss = 0.60931734\n",
      "Iteration 136, loss = 0.60865201\n",
      "Iteration 137, loss = 0.60798960\n",
      "Iteration 138, loss = 0.60732762\n",
      "Iteration 139, loss = 0.60667294\n",
      "Iteration 140, loss = 0.60601473\n",
      "Iteration 141, loss = 0.60536339\n",
      "Iteration 142, loss = 0.60470377\n",
      "Iteration 143, loss = 0.60405379\n",
      "Iteration 144, loss = 0.60339886\n",
      "Iteration 145, loss = 0.60274542\n",
      "Iteration 146, loss = 0.60209260\n",
      "Iteration 147, loss = 0.60144159\n",
      "Iteration 148, loss = 0.60079014\n",
      "Iteration 149, loss = 0.60013774\n",
      "Iteration 150, loss = 0.59948980\n",
      "Iteration 151, loss = 0.59884376\n",
      "Iteration 152, loss = 0.59819802\n",
      "Iteration 153, loss = 0.59755550\n",
      "Iteration 154, loss = 0.59691431\n",
      "Iteration 155, loss = 0.59627564\n",
      "Iteration 156, loss = 0.59564007\n",
      "Iteration 157, loss = 0.59500902\n",
      "Iteration 158, loss = 0.59438261\n",
      "Iteration 159, loss = 0.59374762\n",
      "Iteration 160, loss = 0.59311899\n",
      "Iteration 161, loss = 0.59249492\n",
      "Iteration 162, loss = 0.59187054\n",
      "Iteration 163, loss = 0.59124837\n",
      "Iteration 164, loss = 0.59062805\n",
      "Iteration 165, loss = 0.59001341\n",
      "Iteration 166, loss = 0.58939449\n",
      "Iteration 167, loss = 0.58877654\n",
      "Iteration 168, loss = 0.58816702\n",
      "Iteration 169, loss = 0.58755546\n",
      "Iteration 170, loss = 0.58694338\n",
      "Iteration 171, loss = 0.58633565\n",
      "Iteration 172, loss = 0.58573396\n",
      "Iteration 173, loss = 0.58512682\n",
      "Iteration 174, loss = 0.58452498\n",
      "Iteration 175, loss = 0.58392951\n",
      "Iteration 176, loss = 0.58332849\n",
      "Iteration 177, loss = 0.58273206\n",
      "Iteration 178, loss = 0.58213632\n",
      "Iteration 179, loss = 0.58154692\n",
      "Iteration 180, loss = 0.58096133\n",
      "Iteration 181, loss = 0.58037676\n",
      "Iteration 182, loss = 0.57979447\n",
      "Iteration 183, loss = 0.57921788\n",
      "Iteration 184, loss = 0.57864246\n",
      "Iteration 185, loss = 0.57807230\n",
      "Iteration 186, loss = 0.57751429\n",
      "Iteration 187, loss = 0.57694163\n",
      "Iteration 188, loss = 0.57637296\n",
      "Iteration 189, loss = 0.57581249\n",
      "Iteration 190, loss = 0.57524996\n",
      "Iteration 191, loss = 0.57469515\n",
      "Iteration 192, loss = 0.57413754\n",
      "Iteration 193, loss = 0.57358979\n",
      "Iteration 194, loss = 0.57304626\n",
      "Iteration 195, loss = 0.57249501\n",
      "Iteration 196, loss = 0.57195452\n",
      "Iteration 197, loss = 0.57141762\n",
      "Iteration 198, loss = 0.57088117\n",
      "Iteration 199, loss = 0.57034473\n",
      "Iteration 200, loss = 0.56981079\n",
      "Iteration 201, loss = 0.56928455\n",
      "Iteration 202, loss = 0.56875786\n",
      "Iteration 203, loss = 0.56823679\n",
      "Iteration 204, loss = 0.56771939\n",
      "Iteration 205, loss = 0.56719790\n",
      "Iteration 206, loss = 0.56668085\n",
      "Iteration 207, loss = 0.56618589\n",
      "Iteration 208, loss = 0.56566516\n",
      "Iteration 209, loss = 0.56515339\n",
      "Iteration 210, loss = 0.56465242\n",
      "Iteration 211, loss = 0.56414344\n",
      "Iteration 212, loss = 0.56364796\n",
      "Iteration 213, loss = 0.56315113\n",
      "Iteration 214, loss = 0.56265150\n",
      "Iteration 215, loss = 0.56216100\n",
      "Iteration 216, loss = 0.56167506\n",
      "Iteration 217, loss = 0.56118491\n",
      "Iteration 218, loss = 0.56070535\n",
      "Iteration 219, loss = 0.56022012\n",
      "Iteration 220, loss = 0.55973775\n",
      "Iteration 221, loss = 0.55926277\n",
      "Iteration 222, loss = 0.55879024\n",
      "Iteration 223, loss = 0.55831777\n",
      "Iteration 224, loss = 0.55784903\n",
      "Iteration 225, loss = 0.55738293\n",
      "Iteration 226, loss = 0.55692390\n",
      "Iteration 227, loss = 0.55647598\n",
      "Iteration 228, loss = 0.55602046\n",
      "Iteration 229, loss = 0.55556950\n",
      "Iteration 230, loss = 0.55511899\n",
      "Iteration 231, loss = 0.55467643\n",
      "Iteration 232, loss = 0.55423499\n",
      "Iteration 233, loss = 0.55379738\n",
      "Iteration 234, loss = 0.55336570\n",
      "Iteration 235, loss = 0.55292676\n",
      "Iteration 236, loss = 0.55249892\n",
      "Iteration 237, loss = 0.55206800\n",
      "Iteration 238, loss = 0.55164698\n",
      "Iteration 239, loss = 0.55121854\n",
      "Iteration 240, loss = 0.55080248\n",
      "Iteration 241, loss = 0.55039016\n",
      "Iteration 242, loss = 0.54997558\n",
      "Iteration 243, loss = 0.54956240\n",
      "Iteration 244, loss = 0.54915584\n",
      "Iteration 245, loss = 0.54875266\n",
      "Iteration 246, loss = 0.54834788\n",
      "Iteration 247, loss = 0.54795080\n",
      "Iteration 248, loss = 0.54755859\n",
      "Iteration 249, loss = 0.54715858\n",
      "Iteration 250, loss = 0.54676197\n",
      "Iteration 251, loss = 0.54637527\n",
      "Iteration 252, loss = 0.54598403\n",
      "Iteration 253, loss = 0.54560915\n",
      "Iteration 254, loss = 0.54522126\n",
      "Iteration 255, loss = 0.54484391\n",
      "Iteration 256, loss = 0.54447064\n",
      "Iteration 257, loss = 0.54409639\n",
      "Iteration 258, loss = 0.54372437\n",
      "Iteration 259, loss = 0.54335940\n",
      "Iteration 260, loss = 0.54299272\n",
      "Iteration 261, loss = 0.54262689\n",
      "Iteration 262, loss = 0.54226759\n",
      "Iteration 263, loss = 0.54190722\n",
      "Iteration 264, loss = 0.54155414\n",
      "Iteration 265, loss = 0.54120610\n",
      "Iteration 266, loss = 0.54085499\n",
      "Iteration 267, loss = 0.54051113\n",
      "Iteration 268, loss = 0.54016735\n",
      "Iteration 269, loss = 0.53982573\n",
      "Iteration 270, loss = 0.53948302\n",
      "Iteration 271, loss = 0.53914807\n",
      "Iteration 272, loss = 0.53882912\n",
      "Iteration 273, loss = 0.53848923\n",
      "Iteration 274, loss = 0.53815979\n",
      "Iteration 275, loss = 0.53783436\n",
      "Iteration 276, loss = 0.53751663\n",
      "Iteration 277, loss = 0.53719120\n",
      "Iteration 278, loss = 0.53687923\n",
      "Iteration 279, loss = 0.53656685\n",
      "Iteration 280, loss = 0.53624846\n",
      "Iteration 281, loss = 0.53594472\n",
      "Iteration 282, loss = 0.53564182\n",
      "Iteration 283, loss = 0.53533169\n",
      "Iteration 284, loss = 0.53504163\n",
      "Iteration 285, loss = 0.53473909\n",
      "Iteration 286, loss = 0.53444005\n",
      "Iteration 287, loss = 0.53415412\n",
      "Iteration 288, loss = 0.53386436\n",
      "Iteration 289, loss = 0.53358846\n",
      "Iteration 290, loss = 0.53331134\n",
      "Iteration 291, loss = 0.53301631\n",
      "Iteration 292, loss = 0.53275797\n",
      "Iteration 293, loss = 0.53247258\n",
      "Iteration 294, loss = 0.53219479\n",
      "Iteration 295, loss = 0.53192647\n",
      "Iteration 296, loss = 0.53166517\n",
      "Iteration 297, loss = 0.53139861\n",
      "Iteration 298, loss = 0.53113601\n",
      "Iteration 299, loss = 0.53088081\n",
      "Iteration 300, loss = 0.53062091\n",
      "Iteration 301, loss = 0.53036771\n",
      "Iteration 302, loss = 0.53010759\n",
      "Iteration 303, loss = 0.52986082\n",
      "Iteration 304, loss = 0.52961003\n",
      "Iteration 305, loss = 0.52936300\n",
      "Iteration 306, loss = 0.52911977\n",
      "Iteration 307, loss = 0.52887675\n",
      "Iteration 308, loss = 0.52863610\n",
      "Iteration 309, loss = 0.52840022\n",
      "Iteration 310, loss = 0.52816396\n",
      "Iteration 311, loss = 0.52792949\n",
      "Iteration 312, loss = 0.52769698\n",
      "Iteration 313, loss = 0.52747041\n",
      "Iteration 314, loss = 0.52723892\n",
      "Iteration 315, loss = 0.52701633\n",
      "Iteration 316, loss = 0.52679191\n",
      "Iteration 317, loss = 0.52657790\n",
      "Iteration 318, loss = 0.52635721\n",
      "Iteration 319, loss = 0.52614642\n",
      "Iteration 320, loss = 0.52592590\n",
      "Iteration 321, loss = 0.52571893\n",
      "Iteration 322, loss = 0.52550545\n",
      "Iteration 323, loss = 0.52529444\n",
      "Iteration 324, loss = 0.52509125\n",
      "Iteration 325, loss = 0.52488676\n",
      "Iteration 326, loss = 0.52468561\n",
      "Iteration 327, loss = 0.52448594\n",
      "Iteration 328, loss = 0.52428832\n",
      "Iteration 329, loss = 0.52409166\n",
      "Iteration 330, loss = 0.52389897\n",
      "Iteration 331, loss = 0.52370701\n",
      "Iteration 332, loss = 0.52352127\n",
      "Iteration 333, loss = 0.52333415\n",
      "Iteration 334, loss = 0.52314910\n",
      "Iteration 335, loss = 0.52296464\n",
      "Iteration 336, loss = 0.52277592\n",
      "Iteration 337, loss = 0.52259770\n",
      "Iteration 338, loss = 0.52241946\n",
      "Iteration 339, loss = 0.52225502\n",
      "Iteration 340, loss = 0.52207138\n",
      "Iteration 341, loss = 0.52190160\n",
      "Iteration 342, loss = 0.52173382\n",
      "Iteration 343, loss = 0.52156504\n",
      "Iteration 344, loss = 0.52139946\n",
      "Iteration 345, loss = 0.52124001\n",
      "Iteration 346, loss = 0.52107501\n",
      "Iteration 347, loss = 0.52091528\n",
      "Iteration 348, loss = 0.52075624\n",
      "Iteration 349, loss = 0.52060102\n",
      "Iteration 350, loss = 0.52044796\n",
      "Iteration 351, loss = 0.52029873\n",
      "Iteration 352, loss = 0.52014962\n",
      "Iteration 353, loss = 0.51999759\n",
      "Iteration 354, loss = 0.51984356\n",
      "Iteration 355, loss = 0.51971606\n",
      "Iteration 356, loss = 0.51955631\n",
      "Iteration 357, loss = 0.51941671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 358, loss = 0.51927671\n",
      "Iteration 359, loss = 0.51913385\n",
      "Iteration 360, loss = 0.51899753\n",
      "Iteration 361, loss = 0.51886336\n",
      "Iteration 362, loss = 0.51872845\n",
      "Iteration 363, loss = 0.51859679\n",
      "Iteration 364, loss = 0.51846253\n",
      "Iteration 365, loss = 0.51833727\n",
      "Iteration 366, loss = 0.51821095\n",
      "Iteration 367, loss = 0.51808257\n",
      "Iteration 368, loss = 0.51795687\n",
      "Iteration 369, loss = 0.51783540\n",
      "Iteration 370, loss = 0.51771578\n",
      "Iteration 371, loss = 0.51760610\n",
      "Iteration 372, loss = 0.51747826\n",
      "Iteration 373, loss = 0.51735872\n",
      "Iteration 374, loss = 0.51724474\n",
      "Iteration 375, loss = 0.51713087\n",
      "Iteration 376, loss = 0.51702496\n",
      "Iteration 377, loss = 0.51690490\n",
      "Iteration 378, loss = 0.51679189\n",
      "Iteration 379, loss = 0.51668677\n",
      "Iteration 380, loss = 0.51658411\n",
      "Iteration 381, loss = 0.51647567\n",
      "Iteration 382, loss = 0.51637414\n",
      "Iteration 383, loss = 0.51626813\n",
      "Iteration 384, loss = 0.51616506\n",
      "Iteration 385, loss = 0.51607063\n",
      "Iteration 386, loss = 0.51596511\n",
      "Iteration 387, loss = 0.51587970\n",
      "Iteration 388, loss = 0.51578103\n",
      "Iteration 389, loss = 0.51568577\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74969870\n",
      "Iteration 2, loss = 0.74493354\n",
      "Iteration 3, loss = 0.74037079\n",
      "Iteration 4, loss = 0.73599192\n",
      "Iteration 5, loss = 0.73180007\n",
      "Iteration 6, loss = 0.72778373\n",
      "Iteration 7, loss = 0.72394651\n",
      "Iteration 8, loss = 0.72029849\n",
      "Iteration 9, loss = 0.71680870\n",
      "Iteration 10, loss = 0.71347888\n",
      "Iteration 11, loss = 0.71033034\n",
      "Iteration 12, loss = 0.70734475\n",
      "Iteration 13, loss = 0.70451618\n",
      "Iteration 14, loss = 0.70182801\n",
      "Iteration 15, loss = 0.69927893\n",
      "Iteration 16, loss = 0.69688338\n",
      "Iteration 17, loss = 0.69460154\n",
      "Iteration 18, loss = 0.69245936\n",
      "Iteration 19, loss = 0.69043583\n",
      "Iteration 20, loss = 0.68853777\n",
      "Iteration 21, loss = 0.68674021\n",
      "Iteration 22, loss = 0.68505385\n",
      "Iteration 23, loss = 0.68347117\n",
      "Iteration 24, loss = 0.68197733\n",
      "Iteration 25, loss = 0.68057119\n",
      "Iteration 26, loss = 0.67922222\n",
      "Iteration 27, loss = 0.67796006\n",
      "Iteration 28, loss = 0.67677266\n",
      "Iteration 29, loss = 0.67563399\n",
      "Iteration 30, loss = 0.67455997\n",
      "Iteration 31, loss = 0.67353829\n",
      "Iteration 32, loss = 0.67255593\n",
      "Iteration 33, loss = 0.67161607\n",
      "Iteration 34, loss = 0.67069766\n",
      "Iteration 35, loss = 0.66981908\n",
      "Iteration 36, loss = 0.66896507\n",
      "Iteration 37, loss = 0.66813003\n",
      "Iteration 38, loss = 0.66731765\n",
      "Iteration 39, loss = 0.66651769\n",
      "Iteration 40, loss = 0.66573483\n",
      "Iteration 41, loss = 0.66495727\n",
      "Iteration 42, loss = 0.66419018\n",
      "Iteration 43, loss = 0.66343055\n",
      "Iteration 44, loss = 0.66267358\n",
      "Iteration 45, loss = 0.66192341\n",
      "Iteration 46, loss = 0.66116911\n",
      "Iteration 47, loss = 0.66042169\n",
      "Iteration 48, loss = 0.65967231\n",
      "Iteration 49, loss = 0.65892042\n",
      "Iteration 50, loss = 0.65816838\n",
      "Iteration 51, loss = 0.65741370\n",
      "Iteration 52, loss = 0.65665501\n",
      "Iteration 53, loss = 0.65589415\n",
      "Iteration 54, loss = 0.65513199\n",
      "Iteration 55, loss = 0.65436501\n",
      "Iteration 56, loss = 0.65359285\n",
      "Iteration 57, loss = 0.65281899\n",
      "Iteration 58, loss = 0.65204242\n",
      "Iteration 59, loss = 0.65126345\n",
      "Iteration 60, loss = 0.65047701\n",
      "Iteration 61, loss = 0.64969185\n",
      "Iteration 62, loss = 0.64889959\n",
      "Iteration 63, loss = 0.64810744\n",
      "Iteration 64, loss = 0.64731055\n",
      "Iteration 65, loss = 0.64651680\n",
      "Iteration 66, loss = 0.64571744\n",
      "Iteration 67, loss = 0.64491320\n",
      "Iteration 68, loss = 0.64410824\n",
      "Iteration 69, loss = 0.64330099\n",
      "Iteration 70, loss = 0.64248984\n",
      "Iteration 71, loss = 0.64167747\n",
      "Iteration 72, loss = 0.64086336\n",
      "Iteration 73, loss = 0.64004550\n",
      "Iteration 74, loss = 0.63922598\n",
      "Iteration 75, loss = 0.63840196\n",
      "Iteration 76, loss = 0.63757222\n",
      "Iteration 77, loss = 0.63674635\n",
      "Iteration 78, loss = 0.63591224\n",
      "Iteration 79, loss = 0.63507941\n",
      "Iteration 80, loss = 0.63424638\n",
      "Iteration 81, loss = 0.63341010\n",
      "Iteration 82, loss = 0.63257441\n",
      "Iteration 83, loss = 0.63174262\n",
      "Iteration 84, loss = 0.63091141\n",
      "Iteration 85, loss = 0.63007003\n",
      "Iteration 86, loss = 0.62923315\n",
      "Iteration 87, loss = 0.62839716\n",
      "Iteration 88, loss = 0.62755692\n",
      "Iteration 89, loss = 0.62671472\n",
      "Iteration 90, loss = 0.62587804\n",
      "Iteration 91, loss = 0.62503868\n",
      "Iteration 92, loss = 0.62419931\n",
      "Iteration 93, loss = 0.62336095\n",
      "Iteration 94, loss = 0.62252359\n",
      "Iteration 95, loss = 0.62168051\n",
      "Iteration 96, loss = 0.62085252\n",
      "Iteration 97, loss = 0.62000950\n",
      "Iteration 98, loss = 0.61917994\n",
      "Iteration 99, loss = 0.61833640\n",
      "Iteration 100, loss = 0.61750385\n",
      "Iteration 101, loss = 0.61666638\n",
      "Iteration 102, loss = 0.61582599\n",
      "Iteration 103, loss = 0.61498384\n",
      "Iteration 104, loss = 0.61414552\n",
      "Iteration 105, loss = 0.61330434\n",
      "Iteration 106, loss = 0.61246265\n",
      "Iteration 107, loss = 0.61162407\n",
      "Iteration 108, loss = 0.61078641\n",
      "Iteration 109, loss = 0.60994588\n",
      "Iteration 110, loss = 0.60910531\n",
      "Iteration 111, loss = 0.60826413\n",
      "Iteration 112, loss = 0.60742177\n",
      "Iteration 113, loss = 0.60657871\n",
      "Iteration 114, loss = 0.60573650\n",
      "Iteration 115, loss = 0.60489909\n",
      "Iteration 116, loss = 0.60406030\n",
      "Iteration 117, loss = 0.60321383\n",
      "Iteration 118, loss = 0.60237937\n",
      "Iteration 119, loss = 0.60153112\n",
      "Iteration 120, loss = 0.60069090\n",
      "Iteration 121, loss = 0.59985060\n",
      "Iteration 122, loss = 0.59901153\n",
      "Iteration 123, loss = 0.59817052\n",
      "Iteration 124, loss = 0.59733154\n",
      "Iteration 125, loss = 0.59649532\n",
      "Iteration 126, loss = 0.59566190\n",
      "Iteration 127, loss = 0.59482413\n",
      "Iteration 128, loss = 0.59399222\n",
      "Iteration 129, loss = 0.59315805\n",
      "Iteration 130, loss = 0.59233549\n",
      "Iteration 131, loss = 0.59150305\n",
      "Iteration 132, loss = 0.59067884\n",
      "Iteration 133, loss = 0.58985123\n",
      "Iteration 134, loss = 0.58902737\n",
      "Iteration 135, loss = 0.58820627\n",
      "Iteration 136, loss = 0.58738150\n",
      "Iteration 137, loss = 0.58655630\n",
      "Iteration 138, loss = 0.58573532\n",
      "Iteration 139, loss = 0.58491798\n",
      "Iteration 140, loss = 0.58410424\n",
      "Iteration 141, loss = 0.58329209\n",
      "Iteration 142, loss = 0.58247676\n",
      "Iteration 143, loss = 0.58166986\n",
      "Iteration 144, loss = 0.58085677\n",
      "Iteration 145, loss = 0.58004876\n",
      "Iteration 146, loss = 0.57923968\n",
      "Iteration 147, loss = 0.57843513\n",
      "Iteration 148, loss = 0.57762876\n",
      "Iteration 149, loss = 0.57682343\n",
      "Iteration 150, loss = 0.57601985\n",
      "Iteration 151, loss = 0.57522310\n",
      "Iteration 152, loss = 0.57442443\n",
      "Iteration 153, loss = 0.57363245\n",
      "Iteration 154, loss = 0.57284543\n",
      "Iteration 155, loss = 0.57205966\n",
      "Iteration 156, loss = 0.57127815\n",
      "Iteration 157, loss = 0.57050066\n",
      "Iteration 158, loss = 0.56972994\n",
      "Iteration 159, loss = 0.56895311\n",
      "Iteration 160, loss = 0.56818240\n",
      "Iteration 161, loss = 0.56741489\n",
      "Iteration 162, loss = 0.56665048\n",
      "Iteration 163, loss = 0.56588831\n",
      "Iteration 164, loss = 0.56513015\n",
      "Iteration 165, loss = 0.56437579\n",
      "Iteration 166, loss = 0.56362182\n",
      "Iteration 167, loss = 0.56286754\n",
      "Iteration 168, loss = 0.56211925\n",
      "Iteration 169, loss = 0.56137076\n",
      "Iteration 170, loss = 0.56062438\n",
      "Iteration 171, loss = 0.55988297\n",
      "Iteration 172, loss = 0.55914649\n",
      "Iteration 173, loss = 0.55840997\n",
      "Iteration 174, loss = 0.55767844\n",
      "Iteration 175, loss = 0.55695613\n",
      "Iteration 176, loss = 0.55622881\n",
      "Iteration 177, loss = 0.55551012\n",
      "Iteration 178, loss = 0.55479182\n",
      "Iteration 179, loss = 0.55408262\n",
      "Iteration 180, loss = 0.55337546\n",
      "Iteration 181, loss = 0.55267064\n",
      "Iteration 182, loss = 0.55196811\n",
      "Iteration 183, loss = 0.55127364\n",
      "Iteration 184, loss = 0.55058020\n",
      "Iteration 185, loss = 0.54989763\n",
      "Iteration 186, loss = 0.54922265\n",
      "Iteration 187, loss = 0.54854052\n",
      "Iteration 188, loss = 0.54785314\n",
      "Iteration 189, loss = 0.54718218\n",
      "Iteration 190, loss = 0.54650703\n",
      "Iteration 191, loss = 0.54584178\n",
      "Iteration 192, loss = 0.54517465\n",
      "Iteration 193, loss = 0.54452029\n",
      "Iteration 194, loss = 0.54386604\n",
      "Iteration 195, loss = 0.54320764\n",
      "Iteration 196, loss = 0.54256063\n",
      "Iteration 197, loss = 0.54191922\n",
      "Iteration 198, loss = 0.54127971\n",
      "Iteration 199, loss = 0.54064039\n",
      "Iteration 200, loss = 0.54000756\n",
      "Iteration 201, loss = 0.53938128\n",
      "Iteration 202, loss = 0.53875639\n",
      "Iteration 203, loss = 0.53813797\n",
      "Iteration 204, loss = 0.53752332\n",
      "Iteration 205, loss = 0.53690687\n",
      "Iteration 206, loss = 0.53629673\n",
      "Iteration 207, loss = 0.53570666\n",
      "Iteration 208, loss = 0.53510097\n",
      "Iteration 209, loss = 0.53449898\n",
      "Iteration 210, loss = 0.53391001\n",
      "Iteration 211, loss = 0.53331622\n",
      "Iteration 212, loss = 0.53273310\n",
      "Iteration 213, loss = 0.53215087\n",
      "Iteration 214, loss = 0.53156969\n",
      "Iteration 215, loss = 0.53099533\n",
      "Iteration 216, loss = 0.53043008\n",
      "Iteration 217, loss = 0.52986429\n",
      "Iteration 218, loss = 0.52930834\n",
      "Iteration 219, loss = 0.52874603\n",
      "Iteration 220, loss = 0.52819488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 221, loss = 0.52764295\n",
      "Iteration 222, loss = 0.52709787\n",
      "Iteration 223, loss = 0.52655312\n",
      "Iteration 224, loss = 0.52601521\n",
      "Iteration 225, loss = 0.52548040\n",
      "Iteration 226, loss = 0.52495371\n",
      "Iteration 227, loss = 0.52443698\n",
      "Iteration 228, loss = 0.52391721\n",
      "Iteration 229, loss = 0.52340274\n",
      "Iteration 230, loss = 0.52289157\n",
      "Iteration 231, loss = 0.52238319\n",
      "Iteration 232, loss = 0.52188021\n",
      "Iteration 233, loss = 0.52138438\n",
      "Iteration 234, loss = 0.52089331\n",
      "Iteration 235, loss = 0.52039828\n",
      "Iteration 236, loss = 0.51991639\n",
      "Iteration 237, loss = 0.51942955\n",
      "Iteration 238, loss = 0.51895508\n",
      "Iteration 239, loss = 0.51847432\n",
      "Iteration 240, loss = 0.51801107\n",
      "Iteration 241, loss = 0.51754582\n",
      "Iteration 242, loss = 0.51708384\n",
      "Iteration 243, loss = 0.51662733\n",
      "Iteration 244, loss = 0.51617422\n",
      "Iteration 245, loss = 0.51572674\n",
      "Iteration 246, loss = 0.51527982\n",
      "Iteration 247, loss = 0.51483940\n",
      "Iteration 248, loss = 0.51440305\n",
      "Iteration 249, loss = 0.51396655\n",
      "Iteration 250, loss = 0.51352910\n",
      "Iteration 251, loss = 0.51310256\n",
      "Iteration 252, loss = 0.51267641\n",
      "Iteration 253, loss = 0.51226232\n",
      "Iteration 254, loss = 0.51184192\n",
      "Iteration 255, loss = 0.51143147\n",
      "Iteration 256, loss = 0.51102236\n",
      "Iteration 257, loss = 0.51061582\n",
      "Iteration 258, loss = 0.51021061\n",
      "Iteration 259, loss = 0.50981258\n",
      "Iteration 260, loss = 0.50941513\n",
      "Iteration 261, loss = 0.50902182\n",
      "Iteration 262, loss = 0.50863272\n",
      "Iteration 263, loss = 0.50824613\n",
      "Iteration 264, loss = 0.50786578\n",
      "Iteration 265, loss = 0.50749017\n",
      "Iteration 266, loss = 0.50711315\n",
      "Iteration 267, loss = 0.50674796\n",
      "Iteration 268, loss = 0.50638065\n",
      "Iteration 269, loss = 0.50601779\n",
      "Iteration 270, loss = 0.50565556\n",
      "Iteration 271, loss = 0.50529859\n",
      "Iteration 272, loss = 0.50495483\n",
      "Iteration 273, loss = 0.50460132\n",
      "Iteration 274, loss = 0.50425230\n",
      "Iteration 275, loss = 0.50391203\n",
      "Iteration 276, loss = 0.50357407\n",
      "Iteration 277, loss = 0.50323618\n",
      "Iteration 278, loss = 0.50290780\n",
      "Iteration 279, loss = 0.50258176\n",
      "Iteration 280, loss = 0.50225170\n",
      "Iteration 281, loss = 0.50193093\n",
      "Iteration 282, loss = 0.50161404\n",
      "Iteration 283, loss = 0.50129146\n",
      "Iteration 284, loss = 0.50099187\n",
      "Iteration 285, loss = 0.50067774\n",
      "Iteration 286, loss = 0.50036927\n",
      "Iteration 287, loss = 0.50007181\n",
      "Iteration 288, loss = 0.49977105\n",
      "Iteration 289, loss = 0.49948781\n",
      "Iteration 290, loss = 0.49920773\n",
      "Iteration 291, loss = 0.49890063\n",
      "Iteration 292, loss = 0.49863050\n",
      "Iteration 293, loss = 0.49834400\n",
      "Iteration 294, loss = 0.49806050\n",
      "Iteration 295, loss = 0.49778637\n",
      "Iteration 296, loss = 0.49752345\n",
      "Iteration 297, loss = 0.49725126\n",
      "Iteration 298, loss = 0.49698667\n",
      "Iteration 299, loss = 0.49672795\n",
      "Iteration 300, loss = 0.49646591\n",
      "Iteration 301, loss = 0.49620904\n",
      "Iteration 302, loss = 0.49595174\n",
      "Iteration 303, loss = 0.49570101\n",
      "Iteration 304, loss = 0.49545375\n",
      "Iteration 305, loss = 0.49520468\n",
      "Iteration 306, loss = 0.49496444\n",
      "Iteration 307, loss = 0.49471952\n",
      "Iteration 308, loss = 0.49448239\n",
      "Iteration 309, loss = 0.49425109\n",
      "Iteration 310, loss = 0.49401186\n",
      "Iteration 311, loss = 0.49378478\n",
      "Iteration 312, loss = 0.49355362\n",
      "Iteration 313, loss = 0.49333024\n",
      "Iteration 314, loss = 0.49310162\n",
      "Iteration 315, loss = 0.49288461\n",
      "Iteration 316, loss = 0.49266481\n",
      "Iteration 317, loss = 0.49245803\n",
      "Iteration 318, loss = 0.49223975\n",
      "Iteration 319, loss = 0.49203258\n",
      "Iteration 320, loss = 0.49182098\n",
      "Iteration 321, loss = 0.49161592\n",
      "Iteration 322, loss = 0.49141143\n",
      "Iteration 323, loss = 0.49120774\n",
      "Iteration 324, loss = 0.49101013\n",
      "Iteration 325, loss = 0.49081241\n",
      "Iteration 326, loss = 0.49061953\n",
      "Iteration 327, loss = 0.49042793\n",
      "Iteration 328, loss = 0.49023653\n",
      "Iteration 329, loss = 0.49005360\n",
      "Iteration 330, loss = 0.48986471\n",
      "Iteration 331, loss = 0.48968277\n",
      "Iteration 332, loss = 0.48950226\n",
      "Iteration 333, loss = 0.48932442\n",
      "Iteration 334, loss = 0.48915092\n",
      "Iteration 335, loss = 0.48897936\n",
      "Iteration 336, loss = 0.48879633\n",
      "Iteration 337, loss = 0.48863112\n",
      "Iteration 338, loss = 0.48846201\n",
      "Iteration 339, loss = 0.48830836\n",
      "Iteration 340, loss = 0.48813581\n",
      "Iteration 341, loss = 0.48797447\n",
      "Iteration 342, loss = 0.48781742\n",
      "Iteration 343, loss = 0.48766015\n",
      "Iteration 344, loss = 0.48750782\n",
      "Iteration 345, loss = 0.48735772\n",
      "Iteration 346, loss = 0.48720345\n",
      "Iteration 347, loss = 0.48705593\n",
      "Iteration 348, loss = 0.48691082\n",
      "Iteration 349, loss = 0.48676459\n",
      "Iteration 350, loss = 0.48662347\n",
      "Iteration 351, loss = 0.48648743\n",
      "Iteration 352, loss = 0.48635243\n",
      "Iteration 353, loss = 0.48621703\n",
      "Iteration 354, loss = 0.48607456\n",
      "Iteration 355, loss = 0.48595815\n",
      "Iteration 356, loss = 0.48581199\n",
      "Iteration 357, loss = 0.48568534\n",
      "Iteration 358, loss = 0.48555623\n",
      "Iteration 359, loss = 0.48542905\n",
      "Iteration 360, loss = 0.48530444\n",
      "Iteration 361, loss = 0.48518586\n",
      "Iteration 362, loss = 0.48506025\n",
      "Iteration 363, loss = 0.48494288\n",
      "Iteration 364, loss = 0.48482199\n",
      "Iteration 365, loss = 0.48470851\n",
      "Iteration 366, loss = 0.48459458\n",
      "Iteration 367, loss = 0.48448521\n",
      "Iteration 368, loss = 0.48437038\n",
      "Iteration 369, loss = 0.48426658\n",
      "Iteration 370, loss = 0.48415804\n",
      "Iteration 371, loss = 0.48406635\n",
      "Iteration 372, loss = 0.48394536\n",
      "Iteration 373, loss = 0.48384024\n",
      "Iteration 374, loss = 0.48373957\n",
      "Iteration 375, loss = 0.48363881\n",
      "Iteration 376, loss = 0.48354934\n",
      "Iteration 377, loss = 0.48344254\n",
      "Iteration 378, loss = 0.48334133\n",
      "Iteration 379, loss = 0.48324712\n",
      "Iteration 380, loss = 0.48316084\n",
      "Iteration 381, loss = 0.48306527\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76535648\n",
      "Iteration 2, loss = 0.75971304\n",
      "Iteration 3, loss = 0.75427926\n",
      "Iteration 4, loss = 0.74904084\n",
      "Iteration 5, loss = 0.74399433\n",
      "Iteration 6, loss = 0.73912894\n",
      "Iteration 7, loss = 0.73445138\n",
      "Iteration 8, loss = 0.72996593\n",
      "Iteration 9, loss = 0.72565270\n",
      "Iteration 10, loss = 0.72150844\n",
      "Iteration 11, loss = 0.71755042\n",
      "Iteration 12, loss = 0.71376726\n",
      "Iteration 13, loss = 0.71015868\n",
      "Iteration 14, loss = 0.70670288\n",
      "Iteration 15, loss = 0.70340125\n",
      "Iteration 16, loss = 0.70026535\n",
      "Iteration 17, loss = 0.69726373\n",
      "Iteration 18, loss = 0.69443255\n",
      "Iteration 19, loss = 0.69174188\n",
      "Iteration 20, loss = 0.68920707\n",
      "Iteration 21, loss = 0.68679029\n",
      "Iteration 22, loss = 0.68451150\n",
      "Iteration 23, loss = 0.68236284\n",
      "Iteration 24, loss = 0.68033326\n",
      "Iteration 25, loss = 0.67841588\n",
      "Iteration 26, loss = 0.67659272\n",
      "Iteration 27, loss = 0.67488159\n",
      "Iteration 28, loss = 0.67327552\n",
      "Iteration 29, loss = 0.67175118\n",
      "Iteration 30, loss = 0.67031947\n",
      "Iteration 31, loss = 0.66897234\n",
      "Iteration 32, loss = 0.66769596\n",
      "Iteration 33, loss = 0.66648685\n",
      "Iteration 34, loss = 0.66532743\n",
      "Iteration 35, loss = 0.66423581\n",
      "Iteration 36, loss = 0.66319315\n",
      "Iteration 37, loss = 0.66219363\n",
      "Iteration 38, loss = 0.66124371\n",
      "Iteration 39, loss = 0.66032392\n",
      "Iteration 40, loss = 0.65944400\n",
      "Iteration 41, loss = 0.65858416\n",
      "Iteration 42, loss = 0.65775198\n",
      "Iteration 43, loss = 0.65694259\n",
      "Iteration 44, loss = 0.65614370\n",
      "Iteration 45, loss = 0.65536599\n",
      "Iteration 46, loss = 0.65459202\n",
      "Iteration 47, loss = 0.65383270\n",
      "Iteration 48, loss = 0.65308143\n",
      "Iteration 49, loss = 0.65233061\n",
      "Iteration 50, loss = 0.65158696\n",
      "Iteration 51, loss = 0.65084417\n",
      "Iteration 52, loss = 0.65010022\n",
      "Iteration 53, loss = 0.64935647\n",
      "Iteration 54, loss = 0.64861465\n",
      "Iteration 55, loss = 0.64787173\n",
      "Iteration 56, loss = 0.64712344\n",
      "Iteration 57, loss = 0.64637550\n",
      "Iteration 58, loss = 0.64562375\n",
      "Iteration 59, loss = 0.64486967\n",
      "Iteration 60, loss = 0.64411053\n",
      "Iteration 61, loss = 0.64335024\n",
      "Iteration 62, loss = 0.64258309\n",
      "Iteration 63, loss = 0.64181525\n",
      "Iteration 64, loss = 0.64104283\n",
      "Iteration 65, loss = 0.64026823\n",
      "Iteration 66, loss = 0.63948773\n",
      "Iteration 67, loss = 0.63870469\n",
      "Iteration 68, loss = 0.63791651\n",
      "Iteration 69, loss = 0.63712492\n",
      "Iteration 70, loss = 0.63632913\n",
      "Iteration 71, loss = 0.63553000\n",
      "Iteration 72, loss = 0.63472496\n",
      "Iteration 73, loss = 0.63391882\n",
      "Iteration 74, loss = 0.63310765\n",
      "Iteration 75, loss = 0.63229122\n",
      "Iteration 76, loss = 0.63147345\n",
      "Iteration 77, loss = 0.63065300\n",
      "Iteration 78, loss = 0.62982623\n",
      "Iteration 79, loss = 0.62899994\n",
      "Iteration 80, loss = 0.62816997\n",
      "Iteration 81, loss = 0.62734114\n",
      "Iteration 82, loss = 0.62650763\n",
      "Iteration 83, loss = 0.62567655\n",
      "Iteration 84, loss = 0.62484954\n",
      "Iteration 85, loss = 0.62401018\n",
      "Iteration 86, loss = 0.62317639\n",
      "Iteration 87, loss = 0.62234391\n",
      "Iteration 88, loss = 0.62150423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89, loss = 0.62066317\n",
      "Iteration 90, loss = 0.61982654\n",
      "Iteration 91, loss = 0.61898540\n",
      "Iteration 92, loss = 0.61814546\n",
      "Iteration 93, loss = 0.61730454\n",
      "Iteration 94, loss = 0.61646067\n",
      "Iteration 95, loss = 0.61561137\n",
      "Iteration 96, loss = 0.61477812\n",
      "Iteration 97, loss = 0.61392953\n",
      "Iteration 98, loss = 0.61309430\n",
      "Iteration 99, loss = 0.61224346\n",
      "Iteration 100, loss = 0.61140470\n",
      "Iteration 101, loss = 0.61055985\n",
      "Iteration 102, loss = 0.60971590\n",
      "Iteration 103, loss = 0.60886761\n",
      "Iteration 104, loss = 0.60802219\n",
      "Iteration 105, loss = 0.60717787\n",
      "Iteration 106, loss = 0.60633218\n",
      "Iteration 107, loss = 0.60548818\n",
      "Iteration 108, loss = 0.60464578\n",
      "Iteration 109, loss = 0.60380028\n",
      "Iteration 110, loss = 0.60295550\n",
      "Iteration 111, loss = 0.60210615\n",
      "Iteration 112, loss = 0.60126032\n",
      "Iteration 113, loss = 0.60041913\n",
      "Iteration 114, loss = 0.59957178\n",
      "Iteration 115, loss = 0.59873208\n",
      "Iteration 116, loss = 0.59788939\n",
      "Iteration 117, loss = 0.59704911\n",
      "Iteration 118, loss = 0.59621383\n",
      "Iteration 119, loss = 0.59536755\n",
      "Iteration 120, loss = 0.59452405\n",
      "Iteration 121, loss = 0.59368682\n",
      "Iteration 122, loss = 0.59285291\n",
      "Iteration 123, loss = 0.59201582\n",
      "Iteration 124, loss = 0.59117762\n",
      "Iteration 125, loss = 0.59034168\n",
      "Iteration 126, loss = 0.58951133\n",
      "Iteration 127, loss = 0.58867369\n",
      "Iteration 128, loss = 0.58784471\n",
      "Iteration 129, loss = 0.58700876\n",
      "Iteration 130, loss = 0.58618387\n",
      "Iteration 131, loss = 0.58535283\n",
      "Iteration 132, loss = 0.58452591\n",
      "Iteration 133, loss = 0.58369445\n",
      "Iteration 134, loss = 0.58287038\n",
      "Iteration 135, loss = 0.58204575\n",
      "Iteration 136, loss = 0.58121913\n",
      "Iteration 137, loss = 0.58039157\n",
      "Iteration 138, loss = 0.57956703\n",
      "Iteration 139, loss = 0.57874779\n",
      "Iteration 140, loss = 0.57793176\n",
      "Iteration 141, loss = 0.57711619\n",
      "Iteration 142, loss = 0.57629897\n",
      "Iteration 143, loss = 0.57548899\n",
      "Iteration 144, loss = 0.57467466\n",
      "Iteration 145, loss = 0.57386717\n",
      "Iteration 146, loss = 0.57305536\n",
      "Iteration 147, loss = 0.57225355\n",
      "Iteration 148, loss = 0.57144597\n",
      "Iteration 149, loss = 0.57064278\n",
      "Iteration 150, loss = 0.56984471\n",
      "Iteration 151, loss = 0.56904872\n",
      "Iteration 152, loss = 0.56825085\n",
      "Iteration 153, loss = 0.56745942\n",
      "Iteration 154, loss = 0.56666985\n",
      "Iteration 155, loss = 0.56588457\n",
      "Iteration 156, loss = 0.56510239\n",
      "Iteration 157, loss = 0.56432115\n",
      "Iteration 158, loss = 0.56354341\n",
      "Iteration 159, loss = 0.56276475\n",
      "Iteration 160, loss = 0.56198697\n",
      "Iteration 161, loss = 0.56121358\n",
      "Iteration 162, loss = 0.56044560\n",
      "Iteration 163, loss = 0.55967262\n",
      "Iteration 164, loss = 0.55890377\n",
      "Iteration 165, loss = 0.55813740\n",
      "Iteration 166, loss = 0.55737504\n",
      "Iteration 167, loss = 0.55661309\n",
      "Iteration 168, loss = 0.55584897\n",
      "Iteration 169, loss = 0.55509138\n",
      "Iteration 170, loss = 0.55433527\n",
      "Iteration 171, loss = 0.55358306\n",
      "Iteration 172, loss = 0.55283992\n",
      "Iteration 173, loss = 0.55208950\n",
      "Iteration 174, loss = 0.55134741\n",
      "Iteration 175, loss = 0.55061111\n",
      "Iteration 176, loss = 0.54987096\n",
      "Iteration 177, loss = 0.54914107\n",
      "Iteration 178, loss = 0.54841016\n",
      "Iteration 179, loss = 0.54768467\n",
      "Iteration 180, loss = 0.54696869\n",
      "Iteration 181, loss = 0.54624615\n",
      "Iteration 182, loss = 0.54553112\n",
      "Iteration 183, loss = 0.54482550\n",
      "Iteration 184, loss = 0.54411719\n",
      "Iteration 185, loss = 0.54341648\n",
      "Iteration 186, loss = 0.54272947\n",
      "Iteration 187, loss = 0.54203221\n",
      "Iteration 188, loss = 0.54134071\n",
      "Iteration 189, loss = 0.54065430\n",
      "Iteration 190, loss = 0.53996723\n",
      "Iteration 191, loss = 0.53928777\n",
      "Iteration 192, loss = 0.53860547\n",
      "Iteration 193, loss = 0.53793561\n",
      "Iteration 194, loss = 0.53727214\n",
      "Iteration 195, loss = 0.53660392\n",
      "Iteration 196, loss = 0.53594564\n",
      "Iteration 197, loss = 0.53529193\n",
      "Iteration 198, loss = 0.53464118\n",
      "Iteration 199, loss = 0.53399438\n",
      "Iteration 200, loss = 0.53334583\n",
      "Iteration 201, loss = 0.53270732\n",
      "Iteration 202, loss = 0.53206965\n",
      "Iteration 203, loss = 0.53143667\n",
      "Iteration 204, loss = 0.53080828\n",
      "Iteration 205, loss = 0.53018005\n",
      "Iteration 206, loss = 0.52955585\n",
      "Iteration 207, loss = 0.52894355\n",
      "Iteration 208, loss = 0.52832887\n",
      "Iteration 209, loss = 0.52771480\n",
      "Iteration 210, loss = 0.52711286\n",
      "Iteration 211, loss = 0.52650739\n",
      "Iteration 212, loss = 0.52591456\n",
      "Iteration 213, loss = 0.52531383\n",
      "Iteration 214, loss = 0.52472128\n",
      "Iteration 215, loss = 0.52413230\n",
      "Iteration 216, loss = 0.52355474\n",
      "Iteration 217, loss = 0.52297463\n",
      "Iteration 218, loss = 0.52240215\n",
      "Iteration 219, loss = 0.52182670\n",
      "Iteration 220, loss = 0.52125755\n",
      "Iteration 221, loss = 0.52069269\n",
      "Iteration 222, loss = 0.52013070\n",
      "Iteration 223, loss = 0.51956660\n",
      "Iteration 224, loss = 0.51900921\n",
      "Iteration 225, loss = 0.51846021\n",
      "Iteration 226, loss = 0.51791493\n",
      "Iteration 227, loss = 0.51737826\n",
      "Iteration 228, loss = 0.51684073\n",
      "Iteration 229, loss = 0.51630722\n",
      "Iteration 230, loss = 0.51578335\n",
      "Iteration 231, loss = 0.51525751\n",
      "Iteration 232, loss = 0.51473358\n",
      "Iteration 233, loss = 0.51422208\n",
      "Iteration 234, loss = 0.51371137\n",
      "Iteration 235, loss = 0.51319887\n",
      "Iteration 236, loss = 0.51270119\n",
      "Iteration 237, loss = 0.51220136\n",
      "Iteration 238, loss = 0.51170670\n",
      "Iteration 239, loss = 0.51121105\n",
      "Iteration 240, loss = 0.51073286\n",
      "Iteration 241, loss = 0.51025131\n",
      "Iteration 242, loss = 0.50977354\n",
      "Iteration 243, loss = 0.50930139\n",
      "Iteration 244, loss = 0.50883312\n",
      "Iteration 245, loss = 0.50837104\n",
      "Iteration 246, loss = 0.50790693\n",
      "Iteration 247, loss = 0.50744799\n",
      "Iteration 248, loss = 0.50699236\n",
      "Iteration 249, loss = 0.50654805\n",
      "Iteration 250, loss = 0.50609675\n",
      "Iteration 251, loss = 0.50565130\n",
      "Iteration 252, loss = 0.50521064\n",
      "Iteration 253, loss = 0.50478106\n",
      "Iteration 254, loss = 0.50434633\n",
      "Iteration 255, loss = 0.50392461\n",
      "Iteration 256, loss = 0.50349866\n",
      "Iteration 257, loss = 0.50307366\n",
      "Iteration 258, loss = 0.50265431\n",
      "Iteration 259, loss = 0.50224191\n",
      "Iteration 260, loss = 0.50183232\n",
      "Iteration 261, loss = 0.50142156\n",
      "Iteration 262, loss = 0.50101877\n",
      "Iteration 263, loss = 0.50062136\n",
      "Iteration 264, loss = 0.50022575\n",
      "Iteration 265, loss = 0.49983621\n",
      "Iteration 266, loss = 0.49944541\n",
      "Iteration 267, loss = 0.49906460\n",
      "Iteration 268, loss = 0.49868309\n",
      "Iteration 269, loss = 0.49830617\n",
      "Iteration 270, loss = 0.49793097\n",
      "Iteration 271, loss = 0.49755973\n",
      "Iteration 272, loss = 0.49720434\n",
      "Iteration 273, loss = 0.49683528\n",
      "Iteration 274, loss = 0.49647133\n",
      "Iteration 275, loss = 0.49611678\n",
      "Iteration 276, loss = 0.49576843\n",
      "Iteration 277, loss = 0.49541507\n",
      "Iteration 278, loss = 0.49506991\n",
      "Iteration 279, loss = 0.49473013\n",
      "Iteration 280, loss = 0.49438707\n",
      "Iteration 281, loss = 0.49405631\n",
      "Iteration 282, loss = 0.49372569\n",
      "Iteration 283, loss = 0.49338893\n",
      "Iteration 284, loss = 0.49307219\n",
      "Iteration 285, loss = 0.49274877\n",
      "Iteration 286, loss = 0.49242027\n",
      "Iteration 287, loss = 0.49211231\n",
      "Iteration 288, loss = 0.49179992\n",
      "Iteration 289, loss = 0.49149566\n",
      "Iteration 290, loss = 0.49120342\n",
      "Iteration 291, loss = 0.49088718\n",
      "Iteration 292, loss = 0.49060331\n",
      "Iteration 293, loss = 0.49030249\n",
      "Iteration 294, loss = 0.49000699\n",
      "Iteration 295, loss = 0.48972089\n",
      "Iteration 296, loss = 0.48944070\n",
      "Iteration 297, loss = 0.48915997\n",
      "Iteration 298, loss = 0.48887634\n",
      "Iteration 299, loss = 0.48860162\n",
      "Iteration 300, loss = 0.48833282\n",
      "Iteration 301, loss = 0.48806026\n",
      "Iteration 302, loss = 0.48779546\n",
      "Iteration 303, loss = 0.48752989\n",
      "Iteration 304, loss = 0.48727365\n",
      "Iteration 305, loss = 0.48701237\n",
      "Iteration 306, loss = 0.48676240\n",
      "Iteration 307, loss = 0.48650598\n",
      "Iteration 308, loss = 0.48625822\n",
      "Iteration 309, loss = 0.48601693\n",
      "Iteration 310, loss = 0.48576631\n",
      "Iteration 311, loss = 0.48552865\n",
      "Iteration 312, loss = 0.48528375\n",
      "Iteration 313, loss = 0.48505834\n",
      "Iteration 314, loss = 0.48481616\n",
      "Iteration 315, loss = 0.48458785\n",
      "Iteration 316, loss = 0.48436289\n",
      "Iteration 317, loss = 0.48414598\n",
      "Iteration 318, loss = 0.48391809\n",
      "Iteration 319, loss = 0.48370168\n",
      "Iteration 320, loss = 0.48348462\n",
      "Iteration 321, loss = 0.48326805\n",
      "Iteration 322, loss = 0.48305867\n",
      "Iteration 323, loss = 0.48285028\n",
      "Iteration 324, loss = 0.48264321\n",
      "Iteration 325, loss = 0.48243862\n",
      "Iteration 326, loss = 0.48223824\n",
      "Iteration 327, loss = 0.48204266\n",
      "Iteration 328, loss = 0.48184183\n",
      "Iteration 329, loss = 0.48164507\n",
      "Iteration 330, loss = 0.48145498\n",
      "Iteration 331, loss = 0.48126612\n",
      "Iteration 332, loss = 0.48107833\n",
      "Iteration 333, loss = 0.48089754\n",
      "Iteration 334, loss = 0.48071487\n",
      "Iteration 335, loss = 0.48053562\n",
      "Iteration 336, loss = 0.48034567\n",
      "Iteration 337, loss = 0.48017663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 338, loss = 0.48000139\n",
      "Iteration 339, loss = 0.47983374\n",
      "Iteration 340, loss = 0.47966387\n",
      "Iteration 341, loss = 0.47949593\n",
      "Iteration 342, loss = 0.47933586\n",
      "Iteration 343, loss = 0.47917032\n",
      "Iteration 344, loss = 0.47901430\n",
      "Iteration 345, loss = 0.47886054\n",
      "Iteration 346, loss = 0.47870169\n",
      "Iteration 347, loss = 0.47854954\n",
      "Iteration 348, loss = 0.47839831\n",
      "Iteration 349, loss = 0.47824791\n",
      "Iteration 350, loss = 0.47810298\n",
      "Iteration 351, loss = 0.47796321\n",
      "Iteration 352, loss = 0.47782100\n",
      "Iteration 353, loss = 0.47768101\n",
      "Iteration 354, loss = 0.47753780\n",
      "Iteration 355, loss = 0.47741143\n",
      "Iteration 356, loss = 0.47726930\n",
      "Iteration 357, loss = 0.47714138\n",
      "Iteration 358, loss = 0.47700817\n",
      "Iteration 359, loss = 0.47687701\n",
      "Iteration 360, loss = 0.47675260\n",
      "Iteration 361, loss = 0.47662880\n",
      "Iteration 362, loss = 0.47649943\n",
      "Iteration 363, loss = 0.47638134\n",
      "Iteration 364, loss = 0.47625991\n",
      "Iteration 365, loss = 0.47614182\n",
      "Iteration 366, loss = 0.47602830\n",
      "Iteration 367, loss = 0.47591535\n",
      "Iteration 368, loss = 0.47579814\n",
      "Iteration 369, loss = 0.47569455\n",
      "Iteration 370, loss = 0.47558322\n",
      "Iteration 371, loss = 0.47548597\n",
      "Iteration 372, loss = 0.47536465\n",
      "Iteration 373, loss = 0.47525890\n",
      "Iteration 374, loss = 0.47515852\n",
      "Iteration 375, loss = 0.47505717\n",
      "Iteration 376, loss = 0.47495966\n",
      "Iteration 377, loss = 0.47485540\n",
      "Iteration 378, loss = 0.47475632\n",
      "Iteration 379, loss = 0.47466360\n",
      "Iteration 380, loss = 0.47457585\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77096488\n",
      "Iteration 2, loss = 0.76504383\n",
      "Iteration 3, loss = 0.75931917\n",
      "Iteration 4, loss = 0.75378888\n",
      "Iteration 5, loss = 0.74845549\n",
      "Iteration 6, loss = 0.74331582\n",
      "Iteration 7, loss = 0.73835553\n",
      "Iteration 8, loss = 0.73359732\n",
      "Iteration 9, loss = 0.72900516\n",
      "Iteration 10, loss = 0.72458543\n",
      "Iteration 11, loss = 0.72035041\n",
      "Iteration 12, loss = 0.71630102\n",
      "Iteration 13, loss = 0.71242220\n",
      "Iteration 14, loss = 0.70870622\n",
      "Iteration 15, loss = 0.70514992\n",
      "Iteration 16, loss = 0.70176733\n",
      "Iteration 17, loss = 0.69851457\n",
      "Iteration 18, loss = 0.69543911\n",
      "Iteration 19, loss = 0.69250821\n",
      "Iteration 20, loss = 0.68973901\n",
      "Iteration 21, loss = 0.68710506\n",
      "Iteration 22, loss = 0.68461471\n",
      "Iteration 23, loss = 0.68225526\n",
      "Iteration 24, loss = 0.68002732\n",
      "Iteration 25, loss = 0.67791739\n",
      "Iteration 26, loss = 0.67591512\n",
      "Iteration 27, loss = 0.67403159\n",
      "Iteration 28, loss = 0.67226635\n",
      "Iteration 29, loss = 0.67058935\n",
      "Iteration 30, loss = 0.66901814\n",
      "Iteration 31, loss = 0.66754485\n",
      "Iteration 32, loss = 0.66614634\n",
      "Iteration 33, loss = 0.66482822\n",
      "Iteration 34, loss = 0.66356920\n",
      "Iteration 35, loss = 0.66239011\n",
      "Iteration 36, loss = 0.66127202\n",
      "Iteration 37, loss = 0.66020339\n",
      "Iteration 38, loss = 0.65919874\n",
      "Iteration 39, loss = 0.65823247\n",
      "Iteration 40, loss = 0.65731269\n",
      "Iteration 41, loss = 0.65642265\n",
      "Iteration 42, loss = 0.65556627\n",
      "Iteration 43, loss = 0.65473761\n",
      "Iteration 44, loss = 0.65393158\n",
      "Iteration 45, loss = 0.65315093\n",
      "Iteration 46, loss = 0.65237690\n",
      "Iteration 47, loss = 0.65162370\n",
      "Iteration 48, loss = 0.65088428\n",
      "Iteration 49, loss = 0.65014775\n",
      "Iteration 50, loss = 0.64942204\n",
      "Iteration 51, loss = 0.64869973\n",
      "Iteration 52, loss = 0.64798187\n",
      "Iteration 53, loss = 0.64726408\n",
      "Iteration 54, loss = 0.64655169\n",
      "Iteration 55, loss = 0.64584114\n",
      "Iteration 56, loss = 0.64512401\n",
      "Iteration 57, loss = 0.64440779\n",
      "Iteration 58, loss = 0.64368969\n",
      "Iteration 59, loss = 0.64297065\n",
      "Iteration 60, loss = 0.64224552\n",
      "Iteration 61, loss = 0.64152236\n",
      "Iteration 62, loss = 0.64078866\n",
      "Iteration 63, loss = 0.64005514\n",
      "Iteration 64, loss = 0.63931838\n",
      "Iteration 65, loss = 0.63858121\n",
      "Iteration 66, loss = 0.63783566\n",
      "Iteration 67, loss = 0.63708838\n",
      "Iteration 68, loss = 0.63633408\n",
      "Iteration 69, loss = 0.63557991\n",
      "Iteration 70, loss = 0.63481847\n",
      "Iteration 71, loss = 0.63405394\n",
      "Iteration 72, loss = 0.63328401\n",
      "Iteration 73, loss = 0.63251169\n",
      "Iteration 74, loss = 0.63173772\n",
      "Iteration 75, loss = 0.63095546\n",
      "Iteration 76, loss = 0.63017166\n",
      "Iteration 77, loss = 0.62938614\n",
      "Iteration 78, loss = 0.62859465\n",
      "Iteration 79, loss = 0.62779928\n",
      "Iteration 80, loss = 0.62700612\n",
      "Iteration 81, loss = 0.62620847\n",
      "Iteration 82, loss = 0.62541117\n",
      "Iteration 83, loss = 0.62461021\n",
      "Iteration 84, loss = 0.62381621\n",
      "Iteration 85, loss = 0.62300925\n",
      "Iteration 86, loss = 0.62221019\n",
      "Iteration 87, loss = 0.62140993\n",
      "Iteration 88, loss = 0.62060324\n",
      "Iteration 89, loss = 0.61979595\n",
      "Iteration 90, loss = 0.61899071\n",
      "Iteration 91, loss = 0.61818445\n",
      "Iteration 92, loss = 0.61737394\n",
      "Iteration 93, loss = 0.61656435\n",
      "Iteration 94, loss = 0.61575208\n",
      "Iteration 95, loss = 0.61493735\n",
      "Iteration 96, loss = 0.61413670\n",
      "Iteration 97, loss = 0.61331803\n",
      "Iteration 98, loss = 0.61251423\n",
      "Iteration 99, loss = 0.61169685\n",
      "Iteration 100, loss = 0.61089114\n",
      "Iteration 101, loss = 0.61007983\n",
      "Iteration 102, loss = 0.60927029\n",
      "Iteration 103, loss = 0.60845690\n",
      "Iteration 104, loss = 0.60764477\n",
      "Iteration 105, loss = 0.60683115\n",
      "Iteration 106, loss = 0.60601867\n",
      "Iteration 107, loss = 0.60520763\n",
      "Iteration 108, loss = 0.60439543\n",
      "Iteration 109, loss = 0.60358215\n",
      "Iteration 110, loss = 0.60276924\n",
      "Iteration 111, loss = 0.60195173\n",
      "Iteration 112, loss = 0.60113708\n",
      "Iteration 113, loss = 0.60032995\n",
      "Iteration 114, loss = 0.59951064\n",
      "Iteration 115, loss = 0.59870192\n",
      "Iteration 116, loss = 0.59788910\n",
      "Iteration 117, loss = 0.59707784\n",
      "Iteration 118, loss = 0.59627543\n",
      "Iteration 119, loss = 0.59545648\n",
      "Iteration 120, loss = 0.59464596\n",
      "Iteration 121, loss = 0.59383914\n",
      "Iteration 122, loss = 0.59303292\n",
      "Iteration 123, loss = 0.59223086\n",
      "Iteration 124, loss = 0.59142292\n",
      "Iteration 125, loss = 0.59061965\n",
      "Iteration 126, loss = 0.58981791\n",
      "Iteration 127, loss = 0.58901421\n",
      "Iteration 128, loss = 0.58821719\n",
      "Iteration 129, loss = 0.58741133\n",
      "Iteration 130, loss = 0.58661697\n",
      "Iteration 131, loss = 0.58581739\n",
      "Iteration 132, loss = 0.58501856\n",
      "Iteration 133, loss = 0.58421996\n",
      "Iteration 134, loss = 0.58342179\n",
      "Iteration 135, loss = 0.58262866\n",
      "Iteration 136, loss = 0.58183117\n",
      "Iteration 137, loss = 0.58103199\n",
      "Iteration 138, loss = 0.58023753\n",
      "Iteration 139, loss = 0.57944363\n",
      "Iteration 140, loss = 0.57865492\n",
      "Iteration 141, loss = 0.57786454\n",
      "Iteration 142, loss = 0.57707525\n",
      "Iteration 143, loss = 0.57629252\n",
      "Iteration 144, loss = 0.57550725\n",
      "Iteration 145, loss = 0.57472866\n",
      "Iteration 146, loss = 0.57394284\n",
      "Iteration 147, loss = 0.57316673\n",
      "Iteration 148, loss = 0.57238942\n",
      "Iteration 149, loss = 0.57161381\n",
      "Iteration 150, loss = 0.57084278\n",
      "Iteration 151, loss = 0.57007600\n",
      "Iteration 152, loss = 0.56930867\n",
      "Iteration 153, loss = 0.56854754\n",
      "Iteration 154, loss = 0.56778646\n",
      "Iteration 155, loss = 0.56702797\n",
      "Iteration 156, loss = 0.56627277\n",
      "Iteration 157, loss = 0.56551599\n",
      "Iteration 158, loss = 0.56475975\n",
      "Iteration 159, loss = 0.56400505\n",
      "Iteration 160, loss = 0.56325173\n",
      "Iteration 161, loss = 0.56249814\n",
      "Iteration 162, loss = 0.56175666\n",
      "Iteration 163, loss = 0.56100236\n",
      "Iteration 164, loss = 0.56025663\n",
      "Iteration 165, loss = 0.55951497\n",
      "Iteration 166, loss = 0.55877415\n",
      "Iteration 167, loss = 0.55803977\n",
      "Iteration 168, loss = 0.55729834\n",
      "Iteration 169, loss = 0.55656502\n",
      "Iteration 170, loss = 0.55583366\n",
      "Iteration 171, loss = 0.55510628\n",
      "Iteration 172, loss = 0.55438897\n",
      "Iteration 173, loss = 0.55365989\n",
      "Iteration 174, loss = 0.55294321\n",
      "Iteration 175, loss = 0.55222840\n",
      "Iteration 176, loss = 0.55151173\n",
      "Iteration 177, loss = 0.55080343\n",
      "Iteration 178, loss = 0.55010006\n",
      "Iteration 179, loss = 0.54939506\n",
      "Iteration 180, loss = 0.54870141\n",
      "Iteration 181, loss = 0.54800062\n",
      "Iteration 182, loss = 0.54730818\n",
      "Iteration 183, loss = 0.54662473\n",
      "Iteration 184, loss = 0.54593585\n",
      "Iteration 185, loss = 0.54525576\n",
      "Iteration 186, loss = 0.54458743\n",
      "Iteration 187, loss = 0.54390950\n",
      "Iteration 188, loss = 0.54324209\n",
      "Iteration 189, loss = 0.54257334\n",
      "Iteration 190, loss = 0.54190707\n",
      "Iteration 191, loss = 0.54124531\n",
      "Iteration 192, loss = 0.54058299\n",
      "Iteration 193, loss = 0.53993217\n",
      "Iteration 194, loss = 0.53928140\n",
      "Iteration 195, loss = 0.53863041\n",
      "Iteration 196, loss = 0.53798996\n",
      "Iteration 197, loss = 0.53735058\n",
      "Iteration 198, loss = 0.53671414\n",
      "Iteration 199, loss = 0.53608290\n",
      "Iteration 200, loss = 0.53544779\n",
      "Iteration 201, loss = 0.53482103\n",
      "Iteration 202, loss = 0.53419547\n",
      "Iteration 203, loss = 0.53357670\n",
      "Iteration 204, loss = 0.53295772\n",
      "Iteration 205, loss = 0.53234222\n",
      "Iteration 206, loss = 0.53172959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 207, loss = 0.53112663\n",
      "Iteration 208, loss = 0.53052717\n",
      "Iteration 209, loss = 0.52992241\n",
      "Iteration 210, loss = 0.52933378\n",
      "Iteration 211, loss = 0.52874017\n",
      "Iteration 212, loss = 0.52815464\n",
      "Iteration 213, loss = 0.52756979\n",
      "Iteration 214, loss = 0.52698904\n",
      "Iteration 215, loss = 0.52640595\n",
      "Iteration 216, loss = 0.52583798\n",
      "Iteration 217, loss = 0.52526545\n",
      "Iteration 218, loss = 0.52469940\n",
      "Iteration 219, loss = 0.52413552\n",
      "Iteration 220, loss = 0.52357382\n",
      "Iteration 221, loss = 0.52302045\n",
      "Iteration 222, loss = 0.52246745\n",
      "Iteration 223, loss = 0.52190795\n",
      "Iteration 224, loss = 0.52135822\n",
      "Iteration 225, loss = 0.52082113\n",
      "Iteration 226, loss = 0.52027900\n",
      "Iteration 227, loss = 0.51974997\n",
      "Iteration 228, loss = 0.51921919\n",
      "Iteration 229, loss = 0.51869397\n",
      "Iteration 230, loss = 0.51817342\n",
      "Iteration 231, loss = 0.51765651\n",
      "Iteration 232, loss = 0.51713867\n",
      "Iteration 233, loss = 0.51663263\n",
      "Iteration 234, loss = 0.51612870\n",
      "Iteration 235, loss = 0.51562222\n",
      "Iteration 236, loss = 0.51512776\n",
      "Iteration 237, loss = 0.51464206\n",
      "Iteration 238, loss = 0.51414683\n",
      "Iteration 239, loss = 0.51365727\n",
      "Iteration 240, loss = 0.51318705\n",
      "Iteration 241, loss = 0.51270274\n",
      "Iteration 242, loss = 0.51223369\n",
      "Iteration 243, loss = 0.51176624\n",
      "Iteration 244, loss = 0.51130090\n",
      "Iteration 245, loss = 0.51083798\n",
      "Iteration 246, loss = 0.51037740\n",
      "Iteration 247, loss = 0.50992252\n",
      "Iteration 248, loss = 0.50946860\n",
      "Iteration 249, loss = 0.50902809\n",
      "Iteration 250, loss = 0.50857730\n",
      "Iteration 251, loss = 0.50813199\n",
      "Iteration 252, loss = 0.50769315\n",
      "Iteration 253, loss = 0.50726253\n",
      "Iteration 254, loss = 0.50682761\n",
      "Iteration 255, loss = 0.50640750\n",
      "Iteration 256, loss = 0.50598327\n",
      "Iteration 257, loss = 0.50555562\n",
      "Iteration 258, loss = 0.50514097\n",
      "Iteration 259, loss = 0.50472757\n",
      "Iteration 260, loss = 0.50431563\n",
      "Iteration 261, loss = 0.50390437\n",
      "Iteration 262, loss = 0.50350195\n",
      "Iteration 263, loss = 0.50310470\n",
      "Iteration 264, loss = 0.50270988\n",
      "Iteration 265, loss = 0.50231302\n",
      "Iteration 266, loss = 0.50192416\n",
      "Iteration 267, loss = 0.50154360\n",
      "Iteration 268, loss = 0.50116032\n",
      "Iteration 269, loss = 0.50077796\n",
      "Iteration 270, loss = 0.50040174\n",
      "Iteration 271, loss = 0.50002935\n",
      "Iteration 272, loss = 0.49966828\n",
      "Iteration 273, loss = 0.49930039\n",
      "Iteration 274, loss = 0.49893320\n",
      "Iteration 275, loss = 0.49857545\n",
      "Iteration 276, loss = 0.49822225\n",
      "Iteration 277, loss = 0.49786824\n",
      "Iteration 278, loss = 0.49752008\n",
      "Iteration 279, loss = 0.49717610\n",
      "Iteration 280, loss = 0.49682839\n",
      "Iteration 281, loss = 0.49649097\n",
      "Iteration 282, loss = 0.49616064\n",
      "Iteration 283, loss = 0.49581740\n",
      "Iteration 284, loss = 0.49549091\n",
      "Iteration 285, loss = 0.49516326\n",
      "Iteration 286, loss = 0.49483869\n",
      "Iteration 287, loss = 0.49452005\n",
      "Iteration 288, loss = 0.49420376\n",
      "Iteration 289, loss = 0.49388902\n",
      "Iteration 290, loss = 0.49359257\n",
      "Iteration 291, loss = 0.49326843\n",
      "Iteration 292, loss = 0.49296961\n",
      "Iteration 293, loss = 0.49267170\n",
      "Iteration 294, loss = 0.49236791\n",
      "Iteration 295, loss = 0.49207212\n",
      "Iteration 296, loss = 0.49178207\n",
      "Iteration 297, loss = 0.49149919\n",
      "Iteration 298, loss = 0.49120743\n",
      "Iteration 299, loss = 0.49092577\n",
      "Iteration 300, loss = 0.49064801\n",
      "Iteration 301, loss = 0.49036986\n",
      "Iteration 302, loss = 0.49009578\n",
      "Iteration 303, loss = 0.48982480\n",
      "Iteration 304, loss = 0.48955948\n",
      "Iteration 305, loss = 0.48929176\n",
      "Iteration 306, loss = 0.48903387\n",
      "Iteration 307, loss = 0.48877504\n",
      "Iteration 308, loss = 0.48851639\n",
      "Iteration 309, loss = 0.48826490\n",
      "Iteration 310, loss = 0.48800937\n",
      "Iteration 311, loss = 0.48776513\n",
      "Iteration 312, loss = 0.48751278\n",
      "Iteration 313, loss = 0.48728029\n",
      "Iteration 314, loss = 0.48703021\n",
      "Iteration 315, loss = 0.48679436\n",
      "Iteration 316, loss = 0.48656109\n",
      "Iteration 317, loss = 0.48633355\n",
      "Iteration 318, loss = 0.48610272\n",
      "Iteration 319, loss = 0.48587382\n",
      "Iteration 320, loss = 0.48565398\n",
      "Iteration 321, loss = 0.48542758\n",
      "Iteration 322, loss = 0.48520837\n",
      "Iteration 323, loss = 0.48499211\n",
      "Iteration 324, loss = 0.48477563\n",
      "Iteration 325, loss = 0.48456289\n",
      "Iteration 326, loss = 0.48435360\n",
      "Iteration 327, loss = 0.48414799\n",
      "Iteration 328, loss = 0.48394371\n",
      "Iteration 329, loss = 0.48373729\n",
      "Iteration 330, loss = 0.48353646\n",
      "Iteration 331, loss = 0.48333679\n",
      "Iteration 332, loss = 0.48313827\n",
      "Iteration 333, loss = 0.48294967\n",
      "Iteration 334, loss = 0.48276407\n",
      "Iteration 335, loss = 0.48256986\n",
      "Iteration 336, loss = 0.48237220\n",
      "Iteration 337, loss = 0.48219246\n",
      "Iteration 338, loss = 0.48201106\n",
      "Iteration 339, loss = 0.48183542\n",
      "Iteration 340, loss = 0.48165778\n",
      "Iteration 341, loss = 0.48148122\n",
      "Iteration 342, loss = 0.48131572\n",
      "Iteration 343, loss = 0.48114175\n",
      "Iteration 344, loss = 0.48097452\n",
      "Iteration 345, loss = 0.48081299\n",
      "Iteration 346, loss = 0.48064648\n",
      "Iteration 347, loss = 0.48048415\n",
      "Iteration 348, loss = 0.48032339\n",
      "Iteration 349, loss = 0.48016504\n",
      "Iteration 350, loss = 0.48001805\n",
      "Iteration 351, loss = 0.47985968\n",
      "Iteration 352, loss = 0.47970985\n",
      "Iteration 353, loss = 0.47956366\n",
      "Iteration 354, loss = 0.47940985\n",
      "Iteration 355, loss = 0.47927364\n",
      "Iteration 356, loss = 0.47912441\n",
      "Iteration 357, loss = 0.47899047\n",
      "Iteration 358, loss = 0.47885576\n",
      "Iteration 359, loss = 0.47870614\n",
      "Iteration 360, loss = 0.47857284\n",
      "Iteration 361, loss = 0.47844032\n",
      "Iteration 362, loss = 0.47830059\n",
      "Iteration 363, loss = 0.47817618\n",
      "Iteration 364, loss = 0.47804716\n",
      "Iteration 365, loss = 0.47791609\n",
      "Iteration 366, loss = 0.47779267\n",
      "Iteration 367, loss = 0.47767056\n",
      "Iteration 368, loss = 0.47754759\n",
      "Iteration 369, loss = 0.47743196\n",
      "Iteration 370, loss = 0.47731390\n",
      "Iteration 371, loss = 0.47720291\n",
      "Iteration 372, loss = 0.47708043\n",
      "Iteration 373, loss = 0.47696443\n",
      "Iteration 374, loss = 0.47685130\n",
      "Iteration 375, loss = 0.47673971\n",
      "Iteration 376, loss = 0.47663616\n",
      "Iteration 377, loss = 0.47652222\n",
      "Iteration 378, loss = 0.47641333\n",
      "Iteration 379, loss = 0.47631149\n",
      "Iteration 380, loss = 0.47620728\n",
      "Iteration 381, loss = 0.47610818\n",
      "Iteration 382, loss = 0.47600931\n",
      "Iteration 383, loss = 0.47591068\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77709539\n",
      "Iteration 2, loss = 0.77088850\n",
      "Iteration 3, loss = 0.76487548\n",
      "Iteration 4, loss = 0.75904946\n",
      "Iteration 5, loss = 0.75341687\n",
      "Iteration 6, loss = 0.74798260\n",
      "Iteration 7, loss = 0.74272471\n",
      "Iteration 8, loss = 0.73767516\n",
      "Iteration 9, loss = 0.73278406\n",
      "Iteration 10, loss = 0.72807435\n",
      "Iteration 11, loss = 0.72354682\n",
      "Iteration 12, loss = 0.71920826\n",
      "Iteration 13, loss = 0.71504703\n",
      "Iteration 14, loss = 0.71105372\n",
      "Iteration 15, loss = 0.70722557\n",
      "Iteration 16, loss = 0.70356955\n",
      "Iteration 17, loss = 0.70005246\n",
      "Iteration 18, loss = 0.69670897\n",
      "Iteration 19, loss = 0.69352287\n",
      "Iteration 20, loss = 0.69049818\n",
      "Iteration 21, loss = 0.68762209\n",
      "Iteration 22, loss = 0.68488990\n",
      "Iteration 23, loss = 0.68229966\n",
      "Iteration 24, loss = 0.67984473\n",
      "Iteration 25, loss = 0.67751818\n",
      "Iteration 26, loss = 0.67530704\n",
      "Iteration 27, loss = 0.67322319\n",
      "Iteration 28, loss = 0.67127316\n",
      "Iteration 29, loss = 0.66941689\n",
      "Iteration 30, loss = 0.66767896\n",
      "Iteration 31, loss = 0.66604510\n",
      "Iteration 32, loss = 0.66450377\n",
      "Iteration 33, loss = 0.66305222\n",
      "Iteration 34, loss = 0.66166632\n",
      "Iteration 35, loss = 0.66037551\n",
      "Iteration 36, loss = 0.65915430\n",
      "Iteration 37, loss = 0.65799459\n",
      "Iteration 38, loss = 0.65690769\n",
      "Iteration 39, loss = 0.65587221\n",
      "Iteration 40, loss = 0.65489624\n",
      "Iteration 41, loss = 0.65395719\n",
      "Iteration 42, loss = 0.65306472\n",
      "Iteration 43, loss = 0.65220535\n",
      "Iteration 44, loss = 0.65137763\n",
      "Iteration 45, loss = 0.65058176\n",
      "Iteration 46, loss = 0.64980288\n",
      "Iteration 47, loss = 0.64904824\n",
      "Iteration 48, loss = 0.64831539\n",
      "Iteration 49, loss = 0.64759104\n",
      "Iteration 50, loss = 0.64688338\n",
      "Iteration 51, loss = 0.64618262\n",
      "Iteration 52, loss = 0.64548912\n",
      "Iteration 53, loss = 0.64479973\n",
      "Iteration 54, loss = 0.64411935\n",
      "Iteration 55, loss = 0.64344168\n",
      "Iteration 56, loss = 0.64276228\n",
      "Iteration 57, loss = 0.64208341\n",
      "Iteration 58, loss = 0.64140476\n",
      "Iteration 59, loss = 0.64072535\n",
      "Iteration 60, loss = 0.64004306\n",
      "Iteration 61, loss = 0.63936305\n",
      "Iteration 62, loss = 0.63867271\n",
      "Iteration 63, loss = 0.63798349\n",
      "Iteration 64, loss = 0.63729079\n",
      "Iteration 65, loss = 0.63659726\n",
      "Iteration 66, loss = 0.63589552\n",
      "Iteration 67, loss = 0.63519150\n",
      "Iteration 68, loss = 0.63448105\n",
      "Iteration 69, loss = 0.63376944\n",
      "Iteration 70, loss = 0.63305295\n",
      "Iteration 71, loss = 0.63233411\n",
      "Iteration 72, loss = 0.63160890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73, loss = 0.63088118\n",
      "Iteration 74, loss = 0.63015291\n",
      "Iteration 75, loss = 0.62941423\n",
      "Iteration 76, loss = 0.62867581\n",
      "Iteration 77, loss = 0.62793479\n",
      "Iteration 78, loss = 0.62718865\n",
      "Iteration 79, loss = 0.62643786\n",
      "Iteration 80, loss = 0.62568910\n",
      "Iteration 81, loss = 0.62493634\n",
      "Iteration 82, loss = 0.62418502\n",
      "Iteration 83, loss = 0.62342681\n",
      "Iteration 84, loss = 0.62267870\n",
      "Iteration 85, loss = 0.62191227\n",
      "Iteration 86, loss = 0.62115472\n",
      "Iteration 87, loss = 0.62039344\n",
      "Iteration 88, loss = 0.61962994\n",
      "Iteration 89, loss = 0.61886117\n",
      "Iteration 90, loss = 0.61809360\n",
      "Iteration 91, loss = 0.61732520\n",
      "Iteration 92, loss = 0.61655491\n",
      "Iteration 93, loss = 0.61577760\n",
      "Iteration 94, loss = 0.61500743\n",
      "Iteration 95, loss = 0.61423064\n",
      "Iteration 96, loss = 0.61346535\n",
      "Iteration 97, loss = 0.61268616\n",
      "Iteration 98, loss = 0.61191807\n",
      "Iteration 99, loss = 0.61114282\n",
      "Iteration 100, loss = 0.61037585\n",
      "Iteration 101, loss = 0.60960459\n",
      "Iteration 102, loss = 0.60883352\n",
      "Iteration 103, loss = 0.60806175\n",
      "Iteration 104, loss = 0.60729109\n",
      "Iteration 105, loss = 0.60651947\n",
      "Iteration 106, loss = 0.60574476\n",
      "Iteration 107, loss = 0.60497472\n",
      "Iteration 108, loss = 0.60420300\n",
      "Iteration 109, loss = 0.60342809\n",
      "Iteration 110, loss = 0.60265444\n",
      "Iteration 111, loss = 0.60187774\n",
      "Iteration 112, loss = 0.60110395\n",
      "Iteration 113, loss = 0.60033232\n",
      "Iteration 114, loss = 0.59955290\n",
      "Iteration 115, loss = 0.59878309\n",
      "Iteration 116, loss = 0.59800767\n",
      "Iteration 117, loss = 0.59723662\n",
      "Iteration 118, loss = 0.59647218\n",
      "Iteration 119, loss = 0.59569354\n",
      "Iteration 120, loss = 0.59492057\n",
      "Iteration 121, loss = 0.59415348\n",
      "Iteration 122, loss = 0.59338567\n",
      "Iteration 123, loss = 0.59262087\n",
      "Iteration 124, loss = 0.59185280\n",
      "Iteration 125, loss = 0.59108786\n",
      "Iteration 126, loss = 0.59032521\n",
      "Iteration 127, loss = 0.58955799\n",
      "Iteration 128, loss = 0.58879827\n",
      "Iteration 129, loss = 0.58803259\n",
      "Iteration 130, loss = 0.58727651\n",
      "Iteration 131, loss = 0.58651619\n",
      "Iteration 132, loss = 0.58575778\n",
      "Iteration 133, loss = 0.58500010\n",
      "Iteration 134, loss = 0.58424189\n",
      "Iteration 135, loss = 0.58348815\n",
      "Iteration 136, loss = 0.58273335\n",
      "Iteration 137, loss = 0.58197531\n",
      "Iteration 138, loss = 0.58122366\n",
      "Iteration 139, loss = 0.58047007\n",
      "Iteration 140, loss = 0.57972177\n",
      "Iteration 141, loss = 0.57897411\n",
      "Iteration 142, loss = 0.57822707\n",
      "Iteration 143, loss = 0.57748764\n",
      "Iteration 144, loss = 0.57674625\n",
      "Iteration 145, loss = 0.57600784\n",
      "Iteration 146, loss = 0.57526778\n",
      "Iteration 147, loss = 0.57453723\n",
      "Iteration 148, loss = 0.57380065\n",
      "Iteration 149, loss = 0.57306846\n",
      "Iteration 150, loss = 0.57234021\n",
      "Iteration 151, loss = 0.57161447\n",
      "Iteration 152, loss = 0.57088995\n",
      "Iteration 153, loss = 0.57017105\n",
      "Iteration 154, loss = 0.56945019\n",
      "Iteration 155, loss = 0.56873309\n",
      "Iteration 156, loss = 0.56801945\n",
      "Iteration 157, loss = 0.56730456\n",
      "Iteration 158, loss = 0.56658701\n",
      "Iteration 159, loss = 0.56587467\n",
      "Iteration 160, loss = 0.56516103\n",
      "Iteration 161, loss = 0.56444784\n",
      "Iteration 162, loss = 0.56374372\n",
      "Iteration 163, loss = 0.56302817\n",
      "Iteration 164, loss = 0.56232178\n",
      "Iteration 165, loss = 0.56161724\n",
      "Iteration 166, loss = 0.56091738\n",
      "Iteration 167, loss = 0.56021819\n",
      "Iteration 168, loss = 0.55951478\n",
      "Iteration 169, loss = 0.55881922\n",
      "Iteration 170, loss = 0.55812749\n",
      "Iteration 171, loss = 0.55743642\n",
      "Iteration 172, loss = 0.55675921\n",
      "Iteration 173, loss = 0.55606412\n",
      "Iteration 174, loss = 0.55538280\n",
      "Iteration 175, loss = 0.55470650\n",
      "Iteration 176, loss = 0.55402618\n",
      "Iteration 177, loss = 0.55335433\n",
      "Iteration 178, loss = 0.55268571\n",
      "Iteration 179, loss = 0.55201510\n",
      "Iteration 180, loss = 0.55135479\n",
      "Iteration 181, loss = 0.55069069\n",
      "Iteration 182, loss = 0.55003207\n",
      "Iteration 183, loss = 0.54938146\n",
      "Iteration 184, loss = 0.54872850\n",
      "Iteration 185, loss = 0.54808164\n",
      "Iteration 186, loss = 0.54744660\n",
      "Iteration 187, loss = 0.54679927\n",
      "Iteration 188, loss = 0.54616529\n",
      "Iteration 189, loss = 0.54553040\n",
      "Iteration 190, loss = 0.54489632\n",
      "Iteration 191, loss = 0.54426992\n",
      "Iteration 192, loss = 0.54364289\n",
      "Iteration 193, loss = 0.54301548\n",
      "Iteration 194, loss = 0.54239693\n",
      "Iteration 195, loss = 0.54177500\n",
      "Iteration 196, loss = 0.54116611\n",
      "Iteration 197, loss = 0.54055222\n",
      "Iteration 198, loss = 0.53994335\n",
      "Iteration 199, loss = 0.53933810\n",
      "Iteration 200, loss = 0.53873315\n",
      "Iteration 201, loss = 0.53813452\n",
      "Iteration 202, loss = 0.53753434\n",
      "Iteration 203, loss = 0.53694663\n",
      "Iteration 204, loss = 0.53635376\n",
      "Iteration 205, loss = 0.53576696\n",
      "Iteration 206, loss = 0.53518138\n",
      "Iteration 207, loss = 0.53460712\n",
      "Iteration 208, loss = 0.53403563\n",
      "Iteration 209, loss = 0.53345899\n",
      "Iteration 210, loss = 0.53290382\n",
      "Iteration 211, loss = 0.53233268\n",
      "Iteration 212, loss = 0.53177520\n",
      "Iteration 213, loss = 0.53122110\n",
      "Iteration 214, loss = 0.53066552\n",
      "Iteration 215, loss = 0.53010965\n",
      "Iteration 216, loss = 0.52956784\n",
      "Iteration 217, loss = 0.52902053\n",
      "Iteration 218, loss = 0.52847871\n",
      "Iteration 219, loss = 0.52793994\n",
      "Iteration 220, loss = 0.52740219\n",
      "Iteration 221, loss = 0.52687303\n",
      "Iteration 222, loss = 0.52634570\n",
      "Iteration 223, loss = 0.52581228\n",
      "Iteration 224, loss = 0.52528584\n",
      "Iteration 225, loss = 0.52477363\n",
      "Iteration 226, loss = 0.52425395\n",
      "Iteration 227, loss = 0.52374803\n",
      "Iteration 228, loss = 0.52324136\n",
      "Iteration 229, loss = 0.52273880\n",
      "Iteration 230, loss = 0.52224095\n",
      "Iteration 231, loss = 0.52174528\n",
      "Iteration 232, loss = 0.52124729\n",
      "Iteration 233, loss = 0.52076252\n",
      "Iteration 234, loss = 0.52027538\n",
      "Iteration 235, loss = 0.51978878\n",
      "Iteration 236, loss = 0.51930985\n",
      "Iteration 237, loss = 0.51884835\n",
      "Iteration 238, loss = 0.51836676\n",
      "Iteration 239, loss = 0.51789529\n",
      "Iteration 240, loss = 0.51743966\n",
      "Iteration 241, loss = 0.51697208\n",
      "Iteration 242, loss = 0.51651927\n",
      "Iteration 243, loss = 0.51606610\n",
      "Iteration 244, loss = 0.51561198\n",
      "Iteration 245, loss = 0.51516421\n",
      "Iteration 246, loss = 0.51471746\n",
      "Iteration 247, loss = 0.51427738\n",
      "Iteration 248, loss = 0.51383643\n",
      "Iteration 249, loss = 0.51340763\n",
      "Iteration 250, loss = 0.51297178\n",
      "Iteration 251, loss = 0.51253847\n",
      "Iteration 252, loss = 0.51211231\n",
      "Iteration 253, loss = 0.51169719\n",
      "Iteration 254, loss = 0.51127198\n",
      "Iteration 255, loss = 0.51086258\n",
      "Iteration 256, loss = 0.51045116\n",
      "Iteration 257, loss = 0.51003556\n",
      "Iteration 258, loss = 0.50963500\n",
      "Iteration 259, loss = 0.50923579\n",
      "Iteration 260, loss = 0.50883419\n",
      "Iteration 261, loss = 0.50843357\n",
      "Iteration 262, loss = 0.50804466\n",
      "Iteration 263, loss = 0.50765767\n",
      "Iteration 264, loss = 0.50728065\n",
      "Iteration 265, loss = 0.50688936\n",
      "Iteration 266, loss = 0.50650895\n",
      "Iteration 267, loss = 0.50613925\n",
      "Iteration 268, loss = 0.50576732\n",
      "Iteration 269, loss = 0.50539610\n",
      "Iteration 270, loss = 0.50503166\n",
      "Iteration 271, loss = 0.50467183\n",
      "Iteration 272, loss = 0.50431912\n",
      "Iteration 273, loss = 0.50396624\n",
      "Iteration 274, loss = 0.50360871\n",
      "Iteration 275, loss = 0.50325842\n",
      "Iteration 276, loss = 0.50291677\n",
      "Iteration 277, loss = 0.50257232\n",
      "Iteration 278, loss = 0.50223452\n",
      "Iteration 279, loss = 0.50189630\n",
      "Iteration 280, loss = 0.50156113\n",
      "Iteration 281, loss = 0.50123097\n",
      "Iteration 282, loss = 0.50091118\n",
      "Iteration 283, loss = 0.50057795\n",
      "Iteration 284, loss = 0.50026204\n",
      "Iteration 285, loss = 0.49993981\n",
      "Iteration 286, loss = 0.49962307\n",
      "Iteration 287, loss = 0.49930701\n",
      "Iteration 288, loss = 0.49899979\n",
      "Iteration 289, loss = 0.49869465\n",
      "Iteration 290, loss = 0.49839578\n",
      "Iteration 291, loss = 0.49808597\n",
      "Iteration 292, loss = 0.49779020\n",
      "Iteration 293, loss = 0.49749765\n",
      "Iteration 294, loss = 0.49720218\n",
      "Iteration 295, loss = 0.49691194\n",
      "Iteration 296, loss = 0.49662721\n",
      "Iteration 297, loss = 0.49634950\n",
      "Iteration 298, loss = 0.49606060\n",
      "Iteration 299, loss = 0.49578774\n",
      "Iteration 300, loss = 0.49551328\n",
      "Iteration 301, loss = 0.49523877\n",
      "Iteration 302, loss = 0.49496737\n",
      "Iteration 303, loss = 0.49470275\n",
      "Iteration 304, loss = 0.49443862\n",
      "Iteration 305, loss = 0.49417734\n",
      "Iteration 306, loss = 0.49392415\n",
      "Iteration 307, loss = 0.49366703\n",
      "Iteration 308, loss = 0.49341075\n",
      "Iteration 309, loss = 0.49316546\n",
      "Iteration 310, loss = 0.49291063\n",
      "Iteration 311, loss = 0.49267133\n",
      "Iteration 312, loss = 0.49242206\n",
      "Iteration 313, loss = 0.49219245\n",
      "Iteration 314, loss = 0.49194297\n",
      "Iteration 315, loss = 0.49170916\n",
      "Iteration 316, loss = 0.49147705\n",
      "Iteration 317, loss = 0.49125186\n",
      "Iteration 318, loss = 0.49102545\n",
      "Iteration 319, loss = 0.49079549\n",
      "Iteration 320, loss = 0.49057829\n",
      "Iteration 321, loss = 0.49035157\n",
      "Iteration 322, loss = 0.49013461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 323, loss = 0.48991997\n",
      "Iteration 324, loss = 0.48970371\n",
      "Iteration 325, loss = 0.48949168\n",
      "Iteration 326, loss = 0.48928366\n",
      "Iteration 327, loss = 0.48907939\n",
      "Iteration 328, loss = 0.48887484\n",
      "Iteration 329, loss = 0.48867048\n",
      "Iteration 330, loss = 0.48846978\n",
      "Iteration 331, loss = 0.48827503\n",
      "Iteration 332, loss = 0.48807944\n",
      "Iteration 333, loss = 0.48788992\n",
      "Iteration 334, loss = 0.48770531\n",
      "Iteration 335, loss = 0.48751062\n",
      "Iteration 336, loss = 0.48732012\n",
      "Iteration 337, loss = 0.48713738\n",
      "Iteration 338, loss = 0.48696058\n",
      "Iteration 339, loss = 0.48678477\n",
      "Iteration 340, loss = 0.48660923\n",
      "Iteration 341, loss = 0.48643357\n",
      "Iteration 342, loss = 0.48626610\n",
      "Iteration 343, loss = 0.48609482\n",
      "Iteration 344, loss = 0.48593062\n",
      "Iteration 345, loss = 0.48577021\n",
      "Iteration 346, loss = 0.48560399\n",
      "Iteration 347, loss = 0.48543842\n",
      "Iteration 348, loss = 0.48528000\n",
      "Iteration 349, loss = 0.48512240\n",
      "Iteration 350, loss = 0.48497082\n",
      "Iteration 351, loss = 0.48481921\n",
      "Iteration 352, loss = 0.48466726\n",
      "Iteration 353, loss = 0.48451683\n",
      "Iteration 354, loss = 0.48436512\n",
      "Iteration 355, loss = 0.48422294\n",
      "Iteration 356, loss = 0.48407611\n",
      "Iteration 357, loss = 0.48393906\n",
      "Iteration 358, loss = 0.48380229\n",
      "Iteration 359, loss = 0.48365533\n",
      "Iteration 360, loss = 0.48351791\n",
      "Iteration 361, loss = 0.48338856\n",
      "Iteration 362, loss = 0.48324870\n",
      "Iteration 363, loss = 0.48312117\n",
      "Iteration 364, loss = 0.48299149\n",
      "Iteration 365, loss = 0.48286079\n",
      "Iteration 366, loss = 0.48273437\n",
      "Iteration 367, loss = 0.48261357\n",
      "Iteration 368, loss = 0.48248682\n",
      "Iteration 369, loss = 0.48236817\n",
      "Iteration 370, loss = 0.48224498\n",
      "Iteration 371, loss = 0.48213138\n",
      "Iteration 372, loss = 0.48201088\n",
      "Iteration 373, loss = 0.48189401\n",
      "Iteration 374, loss = 0.48177884\n",
      "Iteration 375, loss = 0.48166499\n",
      "Iteration 376, loss = 0.48155750\n",
      "Iteration 377, loss = 0.48144121\n",
      "Iteration 378, loss = 0.48133465\n",
      "Iteration 379, loss = 0.48123380\n",
      "Iteration 380, loss = 0.48112522\n",
      "Iteration 381, loss = 0.48102253\n",
      "Iteration 382, loss = 0.48091992\n",
      "Iteration 383, loss = 0.48082016\n",
      "Iteration 384, loss = 0.48071510\n",
      "Iteration 385, loss = 0.48062219\n",
      "Iteration 386, loss = 0.48052166\n",
      "Iteration 387, loss = 0.48043313\n",
      "Iteration 388, loss = 0.48034255\n",
      "Iteration 389, loss = 0.48024574\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77875392\n",
      "Iteration 2, loss = 0.77245269\n",
      "Iteration 3, loss = 0.76635034\n",
      "Iteration 4, loss = 0.76043916\n",
      "Iteration 5, loss = 0.75472455\n",
      "Iteration 6, loss = 0.74920598\n",
      "Iteration 7, loss = 0.74386502\n",
      "Iteration 8, loss = 0.73873226\n",
      "Iteration 9, loss = 0.73375939\n",
      "Iteration 10, loss = 0.72896794\n",
      "Iteration 11, loss = 0.72435888\n",
      "Iteration 12, loss = 0.71993837\n",
      "Iteration 13, loss = 0.71569933\n",
      "Iteration 14, loss = 0.71162348\n",
      "Iteration 15, loss = 0.70771934\n",
      "Iteration 16, loss = 0.70398496\n",
      "Iteration 17, loss = 0.70039247\n",
      "Iteration 18, loss = 0.69697194\n",
      "Iteration 19, loss = 0.69371085\n",
      "Iteration 20, loss = 0.69061168\n",
      "Iteration 21, loss = 0.68766740\n",
      "Iteration 22, loss = 0.68486369\n",
      "Iteration 23, loss = 0.68220658\n",
      "Iteration 24, loss = 0.67969045\n",
      "Iteration 25, loss = 0.67729900\n",
      "Iteration 26, loss = 0.67502804\n",
      "Iteration 27, loss = 0.67288306\n",
      "Iteration 28, loss = 0.67087634\n",
      "Iteration 29, loss = 0.66896992\n",
      "Iteration 30, loss = 0.66718549\n",
      "Iteration 31, loss = 0.66550636\n",
      "Iteration 32, loss = 0.66392357\n",
      "Iteration 33, loss = 0.66243430\n",
      "Iteration 34, loss = 0.66101258\n",
      "Iteration 35, loss = 0.65968955\n",
      "Iteration 36, loss = 0.65844214\n",
      "Iteration 37, loss = 0.65725672\n",
      "Iteration 38, loss = 0.65614874\n",
      "Iteration 39, loss = 0.65509473\n",
      "Iteration 40, loss = 0.65410382\n",
      "Iteration 41, loss = 0.65315253\n",
      "Iteration 42, loss = 0.65224798\n",
      "Iteration 43, loss = 0.65138216\n",
      "Iteration 44, loss = 0.65055135\n",
      "Iteration 45, loss = 0.64975357\n",
      "Iteration 46, loss = 0.64897657\n",
      "Iteration 47, loss = 0.64822472\n",
      "Iteration 48, loss = 0.64749854\n",
      "Iteration 49, loss = 0.64678309\n",
      "Iteration 50, loss = 0.64608582\n",
      "Iteration 51, loss = 0.64539683\n",
      "Iteration 52, loss = 0.64471705\n",
      "Iteration 53, loss = 0.64404187\n",
      "Iteration 54, loss = 0.64337567\n",
      "Iteration 55, loss = 0.64271371\n",
      "Iteration 56, loss = 0.64205180\n",
      "Iteration 57, loss = 0.64139061\n",
      "Iteration 58, loss = 0.64073018\n",
      "Iteration 59, loss = 0.64006822\n",
      "Iteration 60, loss = 0.63940609\n",
      "Iteration 61, loss = 0.63874396\n",
      "Iteration 62, loss = 0.63807296\n",
      "Iteration 63, loss = 0.63740327\n",
      "Iteration 64, loss = 0.63673016\n",
      "Iteration 65, loss = 0.63605437\n",
      "Iteration 66, loss = 0.63537350\n",
      "Iteration 67, loss = 0.63468751\n",
      "Iteration 68, loss = 0.63399677\n",
      "Iteration 69, loss = 0.63330489\n",
      "Iteration 70, loss = 0.63260670\n",
      "Iteration 71, loss = 0.63190680\n",
      "Iteration 72, loss = 0.63120184\n",
      "Iteration 73, loss = 0.63049266\n",
      "Iteration 74, loss = 0.62978354\n",
      "Iteration 75, loss = 0.62906404\n",
      "Iteration 76, loss = 0.62834459\n",
      "Iteration 77, loss = 0.62762159\n",
      "Iteration 78, loss = 0.62689450\n",
      "Iteration 79, loss = 0.62616253\n",
      "Iteration 80, loss = 0.62543176\n",
      "Iteration 81, loss = 0.62469804\n",
      "Iteration 82, loss = 0.62396423\n",
      "Iteration 83, loss = 0.62322565\n",
      "Iteration 84, loss = 0.62249686\n",
      "Iteration 85, loss = 0.62174820\n",
      "Iteration 86, loss = 0.62100894\n",
      "Iteration 87, loss = 0.62026431\n",
      "Iteration 88, loss = 0.61951970\n",
      "Iteration 89, loss = 0.61876909\n",
      "Iteration 90, loss = 0.61802017\n",
      "Iteration 91, loss = 0.61727191\n",
      "Iteration 92, loss = 0.61651806\n",
      "Iteration 93, loss = 0.61575885\n",
      "Iteration 94, loss = 0.61500439\n",
      "Iteration 95, loss = 0.61424692\n",
      "Iteration 96, loss = 0.61349701\n",
      "Iteration 97, loss = 0.61273615\n",
      "Iteration 98, loss = 0.61198124\n",
      "Iteration 99, loss = 0.61122716\n",
      "Iteration 100, loss = 0.61047723\n",
      "Iteration 101, loss = 0.60972423\n",
      "Iteration 102, loss = 0.60896965\n",
      "Iteration 103, loss = 0.60821681\n",
      "Iteration 104, loss = 0.60746237\n",
      "Iteration 105, loss = 0.60670819\n",
      "Iteration 106, loss = 0.60595387\n",
      "Iteration 107, loss = 0.60520237\n",
      "Iteration 108, loss = 0.60444890\n",
      "Iteration 109, loss = 0.60369256\n",
      "Iteration 110, loss = 0.60293916\n",
      "Iteration 111, loss = 0.60218249\n",
      "Iteration 112, loss = 0.60142822\n",
      "Iteration 113, loss = 0.60067503\n",
      "Iteration 114, loss = 0.59991657\n",
      "Iteration 115, loss = 0.59916756\n",
      "Iteration 116, loss = 0.59841159\n",
      "Iteration 117, loss = 0.59766120\n",
      "Iteration 118, loss = 0.59691462\n",
      "Iteration 119, loss = 0.59615660\n",
      "Iteration 120, loss = 0.59540393\n",
      "Iteration 121, loss = 0.59465663\n",
      "Iteration 122, loss = 0.59390903\n",
      "Iteration 123, loss = 0.59316282\n",
      "Iteration 124, loss = 0.59241271\n",
      "Iteration 125, loss = 0.59166582\n",
      "Iteration 126, loss = 0.59092200\n",
      "Iteration 127, loss = 0.59017352\n",
      "Iteration 128, loss = 0.58943086\n",
      "Iteration 129, loss = 0.58868332\n",
      "Iteration 130, loss = 0.58794264\n",
      "Iteration 131, loss = 0.58720176\n",
      "Iteration 132, loss = 0.58645982\n",
      "Iteration 133, loss = 0.58571955\n",
      "Iteration 134, loss = 0.58497808\n",
      "Iteration 135, loss = 0.58424427\n",
      "Iteration 136, loss = 0.58350549\n",
      "Iteration 137, loss = 0.58276518\n",
      "Iteration 138, loss = 0.58203180\n",
      "Iteration 139, loss = 0.58129483\n",
      "Iteration 140, loss = 0.58056394\n",
      "Iteration 141, loss = 0.57983401\n",
      "Iteration 142, loss = 0.57910524\n",
      "Iteration 143, loss = 0.57838303\n",
      "Iteration 144, loss = 0.57765987\n",
      "Iteration 145, loss = 0.57694037\n",
      "Iteration 146, loss = 0.57621617\n",
      "Iteration 147, loss = 0.57550121\n",
      "Iteration 148, loss = 0.57478162\n",
      "Iteration 149, loss = 0.57406446\n",
      "Iteration 150, loss = 0.57335183\n",
      "Iteration 151, loss = 0.57264057\n",
      "Iteration 152, loss = 0.57193047\n",
      "Iteration 153, loss = 0.57122646\n",
      "Iteration 154, loss = 0.57051972\n",
      "Iteration 155, loss = 0.56981720\n",
      "Iteration 156, loss = 0.56912069\n",
      "Iteration 157, loss = 0.56841859\n",
      "Iteration 158, loss = 0.56771616\n",
      "Iteration 159, loss = 0.56701838\n",
      "Iteration 160, loss = 0.56632051\n",
      "Iteration 161, loss = 0.56562399\n",
      "Iteration 162, loss = 0.56493664\n",
      "Iteration 163, loss = 0.56423633\n",
      "Iteration 164, loss = 0.56354608\n",
      "Iteration 165, loss = 0.56285575\n",
      "Iteration 166, loss = 0.56217108\n",
      "Iteration 167, loss = 0.56148701\n",
      "Iteration 168, loss = 0.56080033\n",
      "Iteration 169, loss = 0.56011906\n",
      "Iteration 170, loss = 0.55944368\n",
      "Iteration 171, loss = 0.55876602\n",
      "Iteration 172, loss = 0.55810149\n",
      "Iteration 173, loss = 0.55742371\n",
      "Iteration 174, loss = 0.55675690\n",
      "Iteration 175, loss = 0.55609915\n",
      "Iteration 176, loss = 0.55543193\n",
      "Iteration 177, loss = 0.55477381\n",
      "Iteration 178, loss = 0.55411997\n",
      "Iteration 179, loss = 0.55346520\n",
      "Iteration 180, loss = 0.55282223\n",
      "Iteration 181, loss = 0.55216914\n",
      "Iteration 182, loss = 0.55152451\n",
      "Iteration 183, loss = 0.55088824\n",
      "Iteration 184, loss = 0.55024836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 185, loss = 0.54961487\n",
      "Iteration 186, loss = 0.54898979\n",
      "Iteration 187, loss = 0.54835941\n",
      "Iteration 188, loss = 0.54773834\n",
      "Iteration 189, loss = 0.54711595\n",
      "Iteration 190, loss = 0.54649438\n",
      "Iteration 191, loss = 0.54588453\n",
      "Iteration 192, loss = 0.54527044\n",
      "Iteration 193, loss = 0.54465562\n",
      "Iteration 194, loss = 0.54404994\n",
      "Iteration 195, loss = 0.54344213\n",
      "Iteration 196, loss = 0.54284832\n",
      "Iteration 197, loss = 0.54224407\n",
      "Iteration 198, loss = 0.54164776\n",
      "Iteration 199, loss = 0.54105586\n",
      "Iteration 200, loss = 0.54046151\n",
      "Iteration 201, loss = 0.53987452\n",
      "Iteration 202, loss = 0.53928826\n",
      "Iteration 203, loss = 0.53870756\n",
      "Iteration 204, loss = 0.53812670\n",
      "Iteration 205, loss = 0.53755176\n",
      "Iteration 206, loss = 0.53697472\n",
      "Iteration 207, loss = 0.53640730\n",
      "Iteration 208, loss = 0.53584916\n",
      "Iteration 209, loss = 0.53528333\n",
      "Iteration 210, loss = 0.53473953\n",
      "Iteration 211, loss = 0.53418009\n",
      "Iteration 212, loss = 0.53363097\n",
      "Iteration 213, loss = 0.53308810\n",
      "Iteration 214, loss = 0.53254477\n",
      "Iteration 215, loss = 0.53200130\n",
      "Iteration 216, loss = 0.53146957\n",
      "Iteration 217, loss = 0.53093604\n",
      "Iteration 218, loss = 0.53040413\n",
      "Iteration 219, loss = 0.52987662\n",
      "Iteration 220, loss = 0.52934916\n",
      "Iteration 221, loss = 0.52883012\n",
      "Iteration 222, loss = 0.52831466\n",
      "Iteration 223, loss = 0.52779348\n",
      "Iteration 224, loss = 0.52727602\n",
      "Iteration 225, loss = 0.52677483\n",
      "Iteration 226, loss = 0.52626520\n",
      "Iteration 227, loss = 0.52576754\n",
      "Iteration 228, loss = 0.52527112\n",
      "Iteration 229, loss = 0.52477718\n",
      "Iteration 230, loss = 0.52428729\n",
      "Iteration 231, loss = 0.52380027\n",
      "Iteration 232, loss = 0.52331148\n",
      "Iteration 233, loss = 0.52283471\n",
      "Iteration 234, loss = 0.52235664\n",
      "Iteration 235, loss = 0.52187895\n",
      "Iteration 236, loss = 0.52140764\n",
      "Iteration 237, loss = 0.52095347\n",
      "Iteration 238, loss = 0.52047948\n",
      "Iteration 239, loss = 0.52001873\n",
      "Iteration 240, loss = 0.51957110\n",
      "Iteration 241, loss = 0.51911219\n",
      "Iteration 242, loss = 0.51866753\n",
      "Iteration 243, loss = 0.51822414\n",
      "Iteration 244, loss = 0.51777801\n",
      "Iteration 245, loss = 0.51733719\n",
      "Iteration 246, loss = 0.51689872\n",
      "Iteration 247, loss = 0.51646601\n",
      "Iteration 248, loss = 0.51603245\n",
      "Iteration 249, loss = 0.51561116\n",
      "Iteration 250, loss = 0.51518209\n",
      "Iteration 251, loss = 0.51475694\n",
      "Iteration 252, loss = 0.51433749\n",
      "Iteration 253, loss = 0.51392408\n",
      "Iteration 254, loss = 0.51351116\n",
      "Iteration 255, loss = 0.51310508\n",
      "Iteration 256, loss = 0.51269977\n",
      "Iteration 257, loss = 0.51229291\n",
      "Iteration 258, loss = 0.51189753\n",
      "Iteration 259, loss = 0.51150571\n",
      "Iteration 260, loss = 0.51111184\n",
      "Iteration 261, loss = 0.51071832\n",
      "Iteration 262, loss = 0.51033557\n",
      "Iteration 263, loss = 0.50995964\n",
      "Iteration 264, loss = 0.50958472\n",
      "Iteration 265, loss = 0.50919787\n",
      "Iteration 266, loss = 0.50882479\n",
      "Iteration 267, loss = 0.50846120\n",
      "Iteration 268, loss = 0.50809540\n",
      "Iteration 269, loss = 0.50773157\n",
      "Iteration 270, loss = 0.50737133\n",
      "Iteration 271, loss = 0.50701829\n",
      "Iteration 272, loss = 0.50666963\n",
      "Iteration 273, loss = 0.50632280\n",
      "Iteration 274, loss = 0.50597396\n",
      "Iteration 275, loss = 0.50563123\n",
      "Iteration 276, loss = 0.50529532\n",
      "Iteration 277, loss = 0.50495771\n",
      "Iteration 278, loss = 0.50462654\n",
      "Iteration 279, loss = 0.50429446\n",
      "Iteration 280, loss = 0.50396504\n",
      "Iteration 281, loss = 0.50363981\n",
      "Iteration 282, loss = 0.50332505\n",
      "Iteration 283, loss = 0.50300022\n",
      "Iteration 284, loss = 0.50268884\n",
      "Iteration 285, loss = 0.50237160\n",
      "Iteration 286, loss = 0.50206325\n",
      "Iteration 287, loss = 0.50175556\n",
      "Iteration 288, loss = 0.50145102\n",
      "Iteration 289, loss = 0.50115146\n",
      "Iteration 290, loss = 0.50085820\n",
      "Iteration 291, loss = 0.50055780\n",
      "Iteration 292, loss = 0.50026484\n",
      "Iteration 293, loss = 0.49997758\n",
      "Iteration 294, loss = 0.49968919\n",
      "Iteration 295, loss = 0.49940487\n",
      "Iteration 296, loss = 0.49912825\n",
      "Iteration 297, loss = 0.49885122\n",
      "Iteration 298, loss = 0.49857119\n",
      "Iteration 299, loss = 0.49830176\n",
      "Iteration 300, loss = 0.49803596\n",
      "Iteration 301, loss = 0.49776569\n",
      "Iteration 302, loss = 0.49750336\n",
      "Iteration 303, loss = 0.49724420\n",
      "Iteration 304, loss = 0.49698664\n",
      "Iteration 305, loss = 0.49673292\n",
      "Iteration 306, loss = 0.49647986\n",
      "Iteration 307, loss = 0.49622905\n",
      "Iteration 308, loss = 0.49597842\n",
      "Iteration 309, loss = 0.49573592\n",
      "Iteration 310, loss = 0.49548684\n",
      "Iteration 311, loss = 0.49525020\n",
      "Iteration 312, loss = 0.49500904\n",
      "Iteration 313, loss = 0.49478295\n",
      "Iteration 314, loss = 0.49453846\n",
      "Iteration 315, loss = 0.49430988\n",
      "Iteration 316, loss = 0.49408194\n",
      "Iteration 317, loss = 0.49386094\n",
      "Iteration 318, loss = 0.49363839\n",
      "Iteration 319, loss = 0.49341207\n",
      "Iteration 320, loss = 0.49320121\n",
      "Iteration 321, loss = 0.49297700\n",
      "Iteration 322, loss = 0.49277041\n",
      "Iteration 323, loss = 0.49255482\n",
      "Iteration 324, loss = 0.49234440\n",
      "Iteration 325, loss = 0.49213592\n",
      "Iteration 326, loss = 0.49193372\n",
      "Iteration 327, loss = 0.49172912\n",
      "Iteration 328, loss = 0.49152880\n",
      "Iteration 329, loss = 0.49132998\n",
      "Iteration 330, loss = 0.49113352\n",
      "Iteration 331, loss = 0.49094175\n",
      "Iteration 332, loss = 0.49075063\n",
      "Iteration 333, loss = 0.49056289\n",
      "Iteration 334, loss = 0.49037736\n",
      "Iteration 335, loss = 0.49018712\n",
      "Iteration 336, loss = 0.49000022\n",
      "Iteration 337, loss = 0.48982077\n",
      "Iteration 338, loss = 0.48964855\n",
      "Iteration 339, loss = 0.48947141\n",
      "Iteration 340, loss = 0.48929984\n",
      "Iteration 341, loss = 0.48912645\n",
      "Iteration 342, loss = 0.48896161\n",
      "Iteration 343, loss = 0.48879369\n",
      "Iteration 344, loss = 0.48863075\n",
      "Iteration 345, loss = 0.48847254\n",
      "Iteration 346, loss = 0.48831246\n",
      "Iteration 347, loss = 0.48814914\n",
      "Iteration 348, loss = 0.48799649\n",
      "Iteration 349, loss = 0.48783864\n",
      "Iteration 350, loss = 0.48769167\n",
      "Iteration 351, loss = 0.48753860\n",
      "Iteration 352, loss = 0.48739153\n",
      "Iteration 353, loss = 0.48724370\n",
      "Iteration 354, loss = 0.48709471\n",
      "Iteration 355, loss = 0.48695415\n",
      "Iteration 356, loss = 0.48681138\n",
      "Iteration 357, loss = 0.48667850\n",
      "Iteration 358, loss = 0.48654422\n",
      "Iteration 359, loss = 0.48640091\n",
      "Iteration 360, loss = 0.48626444\n",
      "Iteration 361, loss = 0.48613428\n",
      "Iteration 362, loss = 0.48599980\n",
      "Iteration 363, loss = 0.48587715\n",
      "Iteration 364, loss = 0.48574877\n",
      "Iteration 365, loss = 0.48561910\n",
      "Iteration 366, loss = 0.48549500\n",
      "Iteration 367, loss = 0.48537548\n",
      "Iteration 368, loss = 0.48525392\n",
      "Iteration 369, loss = 0.48513438\n",
      "Iteration 370, loss = 0.48501933\n",
      "Iteration 371, loss = 0.48490539\n",
      "Iteration 372, loss = 0.48478443\n",
      "Iteration 373, loss = 0.48467144\n",
      "Iteration 374, loss = 0.48455759\n",
      "Iteration 375, loss = 0.48444504\n",
      "Iteration 376, loss = 0.48433945\n",
      "Iteration 377, loss = 0.48422598\n",
      "Iteration 378, loss = 0.48412136\n",
      "Iteration 379, loss = 0.48402326\n",
      "Iteration 380, loss = 0.48391631\n",
      "Iteration 381, loss = 0.48381335\n",
      "Iteration 382, loss = 0.48371170\n",
      "Iteration 383, loss = 0.48361230\n",
      "Iteration 384, loss = 0.48351042\n",
      "Iteration 385, loss = 0.48341686\n",
      "Iteration 386, loss = 0.48331934\n",
      "Iteration 387, loss = 0.48322727\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75507848\n",
      "Iteration 2, loss = 0.74990718\n",
      "Iteration 3, loss = 0.74492476\n",
      "Iteration 4, loss = 0.74012723\n",
      "Iteration 5, loss = 0.73552143\n",
      "Iteration 6, loss = 0.73111253\n",
      "Iteration 7, loss = 0.72688438\n",
      "Iteration 8, loss = 0.72286008\n",
      "Iteration 9, loss = 0.71898594\n",
      "Iteration 10, loss = 0.71528832\n",
      "Iteration 11, loss = 0.71176528\n",
      "Iteration 12, loss = 0.70841588\n",
      "Iteration 13, loss = 0.70523322\n",
      "Iteration 14, loss = 0.70219877\n",
      "Iteration 15, loss = 0.69931877\n",
      "Iteration 16, loss = 0.69658969\n",
      "Iteration 17, loss = 0.69398001\n",
      "Iteration 18, loss = 0.69152868\n",
      "Iteration 19, loss = 0.68921021\n",
      "Iteration 20, loss = 0.68701745\n",
      "Iteration 21, loss = 0.68495486\n",
      "Iteration 22, loss = 0.68300278\n",
      "Iteration 23, loss = 0.68116202\n",
      "Iteration 24, loss = 0.67942684\n",
      "Iteration 25, loss = 0.67779021\n",
      "Iteration 26, loss = 0.67623041\n",
      "Iteration 27, loss = 0.67476286\n",
      "Iteration 28, loss = 0.67339080\n",
      "Iteration 29, loss = 0.67208171\n",
      "Iteration 30, loss = 0.67084553\n",
      "Iteration 31, loss = 0.66967365\n",
      "Iteration 32, loss = 0.66855680\n",
      "Iteration 33, loss = 0.66749331\n",
      "Iteration 34, loss = 0.66645870\n",
      "Iteration 35, loss = 0.66547673\n",
      "Iteration 36, loss = 0.66453108\n",
      "Iteration 37, loss = 0.66361031\n",
      "Iteration 38, loss = 0.66272394\n",
      "Iteration 39, loss = 0.66185860\n",
      "Iteration 40, loss = 0.66101854\n",
      "Iteration 41, loss = 0.66018874\n",
      "Iteration 42, loss = 0.65937222\n",
      "Iteration 43, loss = 0.65856707\n",
      "Iteration 44, loss = 0.65777147\n",
      "Iteration 45, loss = 0.65698036\n",
      "Iteration 46, loss = 0.65619340\n",
      "Iteration 47, loss = 0.65541089\n",
      "Iteration 48, loss = 0.65463492\n",
      "Iteration 49, loss = 0.65385442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.65307942\n",
      "Iteration 51, loss = 0.65229969\n",
      "Iteration 52, loss = 0.65151961\n",
      "Iteration 53, loss = 0.65073472\n",
      "Iteration 54, loss = 0.64995110\n",
      "Iteration 55, loss = 0.64916184\n",
      "Iteration 56, loss = 0.64836952\n",
      "Iteration 57, loss = 0.64757325\n",
      "Iteration 58, loss = 0.64677565\n",
      "Iteration 59, loss = 0.64597092\n",
      "Iteration 60, loss = 0.64516535\n",
      "Iteration 61, loss = 0.64435644\n",
      "Iteration 62, loss = 0.64353944\n",
      "Iteration 63, loss = 0.64272038\n",
      "Iteration 64, loss = 0.64189662\n",
      "Iteration 65, loss = 0.64107206\n",
      "Iteration 66, loss = 0.64024099\n",
      "Iteration 67, loss = 0.63940558\n",
      "Iteration 68, loss = 0.63856336\n",
      "Iteration 69, loss = 0.63772284\n",
      "Iteration 70, loss = 0.63687613\n",
      "Iteration 71, loss = 0.63602737\n",
      "Iteration 72, loss = 0.63517502\n",
      "Iteration 73, loss = 0.63431758\n",
      "Iteration 74, loss = 0.63346198\n",
      "Iteration 75, loss = 0.63259582\n",
      "Iteration 76, loss = 0.63173109\n",
      "Iteration 77, loss = 0.63086128\n",
      "Iteration 78, loss = 0.62998951\n",
      "Iteration 79, loss = 0.62911196\n",
      "Iteration 80, loss = 0.62823774\n",
      "Iteration 81, loss = 0.62735997\n",
      "Iteration 82, loss = 0.62648078\n",
      "Iteration 83, loss = 0.62560410\n",
      "Iteration 84, loss = 0.62473188\n",
      "Iteration 85, loss = 0.62384346\n",
      "Iteration 86, loss = 0.62296128\n",
      "Iteration 87, loss = 0.62207610\n",
      "Iteration 88, loss = 0.62119187\n",
      "Iteration 89, loss = 0.62030181\n",
      "Iteration 90, loss = 0.61941452\n",
      "Iteration 91, loss = 0.61852657\n",
      "Iteration 92, loss = 0.61764199\n",
      "Iteration 93, loss = 0.61674513\n",
      "Iteration 94, loss = 0.61585410\n",
      "Iteration 95, loss = 0.61496166\n",
      "Iteration 96, loss = 0.61407282\n",
      "Iteration 97, loss = 0.61317920\n",
      "Iteration 98, loss = 0.61228656\n",
      "Iteration 99, loss = 0.61139620\n",
      "Iteration 100, loss = 0.61050898\n",
      "Iteration 101, loss = 0.60962378\n",
      "Iteration 102, loss = 0.60873107\n",
      "Iteration 103, loss = 0.60784186\n",
      "Iteration 104, loss = 0.60695045\n",
      "Iteration 105, loss = 0.60606127\n",
      "Iteration 106, loss = 0.60517064\n",
      "Iteration 107, loss = 0.60428141\n",
      "Iteration 108, loss = 0.60339187\n",
      "Iteration 109, loss = 0.60249919\n",
      "Iteration 110, loss = 0.60160670\n",
      "Iteration 111, loss = 0.60071360\n",
      "Iteration 112, loss = 0.59982104\n",
      "Iteration 113, loss = 0.59892723\n",
      "Iteration 114, loss = 0.59803271\n",
      "Iteration 115, loss = 0.59714111\n",
      "Iteration 116, loss = 0.59624879\n",
      "Iteration 117, loss = 0.59535827\n",
      "Iteration 118, loss = 0.59446730\n",
      "Iteration 119, loss = 0.59357202\n",
      "Iteration 120, loss = 0.59267843\n",
      "Iteration 121, loss = 0.59178865\n",
      "Iteration 122, loss = 0.59090193\n",
      "Iteration 123, loss = 0.59001121\n",
      "Iteration 124, loss = 0.58911695\n",
      "Iteration 125, loss = 0.58822618\n",
      "Iteration 126, loss = 0.58733641\n",
      "Iteration 127, loss = 0.58644320\n",
      "Iteration 128, loss = 0.58555230\n",
      "Iteration 129, loss = 0.58465704\n",
      "Iteration 130, loss = 0.58376878\n",
      "Iteration 131, loss = 0.58287481\n",
      "Iteration 132, loss = 0.58198040\n",
      "Iteration 133, loss = 0.58109226\n",
      "Iteration 134, loss = 0.58019989\n",
      "Iteration 135, loss = 0.57931155\n",
      "Iteration 136, loss = 0.57842584\n",
      "Iteration 137, loss = 0.57753815\n",
      "Iteration 138, loss = 0.57665365\n",
      "Iteration 139, loss = 0.57576755\n",
      "Iteration 140, loss = 0.57488439\n",
      "Iteration 141, loss = 0.57400805\n",
      "Iteration 142, loss = 0.57313075\n",
      "Iteration 143, loss = 0.57226026\n",
      "Iteration 144, loss = 0.57138550\n",
      "Iteration 145, loss = 0.57051818\n",
      "Iteration 146, loss = 0.56964591\n",
      "Iteration 147, loss = 0.56878664\n",
      "Iteration 148, loss = 0.56792282\n",
      "Iteration 149, loss = 0.56705622\n",
      "Iteration 150, loss = 0.56619654\n",
      "Iteration 151, loss = 0.56533781\n",
      "Iteration 152, loss = 0.56448253\n",
      "Iteration 153, loss = 0.56363655\n",
      "Iteration 154, loss = 0.56278287\n",
      "Iteration 155, loss = 0.56193685\n",
      "Iteration 156, loss = 0.56109850\n",
      "Iteration 157, loss = 0.56024946\n",
      "Iteration 158, loss = 0.55940602\n",
      "Iteration 159, loss = 0.55856535\n",
      "Iteration 160, loss = 0.55772617\n",
      "Iteration 161, loss = 0.55688985\n",
      "Iteration 162, loss = 0.55606001\n",
      "Iteration 163, loss = 0.55522084\n",
      "Iteration 164, loss = 0.55439269\n",
      "Iteration 165, loss = 0.55356594\n",
      "Iteration 166, loss = 0.55274428\n",
      "Iteration 167, loss = 0.55192636\n",
      "Iteration 168, loss = 0.55109845\n",
      "Iteration 169, loss = 0.55028398\n",
      "Iteration 170, loss = 0.54947454\n",
      "Iteration 171, loss = 0.54866223\n",
      "Iteration 172, loss = 0.54786219\n",
      "Iteration 173, loss = 0.54705028\n",
      "Iteration 174, loss = 0.54624970\n",
      "Iteration 175, loss = 0.54544991\n",
      "Iteration 176, loss = 0.54465448\n",
      "Iteration 177, loss = 0.54386107\n",
      "Iteration 178, loss = 0.54306919\n",
      "Iteration 179, loss = 0.54228102\n",
      "Iteration 180, loss = 0.54149998\n",
      "Iteration 181, loss = 0.54070922\n",
      "Iteration 182, loss = 0.53993118\n",
      "Iteration 183, loss = 0.53915888\n",
      "Iteration 184, loss = 0.53838856\n",
      "Iteration 185, loss = 0.53761431\n",
      "Iteration 186, loss = 0.53685842\n",
      "Iteration 187, loss = 0.53608587\n",
      "Iteration 188, loss = 0.53533394\n",
      "Iteration 189, loss = 0.53457970\n",
      "Iteration 190, loss = 0.53382660\n",
      "Iteration 191, loss = 0.53307943\n",
      "Iteration 192, loss = 0.53233841\n",
      "Iteration 193, loss = 0.53159267\n",
      "Iteration 194, loss = 0.53085849\n",
      "Iteration 195, loss = 0.53012151\n",
      "Iteration 196, loss = 0.52939825\n",
      "Iteration 197, loss = 0.52866427\n",
      "Iteration 198, loss = 0.52794203\n",
      "Iteration 199, loss = 0.52722718\n",
      "Iteration 200, loss = 0.52650747\n",
      "Iteration 201, loss = 0.52579738\n",
      "Iteration 202, loss = 0.52509057\n",
      "Iteration 203, loss = 0.52438498\n",
      "Iteration 204, loss = 0.52368208\n",
      "Iteration 205, loss = 0.52298391\n",
      "Iteration 206, loss = 0.52228423\n",
      "Iteration 207, loss = 0.52159336\n",
      "Iteration 208, loss = 0.52091223\n",
      "Iteration 209, loss = 0.52022714\n",
      "Iteration 210, loss = 0.51956038\n",
      "Iteration 211, loss = 0.51888023\n",
      "Iteration 212, loss = 0.51821526\n",
      "Iteration 213, loss = 0.51755779\n",
      "Iteration 214, loss = 0.51689445\n",
      "Iteration 215, loss = 0.51623250\n",
      "Iteration 216, loss = 0.51558429\n",
      "Iteration 217, loss = 0.51493343\n",
      "Iteration 218, loss = 0.51428886\n",
      "Iteration 219, loss = 0.51364829\n",
      "Iteration 220, loss = 0.51300579\n",
      "Iteration 221, loss = 0.51236999\n",
      "Iteration 222, loss = 0.51174114\n",
      "Iteration 223, loss = 0.51111021\n",
      "Iteration 224, loss = 0.51048254\n",
      "Iteration 225, loss = 0.50986996\n",
      "Iteration 226, loss = 0.50924774\n",
      "Iteration 227, loss = 0.50863965\n",
      "Iteration 228, loss = 0.50802993\n",
      "Iteration 229, loss = 0.50742429\n",
      "Iteration 230, loss = 0.50681995\n",
      "Iteration 231, loss = 0.50622503\n",
      "Iteration 232, loss = 0.50562341\n",
      "Iteration 233, loss = 0.50503582\n",
      "Iteration 234, loss = 0.50445121\n",
      "Iteration 235, loss = 0.50386346\n",
      "Iteration 236, loss = 0.50328313\n",
      "Iteration 237, loss = 0.50272197\n",
      "Iteration 238, loss = 0.50214219\n",
      "Iteration 239, loss = 0.50157389\n",
      "Iteration 240, loss = 0.50101604\n",
      "Iteration 241, loss = 0.50045283\n",
      "Iteration 242, loss = 0.49989896\n",
      "Iteration 243, loss = 0.49935006\n",
      "Iteration 244, loss = 0.49880618\n",
      "Iteration 245, loss = 0.49825522\n",
      "Iteration 246, loss = 0.49771765\n",
      "Iteration 247, loss = 0.49718450\n",
      "Iteration 248, loss = 0.49665355\n",
      "Iteration 249, loss = 0.49612970\n",
      "Iteration 250, loss = 0.49560439\n",
      "Iteration 251, loss = 0.49508378\n",
      "Iteration 252, loss = 0.49456690\n",
      "Iteration 253, loss = 0.49405730\n",
      "Iteration 254, loss = 0.49354794\n",
      "Iteration 255, loss = 0.49304374\n",
      "Iteration 256, loss = 0.49254381\n",
      "Iteration 257, loss = 0.49204914\n",
      "Iteration 258, loss = 0.49155833\n",
      "Iteration 259, loss = 0.49106929\n",
      "Iteration 260, loss = 0.49058386\n",
      "Iteration 261, loss = 0.49009780\n",
      "Iteration 262, loss = 0.48961654\n",
      "Iteration 263, loss = 0.48914395\n",
      "Iteration 264, loss = 0.48867824\n",
      "Iteration 265, loss = 0.48819995\n",
      "Iteration 266, loss = 0.48773476\n",
      "Iteration 267, loss = 0.48727553\n",
      "Iteration 268, loss = 0.48682075\n",
      "Iteration 269, loss = 0.48636319\n",
      "Iteration 270, loss = 0.48591231\n",
      "Iteration 271, loss = 0.48547108\n",
      "Iteration 272, loss = 0.48502797\n",
      "Iteration 273, loss = 0.48458836\n",
      "Iteration 274, loss = 0.48414685\n",
      "Iteration 275, loss = 0.48371435\n",
      "Iteration 276, loss = 0.48328871\n",
      "Iteration 277, loss = 0.48285867\n",
      "Iteration 278, loss = 0.48243738\n",
      "Iteration 279, loss = 0.48201843\n",
      "Iteration 280, loss = 0.48159702\n",
      "Iteration 281, loss = 0.48118500\n",
      "Iteration 282, loss = 0.48078099\n",
      "Iteration 283, loss = 0.48036935\n",
      "Iteration 284, loss = 0.47996843\n",
      "Iteration 285, loss = 0.47956795\n",
      "Iteration 286, loss = 0.47916917\n",
      "Iteration 287, loss = 0.47877657\n",
      "Iteration 288, loss = 0.47838303\n",
      "Iteration 289, loss = 0.47799908\n",
      "Iteration 290, loss = 0.47761757\n",
      "Iteration 291, loss = 0.47722939\n",
      "Iteration 292, loss = 0.47685572\n",
      "Iteration 293, loss = 0.47647681\n",
      "Iteration 294, loss = 0.47610490\n",
      "Iteration 295, loss = 0.47573710\n",
      "Iteration 296, loss = 0.47537449\n",
      "Iteration 297, loss = 0.47501811\n",
      "Iteration 298, loss = 0.47465174\n",
      "Iteration 299, loss = 0.47430329\n",
      "Iteration 300, loss = 0.47395118\n",
      "Iteration 301, loss = 0.47360044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 302, loss = 0.47325625\n",
      "Iteration 303, loss = 0.47291371\n",
      "Iteration 304, loss = 0.47257302\n",
      "Iteration 305, loss = 0.47224307\n",
      "Iteration 306, loss = 0.47190512\n",
      "Iteration 307, loss = 0.47157660\n",
      "Iteration 308, loss = 0.47124883\n",
      "Iteration 309, loss = 0.47092376\n",
      "Iteration 310, loss = 0.47060002\n",
      "Iteration 311, loss = 0.47028693\n",
      "Iteration 312, loss = 0.46997035\n",
      "Iteration 313, loss = 0.46966509\n",
      "Iteration 314, loss = 0.46935191\n",
      "Iteration 315, loss = 0.46904733\n",
      "Iteration 316, loss = 0.46874564\n",
      "Iteration 317, loss = 0.46844639\n",
      "Iteration 318, loss = 0.46814998\n",
      "Iteration 319, loss = 0.46785203\n",
      "Iteration 320, loss = 0.46756611\n",
      "Iteration 321, loss = 0.46727320\n",
      "Iteration 322, loss = 0.46699642\n",
      "Iteration 323, loss = 0.46670333\n",
      "Iteration 324, loss = 0.46642307\n",
      "Iteration 325, loss = 0.46614497\n",
      "Iteration 326, loss = 0.46586931\n",
      "Iteration 327, loss = 0.46559519\n",
      "Iteration 328, loss = 0.46532136\n",
      "Iteration 329, loss = 0.46505272\n",
      "Iteration 330, loss = 0.46478534\n",
      "Iteration 331, loss = 0.46453135\n",
      "Iteration 332, loss = 0.46426499\n",
      "Iteration 333, loss = 0.46400900\n",
      "Iteration 334, loss = 0.46375132\n",
      "Iteration 335, loss = 0.46349704\n",
      "Iteration 336, loss = 0.46324850\n",
      "Iteration 337, loss = 0.46299676\n",
      "Iteration 338, loss = 0.46275822\n",
      "Iteration 339, loss = 0.46251400\n",
      "Iteration 340, loss = 0.46227670\n",
      "Iteration 341, loss = 0.46203944\n",
      "Iteration 342, loss = 0.46180371\n",
      "Iteration 343, loss = 0.46157370\n",
      "Iteration 344, loss = 0.46134634\n",
      "Iteration 345, loss = 0.46111959\n",
      "Iteration 346, loss = 0.46090204\n",
      "Iteration 347, loss = 0.46067359\n",
      "Iteration 348, loss = 0.46045355\n",
      "Iteration 349, loss = 0.46023905\n",
      "Iteration 350, loss = 0.46002870\n",
      "Iteration 351, loss = 0.45981409\n",
      "Iteration 352, loss = 0.45960930\n",
      "Iteration 353, loss = 0.45939698\n",
      "Iteration 354, loss = 0.45918692\n",
      "Iteration 355, loss = 0.45898485\n",
      "Iteration 356, loss = 0.45878647\n",
      "Iteration 357, loss = 0.45859389\n",
      "Iteration 358, loss = 0.45839731\n",
      "Iteration 359, loss = 0.45819469\n",
      "Iteration 360, loss = 0.45799877\n",
      "Iteration 361, loss = 0.45781405\n",
      "Iteration 362, loss = 0.45761762\n",
      "Iteration 363, loss = 0.45743685\n",
      "Iteration 364, loss = 0.45725281\n",
      "Iteration 365, loss = 0.45706968\n",
      "Iteration 366, loss = 0.45688530\n",
      "Iteration 367, loss = 0.45670484\n",
      "Iteration 368, loss = 0.45652785\n",
      "Iteration 369, loss = 0.45635399\n",
      "Iteration 370, loss = 0.45618143\n",
      "Iteration 371, loss = 0.45601253\n",
      "Iteration 372, loss = 0.45583532\n",
      "Iteration 373, loss = 0.45567217\n",
      "Iteration 374, loss = 0.45549742\n",
      "Iteration 375, loss = 0.45533617\n",
      "Iteration 376, loss = 0.45517322\n",
      "Iteration 377, loss = 0.45500735\n",
      "Iteration 378, loss = 0.45484990\n",
      "Iteration 379, loss = 0.45469879\n",
      "Iteration 380, loss = 0.45453598\n",
      "Iteration 381, loss = 0.45438607\n",
      "Iteration 382, loss = 0.45423318\n",
      "Iteration 383, loss = 0.45408220\n",
      "Iteration 384, loss = 0.45392987\n",
      "Iteration 385, loss = 0.45378845\n",
      "Iteration 386, loss = 0.45364093\n",
      "Iteration 387, loss = 0.45349715\n",
      "Iteration 388, loss = 0.45336182\n",
      "Iteration 389, loss = 0.45322649\n",
      "Iteration 390, loss = 0.45307928\n",
      "Iteration 391, loss = 0.45295115\n",
      "Iteration 392, loss = 0.45281758\n",
      "Iteration 393, loss = 0.45267993\n",
      "Iteration 394, loss = 0.45255490\n",
      "Iteration 395, loss = 0.45241566\n",
      "Iteration 396, loss = 0.45229340\n",
      "Iteration 397, loss = 0.45216351\n",
      "Iteration 398, loss = 0.45204899\n",
      "Iteration 399, loss = 0.45191601\n",
      "Iteration 400, loss = 0.45179509\n",
      "Iteration 401, loss = 0.45167209\n",
      "Iteration 402, loss = 0.45155615\n",
      "Iteration 403, loss = 0.45144110\n",
      "Iteration 404, loss = 0.45132839\n",
      "Iteration 405, loss = 0.45121804\n",
      "Iteration 406, loss = 0.45109978\n",
      "Iteration 407, loss = 0.45098433\n",
      "Iteration 408, loss = 0.45087726\n",
      "Iteration 409, loss = 0.45077151\n",
      "Iteration 410, loss = 0.45066127\n",
      "Iteration 411, loss = 0.45056157\n",
      "Iteration 412, loss = 0.45045449\n",
      "Iteration 413, loss = 0.45035508\n",
      "Iteration 414, loss = 0.45026019\n",
      "Iteration 415, loss = 0.45014884\n",
      "Iteration 416, loss = 0.45005303\n",
      "Iteration 417, loss = 0.44995828\n",
      "Iteration 418, loss = 0.44986488\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72792439\n",
      "Iteration 2, loss = 0.72417703\n",
      "Iteration 3, loss = 0.72059281\n",
      "Iteration 4, loss = 0.71718127\n",
      "Iteration 5, loss = 0.71395417\n",
      "Iteration 6, loss = 0.71091295\n",
      "Iteration 7, loss = 0.70803094\n",
      "Iteration 8, loss = 0.70532948\n",
      "Iteration 9, loss = 0.70276149\n",
      "Iteration 10, loss = 0.70035212\n",
      "Iteration 11, loss = 0.69807762\n",
      "Iteration 12, loss = 0.69595507\n",
      "Iteration 13, loss = 0.69396643\n",
      "Iteration 14, loss = 0.69209065\n",
      "Iteration 15, loss = 0.69033056\n",
      "Iteration 16, loss = 0.68868209\n",
      "Iteration 17, loss = 0.68711329\n",
      "Iteration 18, loss = 0.68565241\n",
      "Iteration 19, loss = 0.68427941\n",
      "Iteration 20, loss = 0.68298161\n",
      "Iteration 21, loss = 0.68176378\n",
      "Iteration 22, loss = 0.68060398\n",
      "Iteration 23, loss = 0.67950294\n",
      "Iteration 24, loss = 0.67845810\n",
      "Iteration 25, loss = 0.67746573\n",
      "Iteration 26, loss = 0.67649967\n",
      "Iteration 27, loss = 0.67557298\n",
      "Iteration 28, loss = 0.67469098\n",
      "Iteration 29, loss = 0.67383387\n",
      "Iteration 30, loss = 0.67299648\n",
      "Iteration 31, loss = 0.67218051\n",
      "Iteration 32, loss = 0.67138285\n",
      "Iteration 33, loss = 0.67059568\n",
      "Iteration 34, loss = 0.66981367\n",
      "Iteration 35, loss = 0.66904504\n",
      "Iteration 36, loss = 0.66828825\n",
      "Iteration 37, loss = 0.66752993\n",
      "Iteration 38, loss = 0.66677686\n",
      "Iteration 39, loss = 0.66602745\n",
      "Iteration 40, loss = 0.66528180\n",
      "Iteration 41, loss = 0.66453108\n",
      "Iteration 42, loss = 0.66378118\n",
      "Iteration 43, loss = 0.66303191\n",
      "Iteration 44, loss = 0.66228187\n",
      "Iteration 45, loss = 0.66152267\n",
      "Iteration 46, loss = 0.66076726\n",
      "Iteration 47, loss = 0.66000992\n",
      "Iteration 48, loss = 0.65925080\n",
      "Iteration 49, loss = 0.65848747\n",
      "Iteration 50, loss = 0.65772629\n",
      "Iteration 51, loss = 0.65695486\n",
      "Iteration 52, loss = 0.65618375\n",
      "Iteration 53, loss = 0.65540794\n",
      "Iteration 54, loss = 0.65463273\n",
      "Iteration 55, loss = 0.65385499\n",
      "Iteration 56, loss = 0.65307345\n",
      "Iteration 57, loss = 0.65229024\n",
      "Iteration 58, loss = 0.65150903\n",
      "Iteration 59, loss = 0.65071668\n",
      "Iteration 60, loss = 0.64992994\n",
      "Iteration 61, loss = 0.64913728\n",
      "Iteration 62, loss = 0.64834081\n",
      "Iteration 63, loss = 0.64754466\n",
      "Iteration 64, loss = 0.64674500\n",
      "Iteration 65, loss = 0.64594613\n",
      "Iteration 66, loss = 0.64514267\n",
      "Iteration 67, loss = 0.64433568\n",
      "Iteration 68, loss = 0.64352621\n",
      "Iteration 69, loss = 0.64271504\n",
      "Iteration 70, loss = 0.64190100\n",
      "Iteration 71, loss = 0.64108775\n",
      "Iteration 72, loss = 0.64027068\n",
      "Iteration 73, loss = 0.63945009\n",
      "Iteration 74, loss = 0.63863432\n",
      "Iteration 75, loss = 0.63780832\n",
      "Iteration 76, loss = 0.63698635\n",
      "Iteration 77, loss = 0.63615933\n",
      "Iteration 78, loss = 0.63533253\n",
      "Iteration 79, loss = 0.63449881\n",
      "Iteration 80, loss = 0.63366895\n",
      "Iteration 81, loss = 0.63283840\n",
      "Iteration 82, loss = 0.63200330\n",
      "Iteration 83, loss = 0.63117119\n",
      "Iteration 84, loss = 0.63034510\n",
      "Iteration 85, loss = 0.62950485\n",
      "Iteration 86, loss = 0.62866920\n",
      "Iteration 87, loss = 0.62783211\n",
      "Iteration 88, loss = 0.62699736\n",
      "Iteration 89, loss = 0.62615718\n",
      "Iteration 90, loss = 0.62531624\n",
      "Iteration 91, loss = 0.62447981\n",
      "Iteration 92, loss = 0.62364396\n",
      "Iteration 93, loss = 0.62280060\n",
      "Iteration 94, loss = 0.62196046\n",
      "Iteration 95, loss = 0.62112290\n",
      "Iteration 96, loss = 0.62027914\n",
      "Iteration 97, loss = 0.61943979\n",
      "Iteration 98, loss = 0.61860118\n",
      "Iteration 99, loss = 0.61776145\n",
      "Iteration 100, loss = 0.61692611\n",
      "Iteration 101, loss = 0.61609095\n",
      "Iteration 102, loss = 0.61525085\n",
      "Iteration 103, loss = 0.61441063\n",
      "Iteration 104, loss = 0.61357229\n",
      "Iteration 105, loss = 0.61273687\n",
      "Iteration 106, loss = 0.61189845\n",
      "Iteration 107, loss = 0.61106114\n",
      "Iteration 108, loss = 0.61022253\n",
      "Iteration 109, loss = 0.60938064\n",
      "Iteration 110, loss = 0.60854016\n",
      "Iteration 111, loss = 0.60769864\n",
      "Iteration 112, loss = 0.60686091\n",
      "Iteration 113, loss = 0.60601701\n",
      "Iteration 114, loss = 0.60517460\n",
      "Iteration 115, loss = 0.60433233\n",
      "Iteration 116, loss = 0.60348945\n",
      "Iteration 117, loss = 0.60265209\n",
      "Iteration 118, loss = 0.60180923\n",
      "Iteration 119, loss = 0.60096666\n",
      "Iteration 120, loss = 0.60011959\n",
      "Iteration 121, loss = 0.59928186\n",
      "Iteration 122, loss = 0.59844369\n",
      "Iteration 123, loss = 0.59760039\n",
      "Iteration 124, loss = 0.59675795\n",
      "Iteration 125, loss = 0.59591461\n",
      "Iteration 126, loss = 0.59507443\n",
      "Iteration 127, loss = 0.59423422\n",
      "Iteration 128, loss = 0.59339432\n",
      "Iteration 129, loss = 0.59255412\n",
      "Iteration 130, loss = 0.59171523\n",
      "Iteration 131, loss = 0.59088009\n",
      "Iteration 132, loss = 0.59003893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 133, loss = 0.58920525\n",
      "Iteration 134, loss = 0.58837258\n",
      "Iteration 135, loss = 0.58754090\n",
      "Iteration 136, loss = 0.58671300\n",
      "Iteration 137, loss = 0.58588274\n",
      "Iteration 138, loss = 0.58505653\n",
      "Iteration 139, loss = 0.58422775\n",
      "Iteration 140, loss = 0.58340443\n",
      "Iteration 141, loss = 0.58258802\n",
      "Iteration 142, loss = 0.58176726\n",
      "Iteration 143, loss = 0.58095224\n",
      "Iteration 144, loss = 0.58013419\n",
      "Iteration 145, loss = 0.57932498\n",
      "Iteration 146, loss = 0.57850794\n",
      "Iteration 147, loss = 0.57770485\n",
      "Iteration 148, loss = 0.57689762\n",
      "Iteration 149, loss = 0.57609080\n",
      "Iteration 150, loss = 0.57528935\n",
      "Iteration 151, loss = 0.57448659\n",
      "Iteration 152, loss = 0.57368864\n",
      "Iteration 153, loss = 0.57290046\n",
      "Iteration 154, loss = 0.57210259\n",
      "Iteration 155, loss = 0.57131348\n",
      "Iteration 156, loss = 0.57053535\n",
      "Iteration 157, loss = 0.56973629\n",
      "Iteration 158, loss = 0.56895290\n",
      "Iteration 159, loss = 0.56816775\n",
      "Iteration 160, loss = 0.56738793\n",
      "Iteration 161, loss = 0.56660835\n",
      "Iteration 162, loss = 0.56583206\n",
      "Iteration 163, loss = 0.56505851\n",
      "Iteration 164, loss = 0.56428854\n",
      "Iteration 165, loss = 0.56352405\n",
      "Iteration 166, loss = 0.56276609\n",
      "Iteration 167, loss = 0.56200637\n",
      "Iteration 168, loss = 0.56124328\n",
      "Iteration 169, loss = 0.56048597\n",
      "Iteration 170, loss = 0.55973906\n",
      "Iteration 171, loss = 0.55898900\n",
      "Iteration 172, loss = 0.55824732\n",
      "Iteration 173, loss = 0.55749984\n",
      "Iteration 174, loss = 0.55676112\n",
      "Iteration 175, loss = 0.55602492\n",
      "Iteration 176, loss = 0.55529423\n",
      "Iteration 177, loss = 0.55456092\n",
      "Iteration 178, loss = 0.55383496\n",
      "Iteration 179, loss = 0.55310929\n",
      "Iteration 180, loss = 0.55239136\n",
      "Iteration 181, loss = 0.55166383\n",
      "Iteration 182, loss = 0.55094726\n",
      "Iteration 183, loss = 0.55023846\n",
      "Iteration 184, loss = 0.54953554\n",
      "Iteration 185, loss = 0.54881658\n",
      "Iteration 186, loss = 0.54812000\n",
      "Iteration 187, loss = 0.54741582\n",
      "Iteration 188, loss = 0.54672391\n",
      "Iteration 189, loss = 0.54603392\n",
      "Iteration 190, loss = 0.54534117\n",
      "Iteration 191, loss = 0.54465735\n",
      "Iteration 192, loss = 0.54398099\n",
      "Iteration 193, loss = 0.54329815\n",
      "Iteration 194, loss = 0.54262629\n",
      "Iteration 195, loss = 0.54195126\n",
      "Iteration 196, loss = 0.54128954\n",
      "Iteration 197, loss = 0.54062012\n",
      "Iteration 198, loss = 0.53996094\n",
      "Iteration 199, loss = 0.53930753\n",
      "Iteration 200, loss = 0.53865237\n",
      "Iteration 201, loss = 0.53800302\n",
      "Iteration 202, loss = 0.53736636\n",
      "Iteration 203, loss = 0.53672019\n",
      "Iteration 204, loss = 0.53607956\n",
      "Iteration 205, loss = 0.53544145\n",
      "Iteration 206, loss = 0.53480649\n",
      "Iteration 207, loss = 0.53417836\n",
      "Iteration 208, loss = 0.53355274\n",
      "Iteration 209, loss = 0.53292998\n",
      "Iteration 210, loss = 0.53231583\n",
      "Iteration 211, loss = 0.53170082\n",
      "Iteration 212, loss = 0.53109535\n",
      "Iteration 213, loss = 0.53049715\n",
      "Iteration 214, loss = 0.52989004\n",
      "Iteration 215, loss = 0.52928686\n",
      "Iteration 216, loss = 0.52869739\n",
      "Iteration 217, loss = 0.52810419\n",
      "Iteration 218, loss = 0.52751756\n",
      "Iteration 219, loss = 0.52693656\n",
      "Iteration 220, loss = 0.52635226\n",
      "Iteration 221, loss = 0.52577523\n",
      "Iteration 222, loss = 0.52520388\n",
      "Iteration 223, loss = 0.52463465\n",
      "Iteration 224, loss = 0.52406635\n",
      "Iteration 225, loss = 0.52350955\n",
      "Iteration 226, loss = 0.52294301\n",
      "Iteration 227, loss = 0.52239196\n",
      "Iteration 228, loss = 0.52183682\n",
      "Iteration 229, loss = 0.52128944\n",
      "Iteration 230, loss = 0.52074517\n",
      "Iteration 231, loss = 0.52021151\n",
      "Iteration 232, loss = 0.51966669\n",
      "Iteration 233, loss = 0.51913223\n",
      "Iteration 234, loss = 0.51860619\n",
      "Iteration 235, loss = 0.51807472\n",
      "Iteration 236, loss = 0.51754820\n",
      "Iteration 237, loss = 0.51703806\n",
      "Iteration 238, loss = 0.51651288\n",
      "Iteration 239, loss = 0.51599747\n",
      "Iteration 240, loss = 0.51549254\n",
      "Iteration 241, loss = 0.51498122\n",
      "Iteration 242, loss = 0.51447927\n",
      "Iteration 243, loss = 0.51398197\n",
      "Iteration 244, loss = 0.51349171\n",
      "Iteration 245, loss = 0.51299277\n",
      "Iteration 246, loss = 0.51250789\n",
      "Iteration 247, loss = 0.51202684\n",
      "Iteration 248, loss = 0.51154752\n",
      "Iteration 249, loss = 0.51107395\n",
      "Iteration 250, loss = 0.51059935\n",
      "Iteration 251, loss = 0.51013312\n",
      "Iteration 252, loss = 0.50966511\n",
      "Iteration 253, loss = 0.50920575\n",
      "Iteration 254, loss = 0.50874746\n",
      "Iteration 255, loss = 0.50829358\n",
      "Iteration 256, loss = 0.50784223\n",
      "Iteration 257, loss = 0.50739764\n",
      "Iteration 258, loss = 0.50695903\n",
      "Iteration 259, loss = 0.50651387\n",
      "Iteration 260, loss = 0.50608253\n",
      "Iteration 261, loss = 0.50564889\n",
      "Iteration 262, loss = 0.50521908\n",
      "Iteration 263, loss = 0.50479374\n",
      "Iteration 264, loss = 0.50438035\n",
      "Iteration 265, loss = 0.50395243\n",
      "Iteration 266, loss = 0.50353966\n",
      "Iteration 267, loss = 0.50313109\n",
      "Iteration 268, loss = 0.50272278\n",
      "Iteration 269, loss = 0.50231998\n",
      "Iteration 270, loss = 0.50191699\n",
      "Iteration 271, loss = 0.50152380\n",
      "Iteration 272, loss = 0.50112916\n",
      "Iteration 273, loss = 0.50074092\n",
      "Iteration 274, loss = 0.50034919\n",
      "Iteration 275, loss = 0.49996249\n",
      "Iteration 276, loss = 0.49958396\n",
      "Iteration 277, loss = 0.49920428\n",
      "Iteration 278, loss = 0.49882903\n",
      "Iteration 279, loss = 0.49846156\n",
      "Iteration 280, loss = 0.49808107\n",
      "Iteration 281, loss = 0.49771658\n",
      "Iteration 282, loss = 0.49736167\n",
      "Iteration 283, loss = 0.49699392\n",
      "Iteration 284, loss = 0.49664187\n",
      "Iteration 285, loss = 0.49628315\n",
      "Iteration 286, loss = 0.49593285\n",
      "Iteration 287, loss = 0.49558166\n",
      "Iteration 288, loss = 0.49523275\n",
      "Iteration 289, loss = 0.49489476\n",
      "Iteration 290, loss = 0.49455417\n",
      "Iteration 291, loss = 0.49421582\n",
      "Iteration 292, loss = 0.49388253\n",
      "Iteration 293, loss = 0.49354786\n",
      "Iteration 294, loss = 0.49321910\n",
      "Iteration 295, loss = 0.49289367\n",
      "Iteration 296, loss = 0.49257374\n",
      "Iteration 297, loss = 0.49225512\n",
      "Iteration 298, loss = 0.49193489\n",
      "Iteration 299, loss = 0.49162554\n",
      "Iteration 300, loss = 0.49131591\n",
      "Iteration 301, loss = 0.49100378\n",
      "Iteration 302, loss = 0.49070066\n",
      "Iteration 303, loss = 0.49040005\n",
      "Iteration 304, loss = 0.49009915\n",
      "Iteration 305, loss = 0.48980549\n",
      "Iteration 306, loss = 0.48951123\n",
      "Iteration 307, loss = 0.48921967\n",
      "Iteration 308, loss = 0.48892859\n",
      "Iteration 309, loss = 0.48864587\n",
      "Iteration 310, loss = 0.48836013\n",
      "Iteration 311, loss = 0.48808612\n",
      "Iteration 312, loss = 0.48780874\n",
      "Iteration 313, loss = 0.48753745\n",
      "Iteration 314, loss = 0.48725874\n",
      "Iteration 315, loss = 0.48699221\n",
      "Iteration 316, loss = 0.48672630\n",
      "Iteration 317, loss = 0.48646397\n",
      "Iteration 318, loss = 0.48620139\n",
      "Iteration 319, loss = 0.48593993\n",
      "Iteration 320, loss = 0.48568630\n",
      "Iteration 321, loss = 0.48543246\n",
      "Iteration 322, loss = 0.48519310\n",
      "Iteration 323, loss = 0.48493530\n",
      "Iteration 324, loss = 0.48468865\n",
      "Iteration 325, loss = 0.48444434\n",
      "Iteration 326, loss = 0.48420535\n",
      "Iteration 327, loss = 0.48396908\n",
      "Iteration 328, loss = 0.48373079\n",
      "Iteration 329, loss = 0.48349752\n",
      "Iteration 330, loss = 0.48326209\n",
      "Iteration 331, loss = 0.48304245\n",
      "Iteration 332, loss = 0.48280995\n",
      "Iteration 333, loss = 0.48258933\n",
      "Iteration 334, loss = 0.48236387\n",
      "Iteration 335, loss = 0.48214620\n",
      "Iteration 336, loss = 0.48192626\n",
      "Iteration 337, loss = 0.48171265\n",
      "Iteration 338, loss = 0.48150808\n",
      "Iteration 339, loss = 0.48129466\n",
      "Iteration 340, loss = 0.48109148\n",
      "Iteration 341, loss = 0.48089114\n",
      "Iteration 342, loss = 0.48068480\n",
      "Iteration 343, loss = 0.48049076\n",
      "Iteration 344, loss = 0.48029237\n",
      "Iteration 345, loss = 0.48009595\n",
      "Iteration 346, loss = 0.47991046\n",
      "Iteration 347, loss = 0.47971385\n",
      "Iteration 348, loss = 0.47952575\n",
      "Iteration 349, loss = 0.47933817\n",
      "Iteration 350, loss = 0.47915686\n",
      "Iteration 351, loss = 0.47897689\n",
      "Iteration 352, loss = 0.47879837\n",
      "Iteration 353, loss = 0.47861876\n",
      "Iteration 354, loss = 0.47843861\n",
      "Iteration 355, loss = 0.47826848\n",
      "Iteration 356, loss = 0.47810492\n",
      "Iteration 357, loss = 0.47793617\n",
      "Iteration 358, loss = 0.47777042\n",
      "Iteration 359, loss = 0.47759954\n",
      "Iteration 360, loss = 0.47743344\n",
      "Iteration 361, loss = 0.47727782\n",
      "Iteration 362, loss = 0.47711549\n",
      "Iteration 363, loss = 0.47695660\n",
      "Iteration 364, loss = 0.47680280\n",
      "Iteration 365, loss = 0.47664963\n",
      "Iteration 366, loss = 0.47649304\n",
      "Iteration 367, loss = 0.47634441\n",
      "Iteration 368, loss = 0.47619921\n",
      "Iteration 369, loss = 0.47605171\n",
      "Iteration 370, loss = 0.47590547\n",
      "Iteration 371, loss = 0.47576235\n",
      "Iteration 372, loss = 0.47562026\n",
      "Iteration 373, loss = 0.47548382\n",
      "Iteration 374, loss = 0.47534017\n",
      "Iteration 375, loss = 0.47520555\n",
      "Iteration 376, loss = 0.47506847\n",
      "Iteration 377, loss = 0.47493492\n",
      "Iteration 378, loss = 0.47480595\n",
      "Iteration 379, loss = 0.47467821\n",
      "Iteration 380, loss = 0.47454421\n",
      "Iteration 381, loss = 0.47442716\n",
      "Iteration 382, loss = 0.47430303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 383, loss = 0.47417139\n",
      "Iteration 384, loss = 0.47405055\n",
      "Iteration 385, loss = 0.47393528\n",
      "Iteration 386, loss = 0.47381751\n",
      "Iteration 387, loss = 0.47370015\n",
      "Iteration 388, loss = 0.47358780\n",
      "Iteration 389, loss = 0.47348005\n",
      "Iteration 390, loss = 0.47336254\n",
      "Iteration 391, loss = 0.47325457\n",
      "Iteration 392, loss = 0.47315601\n",
      "Iteration 393, loss = 0.47304057\n",
      "Iteration 394, loss = 0.47293573\n",
      "Iteration 395, loss = 0.47282635\n",
      "Iteration 396, loss = 0.47272587\n",
      "Iteration 397, loss = 0.47262682\n",
      "Iteration 398, loss = 0.47253928\n",
      "Iteration 399, loss = 0.47242848\n",
      "Iteration 400, loss = 0.47232939\n",
      "Iteration 401, loss = 0.47223274\n",
      "Iteration 402, loss = 0.47214219\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72401596\n",
      "Iteration 2, loss = 0.72046618\n",
      "Iteration 3, loss = 0.71707892\n",
      "Iteration 4, loss = 0.71386719\n",
      "Iteration 5, loss = 0.71083706\n",
      "Iteration 6, loss = 0.70798586\n",
      "Iteration 7, loss = 0.70529318\n",
      "Iteration 8, loss = 0.70276967\n",
      "Iteration 9, loss = 0.70037629\n",
      "Iteration 10, loss = 0.69813649\n",
      "Iteration 11, loss = 0.69602396\n",
      "Iteration 12, loss = 0.69405751\n",
      "Iteration 13, loss = 0.69221389\n",
      "Iteration 14, loss = 0.69047769\n",
      "Iteration 15, loss = 0.68884527\n",
      "Iteration 16, loss = 0.68731655\n",
      "Iteration 17, loss = 0.68585864\n",
      "Iteration 18, loss = 0.68449821\n",
      "Iteration 19, loss = 0.68321365\n",
      "Iteration 20, loss = 0.68199553\n",
      "Iteration 21, loss = 0.68084805\n",
      "Iteration 22, loss = 0.67974751\n",
      "Iteration 23, loss = 0.67869779\n",
      "Iteration 24, loss = 0.67769363\n",
      "Iteration 25, loss = 0.67673491\n",
      "Iteration 26, loss = 0.67579458\n",
      "Iteration 27, loss = 0.67488645\n",
      "Iteration 28, loss = 0.67401507\n",
      "Iteration 29, loss = 0.67316103\n",
      "Iteration 30, loss = 0.67232157\n",
      "Iteration 31, loss = 0.67149912\n",
      "Iteration 32, loss = 0.67068869\n",
      "Iteration 33, loss = 0.66988775\n",
      "Iteration 34, loss = 0.66908776\n",
      "Iteration 35, loss = 0.66829917\n",
      "Iteration 36, loss = 0.66751986\n",
      "Iteration 37, loss = 0.66673546\n",
      "Iteration 38, loss = 0.66595463\n",
      "Iteration 39, loss = 0.66517657\n",
      "Iteration 40, loss = 0.66440127\n",
      "Iteration 41, loss = 0.66361930\n",
      "Iteration 42, loss = 0.66283809\n",
      "Iteration 43, loss = 0.66205591\n",
      "Iteration 44, loss = 0.66127271\n",
      "Iteration 45, loss = 0.66048014\n",
      "Iteration 46, loss = 0.65969248\n",
      "Iteration 47, loss = 0.65890143\n",
      "Iteration 48, loss = 0.65810933\n",
      "Iteration 49, loss = 0.65731361\n",
      "Iteration 50, loss = 0.65651886\n",
      "Iteration 51, loss = 0.65571542\n",
      "Iteration 52, loss = 0.65491275\n",
      "Iteration 53, loss = 0.65410268\n",
      "Iteration 54, loss = 0.65329504\n",
      "Iteration 55, loss = 0.65248504\n",
      "Iteration 56, loss = 0.65167310\n",
      "Iteration 57, loss = 0.65085816\n",
      "Iteration 58, loss = 0.65004600\n",
      "Iteration 59, loss = 0.64922384\n",
      "Iteration 60, loss = 0.64840777\n",
      "Iteration 61, loss = 0.64758495\n",
      "Iteration 62, loss = 0.64675801\n",
      "Iteration 63, loss = 0.64593370\n",
      "Iteration 64, loss = 0.64510490\n",
      "Iteration 65, loss = 0.64427792\n",
      "Iteration 66, loss = 0.64344712\n",
      "Iteration 67, loss = 0.64261312\n",
      "Iteration 68, loss = 0.64177691\n",
      "Iteration 69, loss = 0.64093991\n",
      "Iteration 70, loss = 0.64010131\n",
      "Iteration 71, loss = 0.63926195\n",
      "Iteration 72, loss = 0.63842039\n",
      "Iteration 73, loss = 0.63757606\n",
      "Iteration 74, loss = 0.63673716\n",
      "Iteration 75, loss = 0.63588913\n",
      "Iteration 76, loss = 0.63504502\n",
      "Iteration 77, loss = 0.63419635\n",
      "Iteration 78, loss = 0.63334946\n",
      "Iteration 79, loss = 0.63249457\n",
      "Iteration 80, loss = 0.63164656\n",
      "Iteration 81, loss = 0.63079781\n",
      "Iteration 82, loss = 0.62994357\n",
      "Iteration 83, loss = 0.62909528\n",
      "Iteration 84, loss = 0.62825178\n",
      "Iteration 85, loss = 0.62739645\n",
      "Iteration 86, loss = 0.62654601\n",
      "Iteration 87, loss = 0.62569431\n",
      "Iteration 88, loss = 0.62484502\n",
      "Iteration 89, loss = 0.62399218\n",
      "Iteration 90, loss = 0.62313900\n",
      "Iteration 91, loss = 0.62229105\n",
      "Iteration 92, loss = 0.62144283\n",
      "Iteration 93, loss = 0.62059129\n",
      "Iteration 94, loss = 0.61974221\n",
      "Iteration 95, loss = 0.61889564\n",
      "Iteration 96, loss = 0.61804347\n",
      "Iteration 97, loss = 0.61719765\n",
      "Iteration 98, loss = 0.61635206\n",
      "Iteration 99, loss = 0.61550618\n",
      "Iteration 100, loss = 0.61466612\n",
      "Iteration 101, loss = 0.61382722\n",
      "Iteration 102, loss = 0.61298237\n",
      "Iteration 103, loss = 0.61213879\n",
      "Iteration 104, loss = 0.61129876\n",
      "Iteration 105, loss = 0.61046261\n",
      "Iteration 106, loss = 0.60962371\n",
      "Iteration 107, loss = 0.60878649\n",
      "Iteration 108, loss = 0.60794886\n",
      "Iteration 109, loss = 0.60711183\n",
      "Iteration 110, loss = 0.60627571\n",
      "Iteration 111, loss = 0.60543952\n",
      "Iteration 112, loss = 0.60460752\n",
      "Iteration 113, loss = 0.60377190\n",
      "Iteration 114, loss = 0.60293774\n",
      "Iteration 115, loss = 0.60210621\n",
      "Iteration 116, loss = 0.60127201\n",
      "Iteration 117, loss = 0.60044621\n",
      "Iteration 118, loss = 0.59961464\n",
      "Iteration 119, loss = 0.59878792\n",
      "Iteration 120, loss = 0.59795532\n",
      "Iteration 121, loss = 0.59713474\n",
      "Iteration 122, loss = 0.59631393\n",
      "Iteration 123, loss = 0.59548886\n",
      "Iteration 124, loss = 0.59466276\n",
      "Iteration 125, loss = 0.59384058\n",
      "Iteration 126, loss = 0.59302197\n",
      "Iteration 127, loss = 0.59220364\n",
      "Iteration 128, loss = 0.59138697\n",
      "Iteration 129, loss = 0.59057191\n",
      "Iteration 130, loss = 0.58975811\n",
      "Iteration 131, loss = 0.58895084\n",
      "Iteration 132, loss = 0.58813598\n",
      "Iteration 133, loss = 0.58733125\n",
      "Iteration 134, loss = 0.58652881\n",
      "Iteration 135, loss = 0.58572722\n",
      "Iteration 136, loss = 0.58493104\n",
      "Iteration 137, loss = 0.58413303\n",
      "Iteration 138, loss = 0.58334232\n",
      "Iteration 139, loss = 0.58254628\n",
      "Iteration 140, loss = 0.58176015\n",
      "Iteration 141, loss = 0.58097963\n",
      "Iteration 142, loss = 0.58019678\n",
      "Iteration 143, loss = 0.57942028\n",
      "Iteration 144, loss = 0.57864038\n",
      "Iteration 145, loss = 0.57787045\n",
      "Iteration 146, loss = 0.57709440\n",
      "Iteration 147, loss = 0.57633232\n",
      "Iteration 148, loss = 0.57556699\n",
      "Iteration 149, loss = 0.57480543\n",
      "Iteration 150, loss = 0.57405017\n",
      "Iteration 151, loss = 0.57329462\n",
      "Iteration 152, loss = 0.57254719\n",
      "Iteration 153, loss = 0.57180821\n",
      "Iteration 154, loss = 0.57106256\n",
      "Iteration 155, loss = 0.57032525\n",
      "Iteration 156, loss = 0.56959739\n",
      "Iteration 157, loss = 0.56885416\n",
      "Iteration 158, loss = 0.56812396\n",
      "Iteration 159, loss = 0.56739334\n",
      "Iteration 160, loss = 0.56666845\n",
      "Iteration 161, loss = 0.56594611\n",
      "Iteration 162, loss = 0.56522667\n",
      "Iteration 163, loss = 0.56450897\n",
      "Iteration 164, loss = 0.56379665\n",
      "Iteration 165, loss = 0.56309075\n",
      "Iteration 166, loss = 0.56239159\n",
      "Iteration 167, loss = 0.56169131\n",
      "Iteration 168, loss = 0.56099012\n",
      "Iteration 169, loss = 0.56029367\n",
      "Iteration 170, loss = 0.55960783\n",
      "Iteration 171, loss = 0.55892174\n",
      "Iteration 172, loss = 0.55824359\n",
      "Iteration 173, loss = 0.55756035\n",
      "Iteration 174, loss = 0.55688722\n",
      "Iteration 175, loss = 0.55621560\n",
      "Iteration 176, loss = 0.55555087\n",
      "Iteration 177, loss = 0.55488607\n",
      "Iteration 178, loss = 0.55422649\n",
      "Iteration 179, loss = 0.55356670\n",
      "Iteration 180, loss = 0.55291818\n",
      "Iteration 181, loss = 0.55225999\n",
      "Iteration 182, loss = 0.55161518\n",
      "Iteration 183, loss = 0.55097617\n",
      "Iteration 184, loss = 0.55034615\n",
      "Iteration 185, loss = 0.54969692\n",
      "Iteration 186, loss = 0.54906965\n",
      "Iteration 187, loss = 0.54843860\n",
      "Iteration 188, loss = 0.54782070\n",
      "Iteration 189, loss = 0.54720332\n",
      "Iteration 190, loss = 0.54658459\n",
      "Iteration 191, loss = 0.54597502\n",
      "Iteration 192, loss = 0.54537667\n",
      "Iteration 193, loss = 0.54476848\n",
      "Iteration 194, loss = 0.54417271\n",
      "Iteration 195, loss = 0.54357465\n",
      "Iteration 196, loss = 0.54299038\n",
      "Iteration 197, loss = 0.54239960\n",
      "Iteration 198, loss = 0.54181908\n",
      "Iteration 199, loss = 0.54124359\n",
      "Iteration 200, loss = 0.54066809\n",
      "Iteration 201, loss = 0.54009823\n",
      "Iteration 202, loss = 0.53953967\n",
      "Iteration 203, loss = 0.53897345\n",
      "Iteration 204, loss = 0.53841372\n",
      "Iteration 205, loss = 0.53785766\n",
      "Iteration 206, loss = 0.53730332\n",
      "Iteration 207, loss = 0.53675628\n",
      "Iteration 208, loss = 0.53621401\n",
      "Iteration 209, loss = 0.53567027\n",
      "Iteration 210, loss = 0.53513755\n",
      "Iteration 211, loss = 0.53460575\n",
      "Iteration 212, loss = 0.53408208\n",
      "Iteration 213, loss = 0.53356741\n",
      "Iteration 214, loss = 0.53304201\n",
      "Iteration 215, loss = 0.53252157\n",
      "Iteration 216, loss = 0.53201626\n",
      "Iteration 217, loss = 0.53150760\n",
      "Iteration 218, loss = 0.53100516\n",
      "Iteration 219, loss = 0.53050839\n",
      "Iteration 220, loss = 0.53000700\n",
      "Iteration 221, loss = 0.52951502\n",
      "Iteration 222, loss = 0.52902703\n",
      "Iteration 223, loss = 0.52854148\n",
      "Iteration 224, loss = 0.52805975\n",
      "Iteration 225, loss = 0.52758904\n",
      "Iteration 226, loss = 0.52710925\n",
      "Iteration 227, loss = 0.52664123\n",
      "Iteration 228, loss = 0.52617436\n",
      "Iteration 229, loss = 0.52571636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 230, loss = 0.52525687\n",
      "Iteration 231, loss = 0.52480871\n",
      "Iteration 232, loss = 0.52435270\n",
      "Iteration 233, loss = 0.52390583\n",
      "Iteration 234, loss = 0.52346512\n",
      "Iteration 235, loss = 0.52302010\n",
      "Iteration 236, loss = 0.52258222\n",
      "Iteration 237, loss = 0.52215763\n",
      "Iteration 238, loss = 0.52171847\n",
      "Iteration 239, loss = 0.52129467\n",
      "Iteration 240, loss = 0.52087828\n",
      "Iteration 241, loss = 0.52045062\n",
      "Iteration 242, loss = 0.52003748\n",
      "Iteration 243, loss = 0.51962545\n",
      "Iteration 244, loss = 0.51922633\n",
      "Iteration 245, loss = 0.51881098\n",
      "Iteration 246, loss = 0.51841297\n",
      "Iteration 247, loss = 0.51801717\n",
      "Iteration 248, loss = 0.51762465\n",
      "Iteration 249, loss = 0.51723546\n",
      "Iteration 250, loss = 0.51684721\n",
      "Iteration 251, loss = 0.51646713\n",
      "Iteration 252, loss = 0.51608529\n",
      "Iteration 253, loss = 0.51571116\n",
      "Iteration 254, loss = 0.51533857\n",
      "Iteration 255, loss = 0.51497054\n",
      "Iteration 256, loss = 0.51460115\n",
      "Iteration 257, loss = 0.51424306\n",
      "Iteration 258, loss = 0.51388467\n",
      "Iteration 259, loss = 0.51352458\n",
      "Iteration 260, loss = 0.51317484\n",
      "Iteration 261, loss = 0.51282634\n",
      "Iteration 262, loss = 0.51247819\n",
      "Iteration 263, loss = 0.51213509\n",
      "Iteration 264, loss = 0.51180418\n",
      "Iteration 265, loss = 0.51146060\n",
      "Iteration 266, loss = 0.51112703\n",
      "Iteration 267, loss = 0.51080007\n",
      "Iteration 268, loss = 0.51047248\n",
      "Iteration 269, loss = 0.51015723\n",
      "Iteration 270, loss = 0.50983206\n",
      "Iteration 271, loss = 0.50951568\n",
      "Iteration 272, loss = 0.50920462\n",
      "Iteration 273, loss = 0.50889470\n",
      "Iteration 274, loss = 0.50858046\n",
      "Iteration 275, loss = 0.50827558\n",
      "Iteration 276, loss = 0.50797544\n",
      "Iteration 277, loss = 0.50767660\n",
      "Iteration 278, loss = 0.50737999\n",
      "Iteration 279, loss = 0.50708932\n",
      "Iteration 280, loss = 0.50679348\n",
      "Iteration 281, loss = 0.50650590\n",
      "Iteration 282, loss = 0.50622797\n",
      "Iteration 283, loss = 0.50594023\n",
      "Iteration 284, loss = 0.50566930\n",
      "Iteration 285, loss = 0.50538514\n",
      "Iteration 286, loss = 0.50511706\n",
      "Iteration 287, loss = 0.50483820\n",
      "Iteration 288, loss = 0.50457164\n",
      "Iteration 289, loss = 0.50430577\n",
      "Iteration 290, loss = 0.50404191\n",
      "Iteration 291, loss = 0.50377950\n",
      "Iteration 292, loss = 0.50352075\n",
      "Iteration 293, loss = 0.50326172\n",
      "Iteration 294, loss = 0.50300901\n",
      "Iteration 295, loss = 0.50275952\n",
      "Iteration 296, loss = 0.50251055\n",
      "Iteration 297, loss = 0.50226817\n",
      "Iteration 298, loss = 0.50202156\n",
      "Iteration 299, loss = 0.50178603\n",
      "Iteration 300, loss = 0.50154822\n",
      "Iteration 301, loss = 0.50131061\n",
      "Iteration 302, loss = 0.50107886\n",
      "Iteration 303, loss = 0.50085279\n",
      "Iteration 304, loss = 0.50062280\n",
      "Iteration 305, loss = 0.50039791\n",
      "Iteration 306, loss = 0.50017603\n",
      "Iteration 307, loss = 0.49995456\n",
      "Iteration 308, loss = 0.49973643\n",
      "Iteration 309, loss = 0.49952240\n",
      "Iteration 310, loss = 0.49930636\n",
      "Iteration 311, loss = 0.49910110\n",
      "Iteration 312, loss = 0.49889559\n",
      "Iteration 313, loss = 0.49869325\n",
      "Iteration 314, loss = 0.49848601\n",
      "Iteration 315, loss = 0.49828489\n",
      "Iteration 316, loss = 0.49808485\n",
      "Iteration 317, loss = 0.49789639\n",
      "Iteration 318, loss = 0.49769815\n",
      "Iteration 319, loss = 0.49750418\n",
      "Iteration 320, loss = 0.49732038\n",
      "Iteration 321, loss = 0.49712897\n",
      "Iteration 322, loss = 0.49695342\n",
      "Iteration 323, loss = 0.49676538\n",
      "Iteration 324, loss = 0.49658413\n",
      "Iteration 325, loss = 0.49640526\n",
      "Iteration 326, loss = 0.49623117\n",
      "Iteration 327, loss = 0.49605710\n",
      "Iteration 328, loss = 0.49588764\n",
      "Iteration 329, loss = 0.49571692\n",
      "Iteration 330, loss = 0.49554717\n",
      "Iteration 331, loss = 0.49539182\n",
      "Iteration 332, loss = 0.49522204\n",
      "Iteration 333, loss = 0.49506369\n",
      "Iteration 334, loss = 0.49489708\n",
      "Iteration 335, loss = 0.49474426\n",
      "Iteration 336, loss = 0.49458537\n",
      "Iteration 337, loss = 0.49443355\n",
      "Iteration 338, loss = 0.49428942\n",
      "Iteration 339, loss = 0.49413821\n",
      "Iteration 340, loss = 0.49399195\n",
      "Iteration 341, loss = 0.49385099\n",
      "Iteration 342, loss = 0.49370306\n",
      "Iteration 343, loss = 0.49356729\n",
      "Iteration 344, loss = 0.49342742\n",
      "Iteration 345, loss = 0.49329167\n",
      "Iteration 346, loss = 0.49316104\n",
      "Iteration 347, loss = 0.49302257\n",
      "Iteration 348, loss = 0.49289332\n",
      "Iteration 349, loss = 0.49276146\n",
      "Iteration 350, loss = 0.49263393\n",
      "Iteration 351, loss = 0.49251053\n",
      "Iteration 352, loss = 0.49238698\n",
      "Iteration 353, loss = 0.49226326\n",
      "Iteration 354, loss = 0.49213874\n",
      "Iteration 355, loss = 0.49202214\n",
      "Iteration 356, loss = 0.49190926\n",
      "Iteration 357, loss = 0.49179795\n",
      "Iteration 358, loss = 0.49168571\n",
      "Iteration 359, loss = 0.49156644\n",
      "Iteration 360, loss = 0.49145317\n",
      "Iteration 361, loss = 0.49134900\n",
      "Iteration 362, loss = 0.49124090\n",
      "Iteration 363, loss = 0.49113525\n",
      "Iteration 364, loss = 0.49103228\n",
      "Iteration 365, loss = 0.49092700\n",
      "Iteration 366, loss = 0.49082206\n",
      "Iteration 367, loss = 0.49072602\n",
      "Iteration 368, loss = 0.49062740\n",
      "Iteration 369, loss = 0.49053146\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "x = np.array(df[['medianb2','medianb3','medianb4','medianb5','ndvi','nbrl','dif_ndvi','dif_dnbrl']])\n",
    "y = np.array(df[['verifica']])\n",
    "y = y.ravel()\n",
    "\n",
    "cv_results = model_selection.cross_val_score(clf, x, y, cv=kfold, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
